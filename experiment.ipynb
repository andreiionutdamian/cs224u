{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Hypothesis\n",
    "\n",
    "In past years we have seen a lot of cross-domain application from various deep learning areas to particular real-life cases with apparently little connection. One of those areas is that of applying deep representation learning based on neural language processing to business analytics system and recommender systems in particular.\n",
    "\n",
    "The intuition behind our proposed deep learning pipeline is that we could, at least in theory, generate, through multiple modeling iterations, powerful-enough semantic vector space embeddings for each individual item (product) so that we can *infer replacements items and propose them in the case on original product shortages* â€“ all of these in a self-supervised setting\n",
    "\n",
    "The hypothesis is that we can apply both direct retrofitting-based fine-tuning on the pre-trained product embeddings as well as re-construction of the GloVe vector space with new co-occurrence matrices in order to generate a product vector space model able to generate item-replacement information. More precisely the hypothesis is that our proposed approach will reduce the cosine distance between products that can actually replace each other in real life in a similar manner as presented in the work of Faruqui et al, Dingwall et al, that addresses word vectors, as well as push the distance of the product embeddings that are not similar but still have a semantic relatioship resulted from the vector space optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Data\n",
    "\n",
    "For our experiment we decided to use a real-life transactional dataset that has the following properties. Further information on the data can be observed in the data loading, preparation and minimal visualization of the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We load the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from time import time\n",
    "import textwrap\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup global variables and pretty-prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HOME = 'experiment_data'\n",
    "DATA_FILE = os.path.join(DATA_HOME, 'df_tran_proc_top_15k.csv')\n",
    "META_FILE = os.path.join(DATA_HOME, 'df_items.csv')\n",
    "\n",
    "CHUNK_SIZE = 100 * 1024 ** 2 # read 100MB chunks\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('precision', 4)  \n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(linewidth=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. A few  utility methods\n",
    "\n",
    "In order to confer a real-life setup to our experiment we will generated the MCO with batch reading of the transactional data. For this purpose we have `generate_sparse_mco(file_name)` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_log = []\n",
    "_date = dt.now().strftime(\"%Y%m%d_%H%M\")\n",
    "log_fn = dt.now().strftime(\"logs/\"+_date+\"_log.txt\")\n",
    "\n",
    "def P(s=''):\n",
    "    lst_log.append(s)\n",
    "    print(s, flush=True)\n",
    "    try:\n",
    "        with open(log_fn, 'w') as f:\n",
    "            for item in lst_log:\n",
    "                f.write(\"{}\\n\".format(item))\n",
    "    except:\n",
    "        pass\n",
    "    return\n",
    "\n",
    "def Pr(s=''):\n",
    "    print('\\r' + str(s), end='', flush=True)\n",
    "\n",
    "def add_to_mco(df_chunk, dct_mco, basket_id_field, item_id_field):\n",
    "  Pr(\"  Grouping the transactions...\")\n",
    "  t1 = time()\n",
    "  transactions = df_chunk.groupby(basket_id_field)[item_id_field].apply(list)\n",
    "  nr_trans = df_chunk[basket_id_field].unique().shape[0]\n",
    "  t_trans = time() - t1\n",
    "  Pr(\"  {} transactions grouped in {:.2f}s\".format(\n",
    "      nr_trans, t_trans))\n",
    "  P(\"\")\n",
    "  times = []\n",
    "  time_delta = 1000\n",
    "  last_time = time()\n",
    "  for i, (index, l) in enumerate(transactions.items()):\n",
    "    t1 = time()\n",
    "    market_basket = np.unique(l)  # keep only unique elements\n",
    "    if market_basket.shape[0] == 1:\n",
    "      continue\n",
    "    perm_market_basket = list(itertools.permutations(market_basket, 2))\n",
    "    for pair in perm_market_basket:\n",
    "      if pair not in dct_mco: \n",
    "        dct_mco[pair] = 0\n",
    "      dct_mco[pair] += 1\n",
    "    if (i % time_delta) == 0:\n",
    "      elapsed = time() - last_time\n",
    "      last_time = time()\n",
    "      times.append(elapsed)\n",
    "      mean_time = np.mean(times) / time_delta\n",
    "      remain_time = nr_trans * mean_time - (i + 1) * mean_time\n",
    "      Pr(\"  Processed transactions {:.1f}% - {:.2f} min remaning...\".format(\n",
    "          (i + 1) / nr_trans * 100,\n",
    "         remain_time / 60))\n",
    "    # endfor\n",
    "  # endfor\n",
    "  P(\"\")\n",
    "  return dct_mco\n",
    "\n",
    "\n",
    "def generate_sparse_mco(file_name, chunk_size=CHUNK_SIZE, basket_id_field='BasketId', item_id_field='IDE'):\n",
    "  data_size = os.path.getsize(file_name)\n",
    "  P(\"Reading transactional data file '{}' of size {:.2f} GB...\".format(file_name, data_size / 1024**3))\n",
    "  t1 = time()\n",
    "  chunk_generator = pd.read_csv(file_name, chunksize=chunk_size)\n",
    "\n",
    "  dct_mco = {}\n",
    "  n_rows = 0\n",
    "  for i, df in enumerate(chunk_generator):\n",
    "    n_rows += df.shape[0]\n",
    "    P(\"Processing chunk {} of data - ({} rows so far) ...\".format(i+1, n_rows))\n",
    "    dct_mco = add_to_mco(df, dct_mco, basket_id_field, item_id_field)\n",
    "    \n",
    "  P(\"  Converting dict to sparse matrix...\")\n",
    "  t2 = time()\n",
    "  csr_mco = sparse.csr_matrix((\n",
    "          list(dct_mco.values()),\n",
    "          [list(x) for x in zip(*list(dct_mco.keys()))],\n",
    "      ))\n",
    "  t3 = time()\n",
    "  t_full = t3 - t1\n",
    "  t_csr = t3 - t2\n",
    "  P(\"  MCO Processing done in {:.2f} min (sparse mat creation: {:.2f} min):\".format(\n",
    "      t_full / 60, t_csr / 60))\n",
    "  P(\"  Transactional data:\")\n",
    "  P(textwrap.indent(str(df.iloc[:15]), \" \" * 4))\n",
    "  P(\"  MCO data:\")\n",
    "  P(textwrap.indent(str(csr_mco[:15,:15].toarray()), \" \" * 4))\n",
    "  return csr_mco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Loading, viewing and understanding\n",
    "\n",
    "The real-life provided data comes within a few files. The most notable files are the transactional database file and the metdata file. We are going to read the metadata file and see part of its content as well as read a small chunk for the transactional dataset and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemId</th>\n",
       "      <th>IDE</th>\n",
       "      <th>Freq</th>\n",
       "      <th>ItemName</th>\n",
       "      <th>Ierarhie1</th>\n",
       "      <th>Ierarhie2</th>\n",
       "      <th>IsActive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>545535</td>\n",
       "      <td>0</td>\n",
       "      <td>21248</td>\n",
       "      <td>YUVAL NOAH HARARI / SAPIENS. SCURTA ISTORIE A OMENIRII</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>398648</td>\n",
       "      <td>1</td>\n",
       "      <td>16058</td>\n",
       "      <td>REZERVE STILOU T10 BLUE LAMY SET</td>\n",
       "      <td>11</td>\n",
       "      <td>134</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>406083</td>\n",
       "      <td>2</td>\n",
       "      <td>13371</td>\n",
       "      <td>FELICITARI A 7331335123458</td>\n",
       "      <td>11</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>576732</td>\n",
       "      <td>3</td>\n",
       "      <td>11656</td>\n",
       "      <td>YUVAL NOAH HARARI / HOMO DEUS. SCURTA ISTORIE A VIITORULUI</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>563633</td>\n",
       "      <td>4</td>\n",
       "      <td>10381</td>\n",
       "      <td>MARK MANSON / ARTA SUBTILA A NEPASARII</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>486656</td>\n",
       "      <td>5</td>\n",
       "      <td>8592</td>\n",
       "      <td>ECKHART TOLLE / PUTEREA PREZENTULUI. ED. VI</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>258901</td>\n",
       "      <td>6</td>\n",
       "      <td>8341</td>\n",
       "      <td>TURTA DULCE TIP INIMIOARE</td>\n",
       "      <td>20</td>\n",
       "      <td>269</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>602874</td>\n",
       "      <td>7</td>\n",
       "      <td>7955</td>\n",
       "      <td>YUVAL NOAH HARARI / 21 DE LECTII PENTRU SECOLUL XXI</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>163219</td>\n",
       "      <td>8</td>\n",
       "      <td>7921</td>\n",
       "      <td>CARTI POSTALE (2 S DESIGN) #2000032105856</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>259150</td>\n",
       "      <td>9</td>\n",
       "      <td>7904</td>\n",
       "      <td>JELLY BEAN 75G FT GOURMET BOX F0075-0696F</td>\n",
       "      <td>20</td>\n",
       "      <td>161</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>524657</td>\n",
       "      <td>10</td>\n",
       "      <td>7886</td>\n",
       "      <td>DR. CLARISSA PINKOLA ESTES / FEMEI CARE ALEARGA CU LUPII. POVESTI SI MITURI ALE ARHETIPULUI FEMEII SALBATICE - I122</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>538942</td>\n",
       "      <td>11</td>\n",
       "      <td>7841</td>\n",
       "      <td>FREDRIK BACKMAN / UN BARBAT PE NUME OVE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>213201</td>\n",
       "      <td>12</td>\n",
       "      <td>7815</td>\n",
       "      <td>ROBIN SHARMA / CALUGARUL CARE SI-A VANDUT FERRARI-UL</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>527499</td>\n",
       "      <td>13</td>\n",
       "      <td>7676</td>\n",
       "      <td>TIBI USERIU / 27 DE PASI</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>442042</td>\n",
       "      <td>14</td>\n",
       "      <td>7400</td>\n",
       "      <td>PEN FEATHER BLACK/WHITE ASSORTED / 4345</td>\n",
       "      <td>5</td>\n",
       "      <td>134</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ItemId  IDE   Freq                                                                                                             ItemName  Ierarhie1  Ierarhie2  IsActive\n",
       "0   545535    0  21248                                                               YUVAL NOAH HARARI / SAPIENS. SCURTA ISTORIE A OMENIRII          1         17         1\n",
       "1   398648    1  16058                                                                                     REZERVE STILOU T10 BLUE LAMY SET         11        134         1\n",
       "2   406083    2  13371                                                                                           FELICITARI A 7331335123458         11        107         1\n",
       "3   576732    3  11656                                                           YUVAL NOAH HARARI / HOMO DEUS. SCURTA ISTORIE A VIITORULUI          1         17         1\n",
       "4   563633    4  10381                                                                               MARK MANSON / ARTA SUBTILA A NEPASARII          1         48         1\n",
       "5   486656    5   8592                                                                          ECKHART TOLLE / PUTEREA PREZENTULUI. ED. VI          1         48         1\n",
       "6   258901    6   8341                                                                                            TURTA DULCE TIP INIMIOARE         20        269         1\n",
       "7   602874    7   7955                                                                  YUVAL NOAH HARARI / 21 DE LECTII PENTRU SECOLUL XXI          1         17         1\n",
       "8   163219    8   7921                                                                            CARTI POSTALE (2 S DESIGN) #2000032105856         11         23         1\n",
       "9   259150    9   7904                                                                            JELLY BEAN 75G FT GOURMET BOX F0075-0696F         20        161         1\n",
       "10  524657   10   7886  DR. CLARISSA PINKOLA ESTES / FEMEI CARE ALEARGA CU LUPII. POVESTI SI MITURI ALE ARHETIPULUI FEMEII SALBATICE - I122          1         45         1\n",
       "11  538942   11   7841                                                                              FREDRIK BACKMAN / UN BARBAT PE NUME OVE          1          0         1\n",
       "12  213201   12   7815                                                                 ROBIN SHARMA / CALUGARUL CARE SI-A VANDUT FERRARI-UL          1         48         1\n",
       "13  527499   13   7676                                                                                             TIBI USERIU / 27 DE PASI          1          9         1\n",
       "14  442042   14   7400                                                                              PEN FEATHER BLACK/WHITE ASSORTED / 4345          5        134         1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta = pd.read_csv(META_FILE)\n",
    "df_meta.iloc[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata information taken directly from a real-life production system (ERP) contains raw information minimally describing each product-SKU `IdemId` with product name (`ItemName`) and other information such as number of item sales in observed in the selected period, a unique sequential item identificator (`IDE`) as well as as hierarchy information in two fields `Ierarhie1` and `Ierarhie2` that will be further used as a knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BasketId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>SiteId</th>\n",
       "      <th>TimeStamp</th>\n",
       "      <th>Qtty</th>\n",
       "      <th>ClientId</th>\n",
       "      <th>IsActive</th>\n",
       "      <th>IDE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7130756</td>\n",
       "      <td>441093</td>\n",
       "      <td>26</td>\n",
       "      <td>2016-01-01 14:49:02.403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-103</td>\n",
       "      <td>1</td>\n",
       "      <td>1638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7130756</td>\n",
       "      <td>464012</td>\n",
       "      <td>26</td>\n",
       "      <td>2016-01-01 14:49:02.403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-103</td>\n",
       "      <td>1</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7130756</td>\n",
       "      <td>464013</td>\n",
       "      <td>26</td>\n",
       "      <td>2016-01-01 14:49:02.403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-103</td>\n",
       "      <td>1</td>\n",
       "      <td>2192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7130802</td>\n",
       "      <td>377742</td>\n",
       "      <td>20</td>\n",
       "      <td>2016-01-01 15:49:03.860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-103</td>\n",
       "      <td>1</td>\n",
       "      <td>2132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7130802</td>\n",
       "      <td>405083</td>\n",
       "      <td>20</td>\n",
       "      <td>2016-01-01 15:49:03.860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-103</td>\n",
       "      <td>1</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7130802</td>\n",
       "      <td>405084</td>\n",
       "      <td>20</td>\n",
       "      <td>2016-01-01 15:49:03.860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-103</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7130803</td>\n",
       "      <td>365473</td>\n",
       "      <td>26</td>\n",
       "      <td>2016-01-01 15:49:42.567</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-103</td>\n",
       "      <td>1</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7130803</td>\n",
       "      <td>381249</td>\n",
       "      <td>26</td>\n",
       "      <td>2016-01-01 15:49:42.567</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-103</td>\n",
       "      <td>1</td>\n",
       "      <td>10603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7130809</td>\n",
       "      <td>344274</td>\n",
       "      <td>26</td>\n",
       "      <td>2016-01-01 15:53:19.953</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-103</td>\n",
       "      <td>1</td>\n",
       "      <td>5416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7130809</td>\n",
       "      <td>393877</td>\n",
       "      <td>26</td>\n",
       "      <td>2016-01-01 15:53:19.953</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-103</td>\n",
       "      <td>1</td>\n",
       "      <td>5693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7130810</td>\n",
       "      <td>165748</td>\n",
       "      <td>20</td>\n",
       "      <td>2016-01-01 15:54:17.053</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-103</td>\n",
       "      <td>1</td>\n",
       "      <td>11250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7130810</td>\n",
       "      <td>363903</td>\n",
       "      <td>20</td>\n",
       "      <td>2016-01-01 15:54:17.053</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-103</td>\n",
       "      <td>1</td>\n",
       "      <td>568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7130822</td>\n",
       "      <td>350149</td>\n",
       "      <td>20</td>\n",
       "      <td>2016-01-01 16:05:18.960</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-103</td>\n",
       "      <td>1</td>\n",
       "      <td>6204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7130822</td>\n",
       "      <td>447921</td>\n",
       "      <td>20</td>\n",
       "      <td>2016-01-01 16:05:18.960</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-103</td>\n",
       "      <td>1</td>\n",
       "      <td>2237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7130822</td>\n",
       "      <td>464013</td>\n",
       "      <td>20</td>\n",
       "      <td>2016-01-01 16:05:18.960</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-103</td>\n",
       "      <td>1</td>\n",
       "      <td>2192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    BasketId  ItemId  SiteId                TimeStamp  Qtty  ClientId  IsActive    IDE\n",
       "0    7130756  441093      26  2016-01-01 14:49:02.403   1.0      -103         1   1638\n",
       "1    7130756  464012      26  2016-01-01 14:49:02.403   1.0      -103         1   1975\n",
       "2    7130756  464013      26  2016-01-01 14:49:02.403   1.0      -103         1   2192\n",
       "3    7130802  377742      20  2016-01-01 15:49:03.860   1.0      -103         1   2132\n",
       "4    7130802  405083      20  2016-01-01 15:49:03.860   1.0      -103         1    213\n",
       "5    7130802  405084      20  2016-01-01 15:49:03.860   1.0      -103         1    165\n",
       "6    7130803  365473      26  2016-01-01 15:49:42.567   1.0      -103         1    354\n",
       "7    7130803  381249      26  2016-01-01 15:49:42.567   1.0      -103         1  10603\n",
       "8    7130809  344274      26  2016-01-01 15:53:19.953   1.0      -103         1   5416\n",
       "9    7130809  393877      26  2016-01-01 15:53:19.953   1.0      -103         1   5693\n",
       "10   7130810  165748      20  2016-01-01 15:54:17.053   1.0      -103         1  11250\n",
       "11   7130810  363903      20  2016-01-01 15:54:17.053   1.0      -103         1    568\n",
       "12   7130822  350149      20  2016-01-01 16:05:18.960   1.0      -103         1   6204\n",
       "13   7130822  447921      20  2016-01-01 16:05:18.960   1.0      -103         1   2237\n",
       "14   7130822  464013      20  2016-01-01 16:05:18.960   1.0      -103         1   2192"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_reader = pd.read_csv(DATA_FILE, iterator=True) \n",
    "chunk_reader.get_chunk(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Metrics and overall evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Pipeline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Progress history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. References\n",
    "\n",
    " - Miller, G. A. (1995). WordNet: a lexical database for English. Communications of the ACM, 39-41.\n",
    "\n",
    " - Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Mikolov, Tomas, et al. \"Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 .\n",
    "\n",
    " - Le, Q., & Mikolov, T. (2014). Distributed representations of sentences and documents. International conference on machine learning, (pp. 1188-1196).\n",
    "\n",
    " - Pennington, J., Socher, R., & Manning, C. (2014). GloVe: Global Vectors forWord Representation. Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), (pp. 1532-1543).\n",
    " \n",
    " - Faruqui, M., Dodge, J., Jauhar, S. K., Dyer, C., Hovy, E., & Smith, N. A. (2014). Retrofitting word vectors to semantic lexicons. arXiv preprint arXiv:1411.4166.\n",
    "\n",
    " - MrkÅ¡iÄ‡, N., SÃ©aghdha, D. O., Thomson, B., GaÅ¡iÄ‡, M., Rojas-Barahona, L., Su, P. H., & Young, S. (2016). Counter-fitting word vectors to linguistic constraints. arXiv preprint arXiv:1603.00892.\n",
    "\n",
    " - Ganitkevitch, J., Van Durme, B., & Callison-Burch, C. (2013). PPDB: The paraphrase database. Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, (pp. 758-764).\n",
    "\n",
    " - Grbovic, M., Radosavljevic, V., Djuric, N., Bhamidipati, N., Savla, J., Bhagwan, V., & Sharp, D. (2015). E-commerce in your inbox: Product recommendations at scale. Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 1809-1818.\n",
    " \n",
    " - Lengerich, B. J., Maas, A. L., & Potts, C. (2017). Retrofitting distributional embeddings to knowledge graphs with functional relations. arXiv preprint arXiv:1708.00112.\n",
    "\n",
    " - Volkovs, M., Yu, G. W., & Poutanen, T. (2017). Content-based Neighbor Models for Cold Start. In Proceedings of the Recommender Systems Challenge 2017, (pp. 1-6).\n",
    "\n",
    " - Dingwall, N., & Potts, C. (2018). Mittens: An Extension of GloVe for Learning Domain-Specialized. arXiv preprint arXiv:1803.09901.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
