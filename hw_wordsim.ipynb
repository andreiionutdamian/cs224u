{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework and bake-off: Word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2020\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Set-up](#Set-up)\n",
    "1. [Dataset readers](#Dataset-readers)\n",
    "1. [Dataset comparisons](#Dataset-comparisons)\n",
    "  1. [Vocab overlap](#Vocab-overlap)\n",
    "  1. [Pair overlap and score correlations](#Pair-overlap-and-score-correlations)\n",
    "1. [Evaluation](#Evaluation)\n",
    "  1. [Dataset evaluation](#Dataset-evaluation)\n",
    "  1. [Dataset error analysis](#Dataset-error-analysis)\n",
    "  1. [Full evaluation](#Full-evaluation)\n",
    "1. [Homework questions](#Homework-questions)\n",
    "  1. [PPMI as a baseline [0.5 points]](#PPMI-as-a-baseline-[0.5-points])\n",
    "  1. [Gigaword with LSA at different dimensions [0.5 points]](#Gigaword-with-LSA-at-different-dimensions-[0.5-points])\n",
    "  1. [Gigaword with GloVe for a small number of iterations [0.5 points]](#Gigaword-with-GloVe-for-a-small-number-of-iterations-[0.5-points])\n",
    "  1. [Dice coefficient [0.5 points]](#Dice-coefficient-[0.5-points])\n",
    "  1. [t-test reweighting [2 points]](#t-test-reweighting-[2-points])\n",
    "  1. [Enriching a VSM with subword information [2 points]](#Enriching-a-VSM-with-subword-information-[2-points])\n",
    "  1. [Your original system [3 points]](#Your-original-system-[3-points])\n",
    "1. [Bake-off [1 point]](#Bake-off-[1-point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Word similarity datasets have long been used to evaluate distributed representations. This notebook provides basic code for conducting such analyses with a number of datasets:\n",
    "\n",
    "| Dataset | Pairs | Task-type | Current best Spearman $\\rho$ | Best $\\rho$ paper |   |\n",
    "|---------|-------|-----------|------------------------------|-------------------|---|\n",
    "| [WordSim-353](http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/) | 353 | Relatedness | 82.8 | [Speer et al. 2017](https://arxiv.org/abs/1612.03975) |\n",
    "| [MTurk-771](http://www2.mta.ac.il/~gideon/mturk771.html) | 771 | Relatedness | 81.0 | [Speer et al. 2017](https://arxiv.org/abs/1612.03975) |\n",
    "| [The MEN Test Collection](http://clic.cimec.unitn.it/~elia.bruni/MEN) | 3,000 | Relatedness | 86.6 | [Speer et al. 2017](https://arxiv.org/abs/1612.03975)  | \n",
    "| [SimVerb-3500-dev](http://people.ds.cam.ac.uk/dsg40/simverb.html) | 500 | Similarity | 61.1 | [Mrki&scaron;&cacute; et al. 2016](https://arxiv.org/pdf/1603.00892.pdf) |\n",
    "| [SimVerb-3500-test](http://people.ds.cam.ac.uk/dsg40/simverb.html) | 3,000 | Similarity | 62.4 | [Mrki&scaron;&cacute; et al. 2016](https://arxiv.org/pdf/1603.00892.pdf) |\n",
    "\n",
    "Each of the similarity datasets contains word pairs with an associated human-annotated similarity score. (We convert these to distances to align intuitively with our distance measure functions.) The evaluation code measures the distance between the word pairs in your chosen VSM (which should be a `pd.DataFrame`).\n",
    "\n",
    "The evaluation metric for each dataset is the [Spearman correlation coefficient $\\rho$](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) between the annotated scores and your distances, as is standard in the literature. We also macro-average these correlations across the datasets for an overall summary. (In using the macro-average, we are saying that we care about all the datasets equally, even though they vary in size.)\n",
    "\n",
    "This homework ([questions at the bottom of this notebook](#Homework-questions)) asks you to write code that uses the count matrices in `data/vsmdata` to create and evaluate some baseline models as well as an original model $M$ that you design. This accounts for 9 of the 10 points for this assignment.\n",
    "\n",
    "For the associated bake-off, we will distribute two new word similarity or relatedness datasets and associated reader code, and you will evaluate $M$ (no additional training or tuning allowed!) on those new datasets. Systems that enter will receive the additional homework point, and systems that achieve the top score will receive an additional 0.5 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "import vsm\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VSM_HOME = os.path.join('data', 'vsmdata')\n",
    "\n",
    "WORDSIM_HOME = os.path.join('data', 'wordsim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordsim_dataset_reader(\n",
    "        src_filename, \n",
    "        header=False, \n",
    "        delimiter=',', \n",
    "        score_col_index=2):\n",
    "    \"\"\"Basic reader that works for all similarity datasets. They are \n",
    "    all tabular-style releases where the first two columns give the \n",
    "    word and a later column (`score_col_index`) gives the score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_filename : str\n",
    "        Full path to the source file.\n",
    "    header : bool\n",
    "        Whether `src_filename` has a header. Default: False\n",
    "    delimiter : str\n",
    "        Field delimiter in `src_filename`. Default: ','\n",
    "    score_col_index : int\n",
    "        Column containing the similarity scores Default: 2\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    (str, str, float)\n",
    "       (w1, w2, score) where `score` is the negative of the similarity\n",
    "       score in the file so that we are intuitively aligned with our\n",
    "       distance-based code. To align with our VSMs, all the words are \n",
    "       downcased.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(src_filename) as f:\n",
    "        reader = csv.reader(f, delimiter=delimiter)\n",
    "        if header:\n",
    "            next(reader)\n",
    "        for row in reader:\n",
    "            w1 = row[0].strip().lower()\n",
    "            w2 = row[1].strip().lower()\n",
    "            score = row[score_col_index]\n",
    "            # Negative of scores to align intuitively with distance functions:\n",
    "            score = -float(score)\n",
    "            yield (w1, w2, score)\n",
    "\n",
    "def wordsim353_reader():\n",
    "    \"\"\"WordSim-353: http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/\"\"\"\n",
    "    src_filename = os.path.join(\n",
    "        WORDSIM_HOME, 'wordsim353', 'combined.csv')\n",
    "    return wordsim_dataset_reader(\n",
    "        src_filename, header=True)\n",
    "\n",
    "def mturk771_reader():\n",
    "    \"\"\"MTURK-771: http://www2.mta.ac.il/~gideon/mturk771.html\"\"\"\n",
    "    src_filename = os.path.join(\n",
    "        WORDSIM_HOME, 'MTURK-771.csv')\n",
    "    return wordsim_dataset_reader(\n",
    "        src_filename, header=False)\n",
    "\n",
    "def simverb3500dev_reader():\n",
    "    \"\"\"SimVerb-3500: http://people.ds.cam.ac.uk/dsg40/simverb.html\"\"\"\n",
    "    src_filename = os.path.join(\n",
    "        WORDSIM_HOME, 'SimVerb-3500', 'SimVerb-500-dev.txt')\n",
    "    return wordsim_dataset_reader(\n",
    "        src_filename, delimiter=\"\\t\", header=False, score_col_index=3)\n",
    "\n",
    "def simverb3500test_reader():\n",
    "    \"\"\"SimVerb-3500: http://people.ds.cam.ac.uk/dsg40/simverb.html\"\"\"\n",
    "    src_filename = os.path.join(\n",
    "        WORDSIM_HOME, 'SimVerb-3500', 'SimVerb-3000-test.txt')\n",
    "    return wordsim_dataset_reader(\n",
    "        src_filename, delimiter=\"\\t\", header=False, score_col_index=3)\n",
    "\n",
    "def men_reader():\n",
    "    \"\"\"MEN: http://clic.cimec.unitn.it/~elia.bruni/MEN\"\"\"\n",
    "    src_filename = os.path.join(\n",
    "        WORDSIM_HOME, 'MEN', 'MEN_dataset_natural_form_full')\n",
    "    return wordsim_dataset_reader(\n",
    "        src_filename, header=False, delimiter=' ') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This collection of readers will be useful for flexible evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "READERS = (wordsim353_reader, mturk771_reader, simverb3500dev_reader, \n",
    "           simverb3500test_reader, men_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset comparisons\n",
    "\n",
    "This section does some basic analysis of the datasets. The goal is to obtain a deeper understanding of what problem we're solving â€“ what strengths and weaknesses the datasets have and how they relate to each other. For a full-fledged project, we would want to continue work like this and report on it in the paper, to provide context for the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reader_name(reader):\n",
    "    \"\"\"Return a cleaned-up name for the similarity dataset \n",
    "    iterator `reader`\n",
    "    \"\"\"\n",
    "    return reader.__name__.replace(\"_reader\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab overlap\n",
    "\n",
    "How many vocabulary items are shared across the datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reader_vocab(reader):\n",
    "    \"\"\"Return the set of words (str) in `reader`.\"\"\"\n",
    "    vocab = set()\n",
    "    for w1, w2, _ in reader():\n",
    "        vocab.add(w1)\n",
    "        vocab.add(w2)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reader_vocab_overlap(readers=READERS):\n",
    "    \"\"\"Get data on the vocab-level relationships between pairs of \n",
    "    readers. Returns a a pd.DataFrame containing this information.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for r1, r2 in itertools.product(readers, repeat=2):       \n",
    "        v1 = get_reader_vocab(r1)\n",
    "        v2 = get_reader_vocab(r2)\n",
    "        d = {\n",
    "            'd1': get_reader_name(r1),\n",
    "            'd2': get_reader_name(r2),\n",
    "            'overlap': len(v1 & v2), \n",
    "            'union': len(v1 | v2),\n",
    "            'd1_size': len(v1),\n",
    "            'd2_size': len(v2)}\n",
    "        data.append(d)\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_overlap = get_reader_vocab_overlap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_overlap_crosstab(vocab_overlap):\n",
    "    \"\"\"Return an intuitively formatted `pd.DataFrame` giving \n",
    "    vocab-overlap counts for all the datasets represented in \n",
    "    `vocab_overlap`, the output of `get_reader_vocab_overlap`.\n",
    "    \"\"\"        \n",
    "    xtab = pd.crosstab(\n",
    "        vocab_overlap['d1'], \n",
    "        vocab_overlap['d2'], \n",
    "        values=vocab_overlap['overlap'], \n",
    "        aggfunc=np.mean)\n",
    "    # Blank out the upper right to reduce visual clutter:\n",
    "    for i in range(0, xtab.shape[0]):\n",
    "        for j in range(i+1, xtab.shape[1]):\n",
    "            xtab.iloc[i, j] = ''        \n",
    "    return xtab        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>d2</th>\n",
       "      <th>men</th>\n",
       "      <th>mturk771</th>\n",
       "      <th>simverb3500dev</th>\n",
       "      <th>simverb3500test</th>\n",
       "      <th>wordsim353</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>men</th>\n",
       "      <td>751</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mturk771</th>\n",
       "      <td>230</td>\n",
       "      <td>1113</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simverb3500dev</th>\n",
       "      <td>23</td>\n",
       "      <td>67</td>\n",
       "      <td>536</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simverb3500test</th>\n",
       "      <td>30</td>\n",
       "      <td>94</td>\n",
       "      <td>532</td>\n",
       "      <td>823</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wordsim353</th>\n",
       "      <td>86</td>\n",
       "      <td>158</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "d2               men mturk771 simverb3500dev simverb3500test wordsim353\n",
       "d1                                                                     \n",
       "men              751                                                   \n",
       "mturk771         230     1113                                          \n",
       "simverb3500dev    23       67            536                           \n",
       "simverb3500test   30       94            532             823           \n",
       "wordsim353        86      158             13              17        437"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_overlap_crosstab(vocab_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks reasonable. By design, the SimVerb dev and test sets have a lot of overlap. The other overlap numbers are pretty small, even adjusting for dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair overlap and score correlations\n",
    "\n",
    "How many word pairs are shared across datasets and, for shared pairs, what is the correlation between their scores? That is, do the datasets agree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reader_pairs(reader):\n",
    "    \"\"\"Return the set of alphabetically-sorted word (str) tuples \n",
    "    in `reader`\n",
    "    \"\"\"\n",
    "    return {tuple(sorted([w1, w2])): score for w1, w2, score in reader()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reader_pair_overlap(readers=READERS):\n",
    "    \"\"\"Return a `pd.DataFrame` giving the number of overlapping \n",
    "    word-pairs in pairs of readers, along with the Spearman \n",
    "    correlations.\n",
    "    \"\"\"    \n",
    "    data = []\n",
    "    for r1, r2 in itertools.product(READERS, repeat=2):\n",
    "        if r1.__name__ != r2.__name__:\n",
    "            d1 = get_reader_pairs(r1)\n",
    "            d2 = get_reader_pairs(r2)\n",
    "            overlap = []\n",
    "            for p, s in d1.items():\n",
    "                if p in d2:\n",
    "                    overlap.append([s, d2[p]])\n",
    "            if overlap:\n",
    "                s1, s2 = zip(*overlap)\n",
    "                rho = spearmanr(s1, s2)[0]\n",
    "            else:\n",
    "                rho = None\n",
    "            # Canonical order for the pair:\n",
    "            n1, n2 = sorted([get_reader_name(r1), get_reader_name(r2)])\n",
    "            d = {\n",
    "                'd1': n1,\n",
    "                'd2': n2,\n",
    "                'pair_overlap': len(overlap),\n",
    "                'rho': rho}\n",
    "            data.append(d)\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.sort_values(['pair_overlap','d1','d2'], ascending=False)\n",
    "    # Return only every other row to avoid repeats:\n",
    "    return df[::2].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>pair_overlap</th>\n",
       "      <th>rho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>men</td>\n",
       "      <td>mturk771</td>\n",
       "      <td>11</td>\n",
       "      <td>0.592191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>men</td>\n",
       "      <td>wordsim353</td>\n",
       "      <td>5</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mturk771</td>\n",
       "      <td>simverb3500test</td>\n",
       "      <td>4</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>men</td>\n",
       "      <td>simverb3500test</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>simverb3500dev</td>\n",
       "      <td>simverb3500test</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>simverb3500test</td>\n",
       "      <td>wordsim353</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>simverb3500dev</td>\n",
       "      <td>wordsim353</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mturk771</td>\n",
       "      <td>wordsim353</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mturk771</td>\n",
       "      <td>simverb3500dev</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>men</td>\n",
       "      <td>simverb3500dev</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                d1               d2  pair_overlap       rho\n",
       "0              men         mturk771            11  0.592191\n",
       "1              men       wordsim353             5  0.700000\n",
       "2         mturk771  simverb3500test             4  0.400000\n",
       "3              men  simverb3500test             2  1.000000\n",
       "4   simverb3500dev  simverb3500test             1       NaN\n",
       "5  simverb3500test       wordsim353             0       NaN\n",
       "6   simverb3500dev       wordsim353             0       NaN\n",
       "7         mturk771       wordsim353             0       NaN\n",
       "8         mturk771   simverb3500dev             0       NaN\n",
       "9              men   simverb3500dev             0       NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    display(get_reader_pair_overlap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks reasonable: none of the datasets have a lot of overlapping pairs, so we don't have to worry too much about places where they give conflicting scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "This section builds up the evaluation code that you'll use for the homework and bake-off. For illustrations, I'll read in a VSM created from `data/vsmdata/giga_window5-scaled.csv.gz`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "giga5 = pd.read_csv(\n",
    "    os.path.join(VSM_HOME, \"giga_window5-scaled.csv.gz\"), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity_evaluation(reader, df, distfunc=vsm.cosine):\n",
    "    \"\"\"Word-similarity evalution framework.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    reader : iterator\n",
    "        A reader for a word-similarity dataset. Just has to yield\n",
    "        tuples (word1, word2, score).    \n",
    "    df : pd.DataFrame\n",
    "        The VSM being evaluated.        \n",
    "    distfunc : function mapping vector pairs to floats.\n",
    "        The measure of distance between vectors. Can also be \n",
    "        `vsm.euclidean`, `vsm.matching`, `vsm.jaccard`, as well as \n",
    "        any other float-valued function on pairs of vectors.    \n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If `df.index` is not a subset of the words in `reader`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float, data\n",
    "        `float` is the Spearman rank correlation coefficient between \n",
    "        the dataset scores and the similarity values obtained from \n",
    "        `df` using  `distfunc`. This evaluation is sensitive only to \n",
    "        rankings, not to absolute values.  `data` is a `pd.DataFrame` \n",
    "        with columns['word1', 'word2', 'score', 'distance'].\n",
    "        \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for w1, w2, score in reader():\n",
    "        d = {'word1': w1, 'word2': w2, 'score': score}\n",
    "        for w in [w1, w2]:\n",
    "            if w not in df.index:\n",
    "                raise ValueError(\n",
    "                    \"Word '{}' is in the similarity dataset {} but not in the \"\n",
    "                    \"DataFrame, making this evaluation ill-defined. Please \"\n",
    "                    \"switch to a DataFrame with an appropriate vocabulary.\".\n",
    "                    format(w, get_reader_name(reader))) \n",
    "        d['distance'] = distfunc(df.loc[w1], df.loc[w2])\n",
    "        data.append(d)\n",
    "    data = pd.DataFrame(data)\n",
    "    rho, pvalue = spearmanr(data['score'].values, data['distance'].values)\n",
    "    return rho, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho, eval_df = word_similarity_evaluation(men_reader, giga5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40375964105441753"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sun</td>\n",
       "      <td>sunlight</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>0.956828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>automobile</td>\n",
       "      <td>car</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>0.979143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>river</td>\n",
       "      <td>water</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>0.970105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stairs</td>\n",
       "      <td>staircase</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>0.980475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>morning</td>\n",
       "      <td>sunrise</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>0.963624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word1      word2  score  distance\n",
       "0         sun   sunlight  -50.0  0.956828\n",
       "1  automobile        car  -50.0  0.979143\n",
       "2       river      water  -49.0  0.970105\n",
       "3      stairs  staircase  -49.0  0.980475\n",
       "4     morning    sunrise  -49.0  0.963624"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset error analysis\n",
    "\n",
    "For error analysis, we can look at the words with the largest delta between the gold score and the distance value in our VSM. We do these comparisons based on ranks, just as with our primary metric (Spearman $\\rho$), and we normalize both rankings so that they have a comparable number of levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity_error_analysis(eval_df):    \n",
    "    eval_df['distance_rank'] = _normalized_ranking(eval_df['distance'])\n",
    "    eval_df['score_rank'] = _normalized_ranking(eval_df['score'])\n",
    "    eval_df['error'] =  abs(eval_df['distance_rank'] - eval_df['score_rank'])\n",
    "    return eval_df.sort_values('error')\n",
    "    \n",
    "    \n",
    "def _normalized_ranking(series):\n",
    "    ranks = series.rank(method='dense')\n",
    "    return ranks / ranks.sum()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "      <th>distance</th>\n",
       "      <th>distance_rank</th>\n",
       "      <th>score_rank</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>hummingbird</td>\n",
       "      <td>pelican</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>0.975007</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>2.434543e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2315</th>\n",
       "      <td>lily</td>\n",
       "      <td>pigs</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>0.980834</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>4.016842e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2951</th>\n",
       "      <td>bucket</td>\n",
       "      <td>girls</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.983473</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>4.151568e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>night</td>\n",
       "      <td>sunset</td>\n",
       "      <td>-43.0</td>\n",
       "      <td>0.968690</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>6.520315e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>oak</td>\n",
       "      <td>petals</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>0.979721</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>7.162632e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word1    word2  score  distance  distance_rank  score_rank  \\\n",
       "1041  hummingbird  pelican  -32.0  0.975007       0.000243    0.000244   \n",
       "2315         lily     pigs  -13.0  0.980834       0.000488    0.000487   \n",
       "2951       bucket    girls   -4.0  0.983473       0.000602    0.000603   \n",
       "150         night   sunset  -43.0  0.968690       0.000102    0.000103   \n",
       "2062          oak   petals  -17.0  0.979721       0.000435    0.000436   \n",
       "\n",
       "             error  \n",
       "1041  2.434543e-07  \n",
       "2315  4.016842e-07  \n",
       "2951  4.151568e-07  \n",
       "150   6.520315e-07  \n",
       "2062  7.162632e-07  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity_error_analysis(eval_df).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worst predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "      <th>distance</th>\n",
       "      <th>distance_rank</th>\n",
       "      <th>score_rank</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>branch</td>\n",
       "      <td>twigs</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>0.984622</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>birds</td>\n",
       "      <td>stork</td>\n",
       "      <td>-43.0</td>\n",
       "      <td>0.987704</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>bloom</td>\n",
       "      <td>tulip</td>\n",
       "      <td>-43.0</td>\n",
       "      <td>0.990993</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>bloom</td>\n",
       "      <td>blossom</td>\n",
       "      <td>-43.0</td>\n",
       "      <td>0.991760</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>bloom</td>\n",
       "      <td>rose</td>\n",
       "      <td>-43.0</td>\n",
       "      <td>0.992406</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word1    word2  score  distance  distance_rank  score_rank     error\n",
       "67   branch    twigs  -45.0  0.984622       0.000630    0.000077  0.000553\n",
       "190   birds    stork  -43.0  0.987704       0.000657    0.000103  0.000554\n",
       "185   bloom    tulip  -43.0  0.990993       0.000663    0.000103  0.000561\n",
       "167   bloom  blossom  -43.0  0.991760       0.000664    0.000103  0.000561\n",
       "198   bloom     rose  -43.0  0.992406       0.000664    0.000103  0.000561"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity_error_analysis(eval_df).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full evaluation is just a loop over all the readers on which one want to evaluate, with a macro-average at the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_word_similarity_evaluation(df, readers=READERS, distfunc=vsm.cosine):\n",
    "    \"\"\"Evaluate a VSM against all datasets in `readers`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "    readers : tuple \n",
    "        The similarity dataset readers on which to evaluate.\n",
    "    distfunc : function mapping vector pairs to floats.\n",
    "        The measure of distance between vectors. Can also be \n",
    "        `vsm.euclidean`, `vsm.matching`, `vsm.jaccard`, as well as \n",
    "        any other float-valued function on pairs of vectors.    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Mapping dataset names to Spearman r values.\n",
    "        \n",
    "    \"\"\"        \n",
    "    scores = {}     \n",
    "    for reader in readers:\n",
    "        score, data_df = word_similarity_evaluation(reader, df, distfunc=distfunc)\n",
    "        scores[get_reader_name(reader)] = score\n",
    "    series = pd.Series(scores, name='Spearman r')\n",
    "    series['Macro-average'] = series.mean()\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordsim353         0.327831\n",
       "mturk771           0.143146\n",
       "simverb3500dev    -0.068038\n",
       "simverb3500test   -0.066348\n",
       "men                0.403760\n",
       "Macro-average      0.148070\n",
       "Name: Spearman r, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    display(full_word_similarity_evaluation(giga5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework questions\n",
    "\n",
    "Please embed your homework responses in this notebook, and do not delete any cells from the notebook. (You are free to add as many cells as you like as part of your responses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPMI as a baseline [0.5 points]\n",
    "\n",
    "The insight behind PPMI is a recurring theme in word representation learning, so it is a natural baseline for our task. For this question, write a function called `run_giga_ppmi_baseline` that does the following:\n",
    "\n",
    "1. Reads the Gigaword count matrix with a window of 20 and a flat scaling function into a `pd.DataFrame`s, as is done in the VSM notebooks. The file is `data/vsmdata/giga_window20-flat.csv.gz`, and the VSM notebooks provide examples of the needed code.\n",
    "\n",
    "1. Reweights this count matrix with PPMI.\n",
    "\n",
    "1. Evaluates this reweighted matrix using `full_word_similarity_evaluation`. The return value of `run_giga_ppmi_baseline` should be the return value of this call to `full_word_similarity_evaluation`.\n",
    "\n",
    "The goal of this question is to help you get more familiar with the code in `vsm` and the function `full_word_similarity_evaluation`.\n",
    "\n",
    "The function `test_run_giga_ppmi_baseline` can be used to test that you've implemented this specification correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_giga_ppmi_baseline():    \n",
    "    giga20 = pd.read_csv(os.path.join(VSM_HOME, 'giga_window20-flat.csv.gz'), index_col=0)\n",
    "    giga20_pmi = vsm.pmi(giga20, positive=True)\n",
    "    res1 = full_word_similarity_evaluation(giga20_pmi)\n",
    "    return res1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run_giga_ppmi_baseline(run_giga_ppmi_baseline):\n",
    "    result = run_giga_ppmi_baseline()\n",
    "    ws_result = result.loc['wordsim353'].round(2)\n",
    "    ws_expected = 0.58\n",
    "    assert ws_result == ws_expected, \\\n",
    "        \"Expected wordsim353 value of {}; got {}\".format(ws_expected, ws_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_run_giga_ppmi_baseline(run_giga_ppmi_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gigaword with LSA at different dimensions [0.5 points]\n",
    "\n",
    "We might expect PPMI and LSA to form a solid pipeline that combines the strengths of PPMI with those of dimensionality reduction. However, LSA has a hyper-parameter $k$ â€“ the dimensionality of the final representations â€“ that will impact performance. For this problem, write a wrapper function `run_ppmi_lsa_pipeline` that does the following:\n",
    "\n",
    "1. Takes as input a count `pd.DataFrame` and an LSA parameter `k`.\n",
    "1. Reweights the count matrix with PPMI.\n",
    "1. Applies LSA with dimensionality `k`.\n",
    "1. Evaluates this reweighted matrix using `full_word_similarity_evaluation`. The return value of `run_ppmi_lsa_pipeline` should be the return value of this call to `full_word_similarity_evaluation`.\n",
    "\n",
    "The goal of this question is to help you get a feel for how much LSA alone can contribute to this problem. \n",
    "\n",
    "The  function `test_run_ppmi_lsa_pipeline` will test your function on the count matrix in `data/vsmdata/giga_window20-flat.csv.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ppmi_lsa_pipeline(count_df, k):\n",
    "    pmi_df = vsm.pmi(count_df, positive=True)\n",
    "    lsa_df = vsm.lsa(pmi_df, k=k)\n",
    "    res = full_word_similarity_evaluation(lsa_df)\n",
    "    return res\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run_ppmi_lsa_pipeline(run_ppmi_lsa_pipeline):\n",
    "    giga20 = pd.read_csv(\n",
    "        os.path.join(VSM_HOME, \"giga_window20-flat.csv.gz\"), index_col=0)\n",
    "    results = run_ppmi_lsa_pipeline(giga20, k=10)\n",
    "    men_expected = 0.57\n",
    "    men_result = results.loc['men'].round(2)\n",
    "    assert men_result == men_expected,\\\n",
    "        \"Expected men value of {}; got {}\".format(men_expected, men_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_run_ppmi_lsa_pipeline(run_ppmi_lsa_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gigaword with GloVe for a small number of iterations [0.5 points]\n",
    "\n",
    "Ideally, we would run GloVe for a very large number of iterations on a GPU machine to compare it against its close cousin PMI. However, we don't want this homework to cost you a lot of money or monopolize a lot of your available computing resources, so let's instead just probe GloVe a little bit to see if it has promise for our task. For this problem, write a function `run_small_glove_evals` that does the following:\n",
    "\n",
    "1. Reads in `data/vsmdata/giga_window20-flat.csv.gz`.\n",
    "1. Runs GloVe for 10, 100, and 200 iterations on `data/vsmdata/giga_window20-flat.csv.gz`, using the `mittens` implementation of `GloVe`. \n",
    "  * For all the other parameters to `mittens.GloVe` besides `max_iter`, use the package's defaults.\n",
    "  * Because of the way that implementation is designed, these will have to be separate runs, but they should be relatively quick. \n",
    "1. Stores the values in a `dict` mapping each `max_iter` value to its associated 'Macro-average' score according to `full_word_similarity_evaluation`. `run_small_glove_evals`  should return this `dict`.\n",
    "\n",
    "The trend should give you a sense for whether it is worth running GloVe for more iterations.\n",
    "\n",
    "Some implementation notes:\n",
    "\n",
    "* Your trained GloVe matrix `X` needs to be wrapped in a `pd.DataFrame` to work with `full_word_similarity_evaluation`. `pd.DataFrame(X, index=giga20.index)` will do the trick.\n",
    "\n",
    "* If `glv` is your GloVe model, then running `glv.sess.close()` after each model is trained will silence warnings from TensorFlow about interactive sessions being active.\n",
    "\n",
    "Performance will vary a lot for this function, so there is some uncertainty in the testing, but `test_run_small_glove_evals` will at least check that you wrote a function with the right general logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_small_glove_evals():\n",
    "  from mittens import GloVe\n",
    "  all_res = {}\n",
    "  giga20 = pd.read_csv(\n",
    "      os.path.join(VSM_HOME, \"giga_window20-flat.csv.gz\"), index_col=0)\n",
    "  for max_iter in [10, 100, 200]:\n",
    "    glove_model = GloVe(max_iter=max_iter)\n",
    "    np_giga20 = glove_model.fit(giga20.values)\n",
    "    glove_model.sess.close()\n",
    "    giga20_glove = pd.DataFrame(np_giga20, index=giga20.index)\n",
    "    res = full_word_similarity_evaluation(giga20_glove)\n",
    "    all_res[max_iter] = res.loc['Macro-average'].round(2)    \n",
    "  return all_res\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run_small_glove_evals(run_small_glove_evals):\n",
    "    data = run_small_glove_evals()\n",
    "    for max_iter in (10, 100, 200):\n",
    "        assert max_iter in data\n",
    "        assert isinstance(data[max_iter], float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\anaconda3\\envs\\nlu\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\anaconda3\\envs\\nlu\\lib\\site-packages\\tensorflow_core\\python\\training\\adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 200: loss: 1981474.755"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_run_small_glove_evals(run_small_glove_evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dice coefficient [0.5 points]\n",
    "\n",
    "Implement the Dice coefficient for real-valued vectors, as\n",
    "\n",
    "$$\n",
    "\\textbf{dice}(u, v) = \n",
    "1 - \\frac{\n",
    "  2 \\sum_{i=1}^{n}\\min(u_{i}, v_{i})\n",
    "}{\n",
    "    \\sum_{i=1}^{n} u_{i} + v_{i}\n",
    "}$$\n",
    " \n",
    "You can use `test_dice_implementation` below to check that your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dice_implementation(func):\n",
    "    \"\"\"`func` should be an implementation of `dice` as defined above.\"\"\"\n",
    "    X = np.array([\n",
    "        [  4.,   4.,   2.,   0.],\n",
    "        [  4.,  61.,   8.,  18.],\n",
    "        [  2.,   8.,  10.,   0.],\n",
    "        [  0.,  18.,   0.,   5.]]) \n",
    "    assert func(X[0], X[1]).round(5) == 0.80198\n",
    "    assert func(X[1], X[2]).round(5) == 0.67568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(u, v):\n",
    "    n = 2 * np.minimum(u,v).sum()\n",
    "    d = (u+v).sum()\n",
    "    return 1 - n/d\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_dice_implementation(dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-test reweighting [2 points]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-test statistic can be thought of as a reweighting scheme. For a count matrix $X$, row index $i$, and column index $j$:\n",
    "\n",
    "$$\\textbf{ttest}(X, i, j) = \n",
    "\\frac{\n",
    "    P(X, i, j) - \\big(P(X, i, *)P(X, *, j)\\big)\n",
    "}{\n",
    "\\sqrt{(P(X, i, *)P(X, *, j))}\n",
    "}$$\n",
    "\n",
    "where $P(X, i, j)$ is $X_{ij}$ divided by the total values in $X$, $P(X, i, *)$ is the sum of the values in row $i$ of $X$ divided by the total values in $X$, and $P(X, *, j)$ is the sum of the values in column $j$ of $X$ divided by the total values in $X$.\n",
    "\n",
    "For this problem, implement this reweighting scheme. You can use `test_ttest_implementation` below to check that your implementation is correct. You do not need to use this for any evaluations, though we hope you will be curious enough to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ttest_implementation(func):\n",
    "    \"\"\"`func` should be an implementation of t-test reweighting as \n",
    "    defined above.\n",
    "    \"\"\"\n",
    "    X = pd.DataFrame(np.array([\n",
    "        [  4.,   4.,   2.,   0.],\n",
    "        [  4.,  61.,   8.,  18.],\n",
    "        [  2.,   8.,  10.,   0.],\n",
    "        [  0.,  18.,   0.,   5.]]))    \n",
    "    actual = np.array([\n",
    "        [ 0.33056, -0.07689,  0.04321, -0.10532],\n",
    "        [-0.07689,  0.03839, -0.10874,  0.07574],\n",
    "        [ 0.04321, -0.10874,  0.36111, -0.14894],\n",
    "        [-0.10532,  0.07574, -0.14894,  0.05767]])    \n",
    "    predicted = func(X)\n",
    "    assert np.array_equal(predicted.round(5), actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest(df):\n",
    "    X = df.values\n",
    "    P_X_i_j = X / X.sum()\n",
    "    col = (X.sum(axis=1) / X.sum()).reshape(-1,1)\n",
    "    row = X.sum(axis=0) / X.sum()\n",
    "    P_X_i_s = np.hstack([col for _ in range(X.shape[1])])\n",
    "    P_X_j_s = np.vstack([row for _ in range(X.shape[0])])\n",
    "    d = np.sqrt(P_X_i_s * P_X_j_s)\n",
    "    n = P_X_i_j - (P_X_i_s * P_X_j_s)    \n",
    "    res = pd.DataFrame( n / d, index=df.index, columns=df.columns)\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_ttest_implementation(ttest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriching a VSM with subword information [2 points]\n",
    "\n",
    "It might be useful to combine character-level information with word-level information. To help you begin asssessing this idea, this question asks you to write a function that modifies an existing VSM so that the representation for each word $w$ is the element-wise sum of $w$'s original word-level representation with all the representations for the n-grams $w$ contains. \n",
    "\n",
    "The following starter code should help you structure this and clarify the requirements, and a simple test is included below as well.\n",
    "\n",
    "You don't need to write a lot of code; the motivation for this question is that the function you write could have practical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword_enrichment(df, n=4):\n",
    "    \n",
    "    # 1. Use `vsm.ngram_vsm` to create a character-level \n",
    "    # VSM from `df`, using the above parameter `n` to \n",
    "    # set the size of the ngrams.\n",
    "    \n",
    "    df_ngram = vsm.ngram_vsm(df, n=n)\n",
    "\n",
    "        \n",
    "    # 2. Use `vsm.character_level_rep` to get the representation\n",
    "    # for every word in `df` according to the character-level\n",
    "    # VSM you created above.\n",
    "    \n",
    "    df_new_vsm = df.apply(func=lambda x: pd.Series(vsm.character_level_rep(x.name, cf=df_ngram, n=n),\n",
    "                                                   index=df.columns), \n",
    "                          axis=1)\n",
    "    \n",
    "    # 3. For each representation created at step 2, add in its\n",
    "    # original representation from `df`. (This should use\n",
    "    # element-wise addition; the dimensionality of the vectors\n",
    "    # will be unchanged.)\n",
    "                            \n",
    "    df_final_vsm = df + df_new_vsm\n",
    "\n",
    "    \n",
    "    # 4. Return a `pd.DataFrame` with the same index and column\n",
    "    # values as `df`, but filled with the new representations\n",
    "    # created at step 3.\n",
    "                            \n",
    "    return df_final_vsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_subword_enrichment(func):\n",
    "    \"\"\"`func` should be an implementation of subword_enrichment as \n",
    "    defined above.\n",
    "    \"\"\"\n",
    "    vocab = [\"ABCD\", \"BCDA\", \"CDAB\", \"DABC\"]\n",
    "    df = pd.DataFrame([\n",
    "        [1, 1, 2, 1],\n",
    "        [3, 4, 2, 4],\n",
    "        [0, 0, 1, 0],\n",
    "        [1, 0, 0, 0]], index=vocab)\n",
    "    expected = pd.DataFrame([\n",
    "        [14, 14, 18, 14],\n",
    "        [22, 26, 18, 26],\n",
    "        [10, 10, 14, 10],\n",
    "        [14, 10, 10, 10]], index=vocab)\n",
    "    new_df = func(df, n=2)\n",
    "    assert np.array_equal(expected.columns, new_df.columns), \\\n",
    "        \"Columns are not the same\"\n",
    "    assert np.array_equal(expected.index, new_df.index), \\\n",
    "        \"Indices are not the same\"\n",
    "    assert np.array_equal(expected.values, new_df.values), \\\n",
    "        \"Co-occurrence values aren't the same\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_subword_enrichment(subword_enrichment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your original system [3 points]\n",
    "\n",
    "This question asks you to design your own model. You can of course include steps made above (ideally, the above questions informed your system design!), but your model should not be literally identical to any of the above models. Other ideas: retrofitting, autoencoders, GloVe, subword modeling, ... \n",
    "\n",
    "Requirements:\n",
    "\n",
    "1. Your code must operate on one of the count matrices in `data/vsmdata`. You can choose which one. __Other pretrained vectors cannot be introduced__.\n",
    "\n",
    "1. Your code must be self-contained, so that we can work with your model directly in your homework submission notebook. If your model depends on external data or other resources, please submit a ZIP archive containing these resources along with your submission.\n",
    "\n",
    "In the cell below, please provide a brief technical description of your original system, so that the teaching team can gain an understanding of what it does. This will help us to understand your code and analyze all the submissions to identify patterns and strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your system description in this cell.\n",
    "# Please do not remove this comment.\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    pass\n",
    "    \"\"\"\n",
    "    The general approach of the system was to explore as many available options as possible in a manner similar to a \n",
    "    hand-crafted grid-search (discrete manually defined grid). In terms of actual models four different categories \n",
    "    have been targeted: baseline models (such as PPMI, discounted PPMI, LSA), retrofited baseline models (such as\n",
    "    retrofitted discounted positive PMI), autoencoders and GloVe-based models (autoencoder, based on the simple\n",
    "    provided architecture without any other improvements such as additional hidden layers, use LSA-based dimensional \n",
    "    reduction of the PMI-transformed MCO and particularly we found that positive PMI works better than negative). \n",
    "    The fourth and final category is that of ensembles that concatenate (or concatenate-and-SVM-reduce) multiple \n",
    "    embeddings into one single embedding (for example: GloVe 150 retrofitted with LSA450 - autoencoder150 \n",
    "    retrofitted and then finally LSA reduced).\n",
    "    Another important architectural decision was to train and evaluate each proposed candidate using all provided \n",
    "    MCOs from `data/vsmdata` so that later decide what model-dataset combination works best for our experiment.\n",
    "    \n",
    "    After performing more than 80 training iterations (more than 20 approaches on each of the 4 provided MCOs) we \n",
    "    found out that our best option is to retrofit the results of a auto-encoder trained on the LSA-reduced \n",
    "    discounted positive PMI matrix of the imdb-window5 MCO. With this conclusion the initial stage of \n",
    "    heuristical grid exploration was closed and the focus has been shifted on fine-tuning proposed model. In order to\n",
    "    have a picture of the grid-search results below is a dataframe output of the results where:\n",
    "     - LSAxxx_PPMIyy are LSA reduced at k=xxx positive PMI matrices that might have discounts (2 different approaches)\n",
    "     - AExxx_yK2 are autoencoders with xxx embeddings trained y-thousands epochs \n",
    "     - PPMIxxxx are varios positive PMIs baselines (w/o discount applyed with 2 options)\n",
    "     - Cx are ensembles (two architectures)\n",
    "     - _RETR added after model name signifies that result has been retrofitted with WordNet\n",
    "     \n",
    "                 MODEL                 DATA  wordsim353  mturk771  simverb3500dev  simverb3500test       men  Macro-average\n",
    "39              G75_2K   giga_window20-flat    0.346009  0.401967        0.056723         0.049627  0.487193       0.268304\n",
    "41         G75_2K_RETR   giga_window20-flat    0.321310  0.363652        0.108407         0.103789  0.445423       0.268516\n",
    "42        G200_2K_RETR   giga_window20-flat    0.313237  0.359199        0.126266         0.115625  0.458164       0.274498\n",
    "40             G200_2K   giga_window20-flat    0.335347  0.414257        0.071788         0.062357  0.496666       0.276083\n",
    "82             C2_150r   imdb_window20-flat    0.325856  0.352635        0.149259         0.132554  0.448724       0.281806\n",
    "81              C2_150   imdb_window20-flat    0.332357  0.352534        0.152638         0.134357  0.450520       0.284481\n",
    "66              G75_2K  imdb_window5-scaled    0.370917  0.388565        0.093948         0.105596  0.482576       0.288321\n",
    "68         G75_2K_RETR  imdb_window5-scaled    0.374163  0.385513        0.197215         0.195217  0.467951       0.324012\n",
    "27              C2_150   giga_window20-flat    0.394362  0.427948        0.165727         0.154533  0.551803       0.338874\n",
    "28             C2_150r   giga_window20-flat    0.394906  0.428740        0.174473         0.159296  0.560530       0.343589\n",
    "12              G75_2K  giga_window5-scaled    0.402494  0.479712        0.168492         0.112990  0.589486       0.350635\n",
    "67             G200_2K  imdb_window5-scaled    0.438300  0.474744        0.169428         0.143061  0.544740       0.354055\n",
    "14         G75_2K_RETR  giga_window5-scaled    0.388175  0.441037        0.214476         0.181746  0.560286       0.357144\n",
    "69        G200_2K_RETR  imdb_window5-scaled    0.399811  0.441326        0.261823         0.233856  0.524049       0.372173\n",
    "13             G200_2K  giga_window5-scaled    0.432319  0.508929        0.186463         0.147255  0.621126       0.379219\n",
    "15        G200_2K_RETR  giga_window5-scaled    0.404446  0.471663        0.247540         0.221316  0.592824       0.387558\n",
    "55             C2_150r  imdb_window5-scaled    0.430801  0.453289        0.264101         0.240273  0.556222       0.388937\n",
    "54              C2_150  imdb_window5-scaled    0.442718  0.460120        0.269422         0.240553  0.573499       0.397263\n",
    "1              C2_150r  giga_window5-scaled    0.443774  0.498379        0.250595         0.227297  0.630670       0.410143\n",
    "52        LSA75_PPMID2   giga_window20-flat    0.551618  0.488368        0.230429         0.155304  0.656758       0.416495\n",
    "0               C2_150  giga_window5-scaled    0.447012  0.509440        0.256311         0.231371  0.643621       0.417551\n",
    "43                PPMI   giga_window20-flat    0.582573  0.495228        0.232637         0.158925  0.624885       0.418850\n",
    "45              PPMID2   giga_window20-flat    0.587157  0.507723        0.241369         0.166956  0.637283       0.428097\n",
    "83              C1_150   imdb_window20-flat    0.518883  0.541709        0.245962         0.191885  0.648792       0.429446\n",
    "84             C1_150r   imdb_window20-flat    0.519434  0.542172        0.246448         0.192447  0.649361       0.429972\n",
    "51       LSA200_PPMID2   giga_window20-flat    0.573059  0.512123        0.247582         0.172587  0.669115       0.434893\n",
    "47       LSA100_PPMID1   giga_window20-flat    0.582617  0.508020        0.244603         0.164431  0.693490       0.438632\n",
    "29              C1_150   giga_window20-flat    0.573512  0.515020        0.253321         0.187255  0.672496       0.440321\n",
    "30             C1_150r   giga_window20-flat    0.573272  0.515224        0.253351         0.187827  0.673018       0.440538\n",
    "36          AE100_1Kd2   giga_window20-flat    0.574683  0.527902        0.253993         0.179512  0.682303       0.443679\n",
    "86          AE100_1Kd1   imdb_window20-flat    0.585852  0.569396        0.193383         0.174075  0.696205       0.443782\n",
    "44              PPMID1   giga_window20-flat    0.615723  0.514810        0.256390         0.172416  0.669964       0.445860\n",
    "85          AE200_1Kd1   imdb_window20-flat    0.595826  0.566020        0.207111         0.179068  0.684143       0.446434\n",
    "46       LSA200_PPMID1   giga_window20-flat    0.601415  0.527075        0.255420         0.174647  0.694131       0.450538\n",
    "32          AE100_1Kd1   giga_window20-flat    0.600119  0.532503        0.248483         0.184230  0.706567       0.454380\n",
    "48    LSA200_PPMI_RETR   giga_window20-flat    0.559960  0.517598        0.285164         0.259575  0.659936       0.456446\n",
    "87     AE200_1Kd1_RETR   imdb_window20-flat    0.558048  0.522121        0.307551         0.256777  0.658868       0.460673\n",
    "25        LSA75_PPMID2  giga_window5-scaled    0.551251  0.579595        0.264351         0.196538  0.721627       0.462672\n",
    "90          AE100_1Kd2   imdb_window20-flat    0.580800  0.563325        0.267625         0.219934  0.695829       0.465503\n",
    "50   LSA75_PPMID1_RETR   giga_window20-flat    0.577490  0.524386        0.285175         0.247567  0.694228       0.465769\n",
    "53  LSA200_PPMID2_RETR   giga_window20-flat    0.566863  0.526416        0.296860         0.265071  0.674468       0.465936\n",
    "88     AE100_1Kd1_RETR   imdb_window20-flat    0.553460  0.527072        0.306167         0.265256  0.679480       0.466287\n",
    "35          AE200_1Kd2   giga_window20-flat    0.599898  0.563121        0.275055         0.207868  0.695622       0.468313\n",
    "31          AE200_1Kd1   giga_window20-flat    0.618046  0.567278        0.271859         0.200761  0.710471       0.473683\n",
    "89          AE200_1Kd2   imdb_window20-flat    0.602134  0.590578        0.249717         0.235149  0.696128       0.474741\n",
    "70                PPMI  imdb_window5-scaled    0.577190  0.562433        0.310526         0.270943  0.660408       0.476300\n",
    "9           AE100_1Kd2  giga_window5-scaled    0.568349  0.595862        0.259853         0.226785  0.743782       0.478926\n",
    "72              PPMID2  imdb_window5-scaled    0.581862  0.572660        0.296686         0.271814  0.675204       0.479645\n",
    "71              PPMID1  imdb_window5-scaled    0.588492  0.563234        0.307426         0.266128  0.676107       0.480277\n",
    "49  LSA200_PPMID1_RETR   giga_window20-flat    0.596064  0.539958        0.304601         0.265470  0.696623       0.480543\n",
    "38     AE100_1Kd2_RETR   giga_window20-flat    0.595299  0.541724        0.314201         0.272006  0.690043       0.482655\n",
    "79        LSA75_PPMID2  imdb_window5-scaled    0.603732  0.586757        0.266972         0.255839  0.702384       0.483137\n",
    "17              PPMID1  giga_window5-scaled    0.591688  0.581325        0.306978         0.239871  0.706676       0.485307\n",
    "20       LSA100_PPMID1  giga_window5-scaled    0.568276  0.599770        0.294443         0.230995  0.750179       0.488733\n",
    "5           AE100_1Kd1  giga_window5-scaled    0.561517  0.591780        0.301127         0.244475  0.750541       0.489888\n",
    "16                PPMI  giga_window5-scaled    0.604212  0.587155        0.316475         0.250925  0.696400       0.491033\n",
    "34     AE100_1Kd1_RETR   giga_window20-flat    0.612804  0.554966        0.310947         0.274225  0.715919       0.493772\n",
    "24       LSA200_PPMID2  giga_window5-scaled    0.582203  0.619073        0.286614         0.236081  0.746754       0.494145\n",
    "18              PPMID2  giga_window5-scaled    0.590522  0.609254        0.311360         0.247677  0.712918       0.494346\n",
    "4           AE200_1Kd1  giga_window5-scaled    0.587092  0.612370        0.275897         0.249625  0.749850       0.494967\n",
    "58          AE200_1Kd1  imdb_window5-scaled    0.607768  0.599900        0.275105         0.268931  0.729448       0.496230\n",
    "59          AE100_1Kd1  imdb_window5-scaled    0.593525  0.601826        0.295940         0.266454  0.732591       0.498067\n",
    "63          AE100_1Kd2  imdb_window5-scaled    0.592976  0.602873        0.281321         0.282096  0.741488       0.500151\n",
    "19       LSA200_PPMID1  giga_window5-scaled    0.592502  0.622624        0.294481         0.241731  0.750920       0.500452\n",
    "37     AE200_1Kd2_RETR   giga_window20-flat    0.605090  0.565620        0.340461         0.303114  0.700335       0.502924\n",
    "8           AE200_1Kd2  giga_window5-scaled    0.594110  0.632567        0.276709         0.255564  0.758668       0.503523\n",
    "2               C1_150  giga_window5-scaled    0.586615  0.619190        0.311337         0.265907  0.744875       0.505585\n",
    "3              C1_150r  giga_window5-scaled    0.586178  0.620189        0.311115         0.267009  0.745321       0.505962\n",
    "23   LSA75_PPMID1_RETR  giga_window5-scaled    0.537824  0.590572        0.344034         0.315286  0.743571       0.506257\n",
    "74       LSA100_PPMID1  imdb_window5-scaled    0.604813  0.616821        0.308645         0.266924  0.735871       0.506615\n",
    "91     AE200_1Kd2_RETR   imdb_window20-flat    0.585607  0.563341        0.377524         0.323619  0.687013       0.507421\n",
    "73       LSA200_PPMID1  imdb_window5-scaled    0.618076  0.620043        0.306045         0.274083  0.737493       0.511148\n",
    "33     AE200_1Kd1_RETR   giga_window20-flat    0.623699  0.586210        0.338690         0.299378  0.713936       0.512382\n",
    "11     AE100_1Kd2_RETR  giga_window5-scaled    0.574093  0.602111        0.325602         0.329555  0.751056       0.516483\n",
    "62          AE200_1Kd2  imdb_window5-scaled    0.629625  0.614388        0.300465         0.296204  0.743991       0.516935\n",
    "78       LSA200_PPMID2  imdb_window5-scaled    0.630171  0.618935        0.307187         0.298894  0.740287       0.519095\n",
    "21    LSA200_PPMI_RETR  giga_window5-scaled    0.580715  0.605465        0.353050         0.339956  0.736162       0.523070\n",
    "7      AE100_1Kd1_RETR  giga_window5-scaled    0.566219  0.603120        0.352415         0.338784  0.761193       0.524346\n",
    "26  LSA200_PPMID2_RETR  giga_window5-scaled    0.572713  0.614095        0.348592         0.338870  0.748955       0.524645\n",
    "77   LSA75_PPMID1_RETR  imdb_window5-scaled    0.581110  0.593024        0.381243         0.350897  0.719023       0.525059\n",
    "22  LSA200_PPMID1_RETR  giga_window5-scaled    0.583648  0.612118        0.348962         0.336825  0.752110       0.526732\n",
    "56              C1_150  imdb_window5-scaled    0.623301  0.638265        0.347895         0.306515  0.736579       0.530511\n",
    "57             C1_150r  imdb_window5-scaled    0.624408  0.638717        0.347726         0.307648  0.736912       0.531082\n",
    "6      AE200_1Kd1_RETR  giga_window5-scaled    0.588966  0.621233        0.357107         0.350816  0.756436       0.534911\n",
    "76  LSA200_PPMID1_RETR  imdb_window5-scaled    0.610808  0.590201        0.398355         0.356933  0.721921       0.535644\n",
    "75    LSA200_PPMI_RETR  imdb_window5-scaled    0.599354  0.587413        0.413494         0.364765  0.714033       0.535812\n",
    "10     AE200_1Kd2_RETR  giga_window5-scaled    0.591959  0.633071        0.355606         0.355505  0.760712       0.539371\n",
    "60     AE200_1Kd1_RETR  imdb_window5-scaled    0.615839  0.591141        0.398939         0.367284  0.727692       0.540179\n",
    "61     AE100_1Kd1_RETR  imdb_window5-scaled    0.608664  0.600581        0.404892         0.365140  0.744997       0.544855\n",
    "65     AE100_1Kd2_RETR  imdb_window5-scaled    0.623134  0.627776        0.376672         0.377537  0.747669       0.550558\n",
    "80  LSA200_PPMID2_RETR  imdb_window5-scaled    0.627497  0.617972        0.409345         0.394991  0.746077       0.559176\n",
    "64     AE200_1Kd2_RETR  imdb_window5-scaled    0.640188  0.613962        0.407867         0.399198  0.753825       0.563008     \n",
    "\n",
    "    During the stage of fine-tuning the autoencoder model we decided to increase the number of epochs to 10K and introduce\n",
    "    a early stopping mechanism to the training procedure (evaluate after each 100 epochs, early stop after 5 failed \n",
    "    evaluation). We re-ran the proposed architectures on each available MCO together with the DPPMI-based LSA in order\n",
    "    to obtain the final \"best\" embedding matrix.  Below are the results of this final model search phase:\n",
    "    \n",
    "  MODEL                 DATA  wordsim353  mturk771  simverb3500dev  simverb3500test       men  Macro-average\n",
    "41    LSA200_PPMI_RETR   imdb_window20-flat    0.446412  0.479198        0.315149         0.238742  0.597088       0.415318\n",
    "42  LSA200_PPMID1_RETR   imdb_window20-flat    0.502515  0.497265        0.304745         0.237583  0.633848       0.435191\n",
    "19    LSA200_PPMI_RETR   giga_window20-flat    0.559960  0.517598        0.285164         0.259575  0.659936       0.456446\n",
    "21  LSA200_PPMID2_RETR   giga_window20-flat    0.566863  0.526416        0.296860         0.265071  0.674468       0.465936\n",
    "43  LSA200_PPMID2_RETR   imdb_window20-flat    0.511102  0.532578        0.360753         0.299072  0.652557       0.471212\n",
    "20  LSA200_PPMID1_RETR   giga_window20-flat    0.596064  0.539958        0.304601         0.265470  0.696623       0.480543\n",
    "37     AE200_f3d1_RETR   imdb_window20-flat    0.616646  0.541377        0.326162         0.284005  0.687487       0.491135\n",
    "38     AE100_f3d1_RETR   imdb_window20-flat    0.600432  0.559047        0.326005         0.285873  0.702826       0.494837\n",
    "33     AE200_f2d1_RETR   imdb_window20-flat    0.614167  0.547061        0.347045         0.285870  0.684567       0.495742\n",
    "34     AE100_f2d1_RETR   imdb_window20-flat    0.601524  0.560869        0.325217         0.287026  0.704686       0.495864\n",
    "14     AE100_f2d2_RETR   giga_window20-flat    0.605782  0.580163        0.338330         0.299358  0.726288       0.509984\n",
    "18     AE100_f3d2_RETR   giga_window20-flat    0.615087  0.585906        0.340997         0.297099  0.725216       0.512861\n",
    "16     AE100_f3d1_RETR   giga_window20-flat    0.624479  0.580914        0.330287         0.293817  0.736412       0.513182\n",
    "12     AE100_f2d1_RETR   giga_window20-flat    0.617250  0.588658        0.336470         0.298640  0.738844       0.515972\n",
    "8     LSA200_PPMI_RETR  giga_window5-scaled    0.580715  0.605465        0.353050         0.339956  0.736162       0.523070\n",
    "3      AE100_f2d2_RETR  giga_window5-scaled    0.582202  0.611080        0.332042         0.337763  0.755006       0.523619\n",
    "7      AE100_f3d2_RETR  giga_window5-scaled    0.580033  0.613528        0.335735         0.338676  0.755206       0.524636\n",
    "10  LSA200_PPMID2_RETR  giga_window5-scaled    0.572713  0.614095        0.348592         0.338870  0.748955       0.524645\n",
    "9   LSA200_PPMID1_RETR  giga_window5-scaled    0.583648  0.612118        0.348962         0.336825  0.752110       0.526732\n",
    "36     AE100_f2d2_RETR   imdb_window20-flat    0.617523  0.591198        0.377491         0.332253  0.719064       0.527506\n",
    "39     AE200_f3d2_RETR   imdb_window20-flat    0.622171  0.586796        0.377890         0.345824  0.710781       0.528692\n",
    "17     AE200_f3d2_RETR   giga_window20-flat    0.627954  0.598789        0.358710         0.325629  0.732573       0.528731\n",
    "40     AE100_f3d2_RETR   imdb_window20-flat    0.622195  0.594116        0.381423         0.329926  0.718871       0.529306\n",
    "13     AE200_f2d2_RETR   giga_window20-flat    0.635104  0.598258        0.359671         0.325113  0.728910       0.529411\n",
    "35     AE200_f2d2_RETR   imdb_window20-flat    0.624684  0.587335        0.383164         0.340411  0.716238       0.530366\n",
    "5      AE100_f3d1_RETR  giga_window5-scaled    0.583922  0.611640        0.359178         0.342194  0.763243       0.532036\n",
    "11     AE200_f2d1_RETR   giga_window20-flat    0.635968  0.608072        0.360000         0.319900  0.740668       0.532922\n",
    "1      AE100_f2d1_RETR  giga_window5-scaled    0.581132  0.624879        0.357372         0.343446  0.759918       0.533349\n",
    "15     AE200_f3d1_RETR   giga_window20-flat    0.645774  0.610491        0.356034         0.318727  0.738964       0.533998\n",
    "31  LSA200_PPMID1_RETR  imdb_window5-scaled    0.610808  0.590201        0.398355         0.356933  0.721921       0.535644\n",
    "30    LSA200_PPMI_RETR  imdb_window5-scaled    0.599354  0.587413        0.413494         0.364765  0.714033       0.535812\n",
    "4      AE200_f3d1_RETR  giga_window5-scaled    0.593181  0.631004        0.368145         0.357143  0.758945       0.541684\n",
    "0      AE200_f2d1_RETR  giga_window5-scaled    0.603547  0.634335        0.357063         0.361884  0.761070       0.543580\n",
    "6      AE200_f3d2_RETR  giga_window5-scaled    0.587981  0.641383        0.361646         0.362897  0.772710       0.545323\n",
    "25     AE100_f2d2_RETR  imdb_window5-scaled    0.628521  0.629260        0.362378         0.375002  0.738579       0.546748\n",
    "22     AE200_f2d1_RETR  imdb_window5-scaled    0.625893  0.596149        0.405862         0.377832  0.729325       0.547012\n",
    "2      AE200_f2d2_RETR  giga_window5-scaled    0.601161  0.636068        0.365801         0.372329  0.767164       0.548505\n",
    "27     AE100_f3d1_RETR  imdb_window5-scaled    0.617286  0.599758        0.409597         0.371462  0.749250       0.549471\n",
    "23     AE100_f2d1_RETR  imdb_window5-scaled    0.621731  0.618707        0.404083         0.371269  0.751147       0.553387\n",
    "29     AE100_f3d2_RETR  imdb_window5-scaled    0.642001  0.626371        0.376091         0.378915  0.746271       0.553930\n",
    "32  LSA200_PPMID2_RETR  imdb_window5-scaled    0.627497  0.617972        0.409345         0.394991  0.746077       0.559176\n",
    "26     AE200_f3d1_RETR  imdb_window5-scaled    0.638723  0.619183        0.417913         0.379170  0.744900       0.559978\n",
    "28     AE200_f3d2_RETR  imdb_window5-scaled    0.631439  0.621045        0.409567         0.401222  0.749504       0.562556\n",
    "24     AE200_f2d2_RETR  imdb_window5-scaled    0.648426  0.619787        0.410915         0.401384  0.747792       0.565661    \n",
    "    \n",
    "    \n",
    "    Observation: due to the small size of the MCOs one of the most obvious candidates for best-model: GloVe is easily \n",
    "    outperformed by other methods/models when tested against the validation procedure/datasets.\n",
    "    \n",
    "    Finally, the proposed embeddings matrix is directly generated by a retrofitted autoencoder that outputs from the\n",
    "    encoding layer a embedding of size 300 after it has been trained on the imdb MCO with window size 5 weighted with \n",
    "    DPPMI and reduced to 600 features with LSA. \n",
    "    In order to further enhance the retrofitting mechanism the method proposed by MrkÅ¡iÄ‡ et al was used (based on the\n",
    "    source code available from GitHub repository https://github.com/nmrksic/counter-fitting). \n",
    "    This counter-fitting mecanism was only applied for about ~300 antonym pairs and only after all the previously \n",
    "    mentioned steps (DPPMI-LSA-AE-RETRO).\n",
    "    \"\"\"\n",
    "    \n",
    "##########################################################\n",
    "##########################################################\n",
    "##########################################################\n",
    "##########################################################\n",
    "##########################################################\n",
    "##########################################################\n",
    "        \n",
    "\n",
    "\n",
    "def normalise_word_vectors(word_vectors, norm=1.0):\n",
    "    \"\"\"\n",
    "    This method normalises the collection of word vectors provided in the word_vectors dictionary.\n",
    "    \n",
    "    \n",
    "    adapted from:\n",
    "      https://raw.githubusercontent.com/nmrksic/counter-fitting/master/counterfitting.py\n",
    "    \"\"\"\n",
    "    for word in word_vectors:\n",
    "        word_vectors[word] /= np.sqrt((word_vectors[word]**2).sum() + 1e-6)\n",
    "        word_vectors[word] = word_vectors[word] * norm\n",
    "    return word_vectors\n",
    "\n",
    "def distance(v1, v2, normalised_vectors=True):\n",
    "\t\"\"\"\n",
    "\tReturns the cosine distance between two vectors. \n",
    "\tIf the vectors are normalised, there is no need for the denominator, which is always one. \n",
    "\n",
    "    adapted from:\n",
    "      https://raw.githubusercontent.com/nmrksic/counter-fitting/master/counterfitting.py\n",
    "\t\"\"\"\n",
    "\tif normalised_vectors:\n",
    "\t\treturn 1 - np.dot(v1, v2)\n",
    "\telse:\n",
    "\t\treturn 1 - np.dot(v1, v2) / ( np.linalg.norm(v1) * np.linalg.norm(v2) )\n",
    "\n",
    "\n",
    "def vector_partial_gradient(u, v, normalised_vectors=True):\n",
    "\t\"\"\"\n",
    "\tThis function returns the gradient of cosine distance: \\frac{ \\partial dist(u,v)}{ \\partial u}\n",
    "\tIf they are both of norm 1 (we do full batch and we renormalise at every step), we can save some time.\n",
    "    \n",
    "    adapted from:\n",
    "      https://raw.githubusercontent.com/nmrksic/counter-fitting/master/counterfitting.py\n",
    "\t\"\"\"\n",
    "\n",
    "\tif normalised_vectors:\n",
    "\t\tgradient = u * np.dot(u,v)  - v \n",
    "\telse:\t\t\n",
    "\t\tnorm_u = np.linalg.norm(u)\n",
    "\t\tnorm_v = np.linalg.norm(v)\n",
    "\t\tnominator = u * np.dot(u,v) - v * np.power(norm_u, 2)\n",
    "\t\tdenominator = norm_v * np.power(norm_u, 3)\n",
    "\t\tgradient = nominator / denominator\n",
    "\n",
    "\treturn gradient\n",
    "\n",
    "\n",
    "def one_step_SGD(word_vectors, antonym_pairs,\n",
    "                 delta=1.0, lr=0.1, gamma=0):\n",
    "    \"\"\"\n",
    "    This method performs a step of SGD to optimise the counterfitting cost function.\n",
    "\n",
    "    adapted from:\n",
    "      https://raw.githubusercontent.com/nmrksic/counter-fitting/master/counterfitting.py\n",
    "    \"\"\"\n",
    "    from copy import deepcopy\n",
    "    new_word_vectors = deepcopy(word_vectors)\n",
    "\n",
    "    gradient_updates = {}\n",
    "    update_count = {}\n",
    "\n",
    "    # AR term:\n",
    "    for i, (word_i, word_j) in enumerate(antonym_pairs):\n",
    "        print(\"\\r    Processing antonym pairs {:.1f}%...\".format(i/len(antonym_pairs)*100), flush=True, end='')\n",
    "\n",
    "        current_distance = distance(new_word_vectors[word_i], new_word_vectors[word_j])\n",
    "\n",
    "        if current_distance < delta:\n",
    "    \n",
    "            gradient = vector_partial_gradient( new_word_vectors[word_i], new_word_vectors[word_j])\n",
    "            gradient = gradient * lr \n",
    "\n",
    "            if word_i in gradient_updates:\n",
    "                gradient_updates[word_i] += gradient\n",
    "                update_count[word_i] += 1\n",
    "            else:\n",
    "                gradient_updates[word_i] = gradient\n",
    "                update_count[word_i] = 1\n",
    "\n",
    "   \n",
    "    print(\"\\r    Applying gradients...\", flush=True, end='')\n",
    "    for word in gradient_updates:\n",
    "        # we've found that scaling the update term for each word helps with convergence speed. \n",
    "        update_term = gradient_updates[word] / (update_count[word]) \n",
    "        new_word_vectors[word] += update_term \n",
    "    print(\"\\r    Done Applying gradients.\", flush=True, end='')\n",
    "        \n",
    "    return normalise_word_vectors(new_word_vectors)\n",
    "  \n",
    "  \n",
    "def counter_fit_antonyms(dct_word_vectors,  antonyms, epochs=20, lr=0.1):\n",
    "  \"\"\"\n",
    "  This method repeatedly applies SGD steps to counter-fit word vectors to linguistic constraints. \n",
    "  \n",
    "  https://raw.githubusercontent.com/nmrksic/counter-fitting/master/counterfitting.py\n",
    "  \"\"\"\n",
    "  word_vectors = normalise_word_vectors(dct_word_vectors)\n",
    "  \n",
    "  current_iteration = 0\n",
    "  \n",
    "  \n",
    "  max_iter = epochs\n",
    "  print(\"Antonym pairs:\", len(antonyms), flush=True)\n",
    "  print(\"Running the optimisation procedure for\", max_iter, \"SGD steps...\", flush=True)\n",
    "  \n",
    "  while current_iteration < max_iter:\n",
    "    current_iteration += 1\n",
    "    print(\"\\r  Counter-fitting SGD step {}...\".format(current_iteration), flush=True, end='')\n",
    "    word_vectors = one_step_SGD(word_vectors, antonyms, lr=lr)\n",
    "  print(\"\")\n",
    "  return word_vectors  \n",
    "\n",
    "##################################################        \n",
    "##################################################  \n",
    "  \n",
    "\n",
    "def eval_model(dct, dataset_name, model_name, df, distfunc=vsm.cosine):\n",
    "  print(\"\\nEvaluation of model '{}' {} based on '{}' MCO\".format(\n",
    "        model_name, df.shape, dataset_name),\n",
    "        flush=True)\n",
    "  if 'MODEL' not in dct:\n",
    "    dct['MODEL'] = []\n",
    "  if 'DATA' not in dct:\n",
    "    dct['DATA'] = []\n",
    "\n",
    "  if 'DST' not in dct:\n",
    "    dct['DST'] = []\n",
    "  \n",
    "  dct['DATA'].append(dataset_name)\n",
    "  dct['MODEL'].append(model_name)\n",
    "  dct['DST'].append(distfunc.__name__)\n",
    "  \n",
    "  res = full_word_similarity_evaluation(df, distfunc=distfunc)\n",
    "  print(\"Results for '{}' on data '{}' with distfunc={}:\".format(\n",
    "      model_name, dataset_name, distfunc.__name__), flush=True)\n",
    "  \n",
    "  for key in dict(res):\n",
    "    if key not in dct:\n",
    "      dct[key] = []\n",
    "    dct[key].append(res[key])\n",
    "    \n",
    "  return dct, res['Macro-average']\n",
    "\n",
    "def grid_search_vsm(files, dct_model_funcs):\n",
    "  pd.set_option('display.max_rows', 500)\n",
    "  pd.set_option('display.max_columns', 500)\n",
    "  pd.set_option('display.width', 1000)\n",
    "  \n",
    "  best_model = None\n",
    "  best_macro = 0\n",
    "  best_model_name = ''\n",
    "  dct_res = {}\n",
    "  dist_funcs = [vsm.cosine] # [dice, vsm.cosine]\n",
    "  for distfunc in dist_funcs:\n",
    "    print(\"Using distfunc={}\".format(distfunc.__name__))\n",
    "    for fn in files:\n",
    "      print(\"Loading '{}'\".format(fn), flush=True)\n",
    "      data = pd.read_csv(os.path.join(VSM_HOME, fn), index_col=0)  \n",
    "      print(\"  Loaded {}\".format(data.shape))\n",
    "      data_name = fn[:-7]\n",
    "      for model_name, model_func in dct_model_funcs.items():\n",
    "        print(\"=\" * 70)\n",
    "        print(\"Running '{}' on '{}'\".format(model_name, data_name), flush=True)\n",
    "        df = model_func(data)\n",
    "        if df is None:\n",
    "          print(\"{} returned None\".format(model_func.__name__))\n",
    "          continue\n",
    "        print(\"Done running {}. Obtained df: {}\".format(\n",
    "            model_func.__name__, df.shape), flush=True)\n",
    "        dct_res, macro = eval_model(dct_res, \n",
    "                                    dataset_name=data_name, \n",
    "                                    model_name=model_name, \n",
    "                                    df=df,\n",
    "                                    distfunc=distfunc\n",
    "                                    )\n",
    "        if best_macro < macro:\n",
    "          old_best_file = best_model_name\n",
    "          best_macro = macro\n",
    "          best_model = df\n",
    "          best_model_name = 'best_model_{:.4f}_'.format(macro).replace('.','') + model_name + '_' + fn\n",
    "          print(\"Found new best macro-average: {:.4f}\".format(best_macro), flush=True)\n",
    "          if old_best_file != '':\n",
    "            try:\n",
    "              old_best_file = old_best_file + '.csv.gz'\n",
    "              os.remove(old_best_file)\n",
    "              print(\"Old best '{}' deleted.\".format(old_best_file))\n",
    "            except:\n",
    "              print(\"ERROR: Cound not remove file '{}' !!!\".format(old_best_file))            \n",
    "          best_model.to_csv(best_model_name, compression='gzip')\n",
    "          \n",
    "        df_res = pd.DataFrame(dct_res).sort_values('Macro-average')\n",
    "        df_res.to_csv(\"20200303_test.csv\")\n",
    "        print(\"\\nResults so far:\\n{}\".format(df_res), flush=True)\n",
    "  \n",
    "  return best_model\n",
    "\n",
    "\n",
    "def glove_model(df, n_embeds=75, max_iters=10000, retrofit=False):\n",
    "  from mittens import GloVe\n",
    "  print(\"Computing GloVe model with {} embeds for {} iters\".format(\n",
    "      n_embeds, max_iters), flush=True)\n",
    "  glove_model = GloVe(n=n_embeds, max_iter=max_iters)\n",
    "  np_res = glove_model.fit(df.values)\n",
    "  glove_model.sess.close()\n",
    "  print(\"\", flush=True)\n",
    "  df_res = pd.DataFrame(np_res, index=df.index)\n",
    "  if retrofit:\n",
    "    df_out = retrofit_model(df_res)\n",
    "  else:\n",
    "    df_out = df_res\n",
    "  return df_out\n",
    "\n",
    "\n",
    "\n",
    "def calc_delta(mco):\n",
    "  col_totals = np.array(mco).sum(axis=0)\n",
    "  row_totals = np.array(mco).sum(axis=1)\n",
    "  cm = [col_totals for _ in range(mco.shape[0])]\n",
    "  col_mat = np.vstack(cm)\n",
    "  row_mat = row_totals.reshape((-1,1))\n",
    "  rm = [row_mat for _ in range(mco.shape[1])]\n",
    "  row_mat = np.hstack(rm)\n",
    "  d1 = mco / (mco + 1)\n",
    "  mins = np.minimum(col_mat, row_mat)\n",
    "  d2 = mins / (mins + 1)\n",
    "  delta = d1 * d2\n",
    "  return delta\n",
    "\n",
    "def pmid(m, positive=True, delta_on_pmi=True, before=True):\n",
    "  df = vsm.observed_over_expected(m)\n",
    "  # Silence distracting warnings about log(0):\n",
    "  with np.errstate(divide='ignore'):\n",
    "    pmi = np.log(df)\n",
    "  pmi[np.isinf(pmi)] = 0.0  # log(0) = 0\n",
    "  if positive and before:\n",
    "      pmi[pmi < 0] = 0.0\n",
    "  delta = calc_delta(pmi if delta_on_pmi else m)\n",
    "  pmi = pmi * delta\n",
    "  if positive and not before:\n",
    "    pmi[pmi < 0] = 0.0\n",
    "  return pmi\n",
    "\n",
    "\n",
    "def counterfit_model(data):\n",
    "  print(\"Counter-fitting on {} embeds\".format( data.shape), flush=True)\n",
    "\n",
    "\n",
    "  from nltk.corpus import wordnet as wn\n",
    "  print(\"  Preparing antonyms\", flush=True)\n",
    "  ant_set = set()\n",
    "  for ss in wn.all_synsets():\n",
    "    lema = ss.lemmas()[0]      \n",
    "    w1 = lema.name()\n",
    "    ants = [lem.name() for lem in lema.antonyms()]\n",
    "    if len(ants)>0:\n",
    "      for w2 in ants:\n",
    "        if w1 in data.index and w2 in data.index:\n",
    "          ant_set.add((w1, w2))\n",
    "  \n",
    "    \n",
    "  dct_word_vectors = {k:v for k,v in zip(data.index, data.values)}\n",
    "  for word in ['expensive','east','smart','adult']:\n",
    "    if word in data:\n",
    "      break\n",
    "  neibs1 = vsm.neighbors(word, data)\n",
    "  dct_new_embeds = counter_fit_antonyms(dct_word_vectors, ant_set, lr=0.1, epochs=30)\n",
    "  df = pd.DataFrame.from_dict(dct_new_embeds, orient='index')\n",
    "  neibs = vsm.neighbors(word, df)\n",
    "\n",
    "  print(\"  Status before counter-fitting for word: {}\".format(word), flush=True)\n",
    "  for w in dict(neibs1.iloc[:5]):\n",
    "    print(\"  {:<20} {:.3f}\".format(w+':', neibs1[w]))\n",
    "  print(\"  Status AFTER counter-fitting for word: {}\".format(word), flush=True)\n",
    "  for w in dict(neibs.iloc[:5]):\n",
    "    print(\"  {:<20} {:.3f}\".format(w+':', neibs[w]))\n",
    "  return df\n",
    "  \n",
    "\n",
    "def retrofit_model(data, name=''):\n",
    "  from retrofitting import Retrofitter\n",
    "  print(\"Retrofitting model '{}' with {} embeds\".format(name, data.shape[1]), flush=True)\n",
    "  from nltk.corpus import wordnet as wn\n",
    "  print(\"  Constructing edges for words similarity\", flush=True)\n",
    "  edges = defaultdict(set)\n",
    "  for ss in wn.all_synsets():\n",
    "    lem_names = {lem.name() for lem in ss.lemmas()}\n",
    "    for lem in lem_names:\n",
    "      edges[lem] |= lem_names            \n",
    "  print(\"  Preparing indices...\",flush=True)\n",
    "  lookup = dict(zip(data.index, range(data.shape[0])))\n",
    "  index_edges = defaultdict(set)\n",
    "  for start, finish_nodes in edges.items():\n",
    "      s = lookup.get(start)\n",
    "      if s:\n",
    "          f = {lookup[n] for n in finish_nodes if n in lookup}\n",
    "          if f:\n",
    "              index_edges[s] = f  \n",
    "  \n",
    "  wn_retro = Retrofitter(verbose=True,\n",
    "                         max_iter=1000,\n",
    "                         tol=1e-4,\n",
    "                         )\n",
    "  print(\"  Running retrofitter ...\", flush=True)\n",
    "  retro_result = wn_retro.fit(data, index_edges)\n",
    "  print(\"\")\n",
    "  return retro_result\n",
    "\n",
    "\n",
    "\n",
    "def lsa_model(data, k=100, use_ttest=False, disc_pmi=True, retrofit=False, delta=True):\n",
    "  if use_ttest:\n",
    "    print(\"Computing ttest reweighting for LSA {}\".format(k), flush=True)\n",
    "    lsa_input = ttest(data)\n",
    "  else:\n",
    "    if disc_pmi:\n",
    "      print(\"Computing discounted positive PMI reweighting for LSA {} (delta:{})\".format(\n",
    "          k, delta), flush=True)\n",
    "      lsa_input = pmid(data, delta_on_pmi=delta)\n",
    "    else:\n",
    "      print(\"Computing positive PMI reweighting for LSA {}\".format(k), flush=True)\n",
    "      lsa_input = vsm.pmi(data)    \n",
    "  print(\"Computing LSA k={}...\".format(k))\n",
    "  lsa_output = vsm.lsa(lsa_input, k=k)\n",
    "  if retrofit:\n",
    "    df_out = retrofit_model(lsa_output)\n",
    "  else:\n",
    "    df_out = lsa_output\n",
    "  return df_out\n",
    "\n",
    "def ae_model(data, n_embeds, \n",
    "             distfunc=vsm.cosine,\n",
    "             epochs=100000, \n",
    "             retrofit=True, \n",
    "             delta=True, \n",
    "             max_patience=5, \n",
    "             lr=1e-2,\n",
    "             lsa_factor=2,\n",
    "             end_counterfit=False,\n",
    "             ):\n",
    "  \n",
    "  from torch_autoencoder import TorchAutoencoder\n",
    "  \n",
    "  print(\"Generating autoencoder based model with {} embeds...\".format(n_embeds), flush=True)\n",
    "  lsa_output = lsa_model(data, k=int(n_embeds * lsa_factor), disc_pmi=True, use_ttest=False, delta=delta)\n",
    "\n",
    "  n_step_epochs = 100\n",
    "  steps = epochs // n_step_epochs\n",
    "    \n",
    "  ae_model = TorchAutoencoder(max_iter=n_step_epochs, \n",
    "                              hidden_dim=n_embeds, \n",
    "                              eta=lr,\n",
    "                              warm_start=True)\n",
    "  best_macro = 0\n",
    "  best_model = None\n",
    "  patience = 0\n",
    "  print(\"Performing autoencoder training for {} steps of {} epochs with early stopping on {}...\".format(\n",
    "      steps, n_step_epochs, lsa_output.shape), flush=True)\n",
    "  for step in range(1, steps+1):\n",
    "    print(\"Fitting step {}/{} for {} step-epochs\".format(step, steps, n_step_epochs), flush=True)\n",
    "    ae_output = ae_model.fit(lsa_output)\n",
    "    print(\"\\nCalculating step {} results...\".format(step))\n",
    "    print(\"  Before retrofit ...\")\n",
    "    res = full_word_similarity_evaluation(ae_output, distfunc=distfunc)\n",
    "    mb = res['Macro-average']\n",
    "    if retrofit:\n",
    "      df_out = retrofit_model(ae_output)\n",
    "    else:\n",
    "      df_out = ae_output\n",
    "    res = full_word_similarity_evaluation(df_out, distfunc=distfunc)\n",
    "    macro = res['Macro-average']\n",
    "    print(\"  Before retro: {:.4f}\".format(mb))\n",
    "    print(\"  After retro:  {:.4f}\".format(macro))\n",
    "    print(\"  {}\".format(\"Good!\" if macro>mb else \"WORSE!!!\"), flush=True)\n",
    "    if macro > best_macro:\n",
    "      patience = 0\n",
    "      best_macro = macro\n",
    "      best_model = df_out\n",
    "      print(\"Found best model at epoch {}\".format(step * n_step_epochs), flush=True)\n",
    "    else:\n",
    "      patience += 1\n",
    "      print(\"Macro {:.4f} < {:.4f} best. Patience {}/{}\".format(\n",
    "          macro, best_macro, patience, max_patience))\n",
    "      \n",
    "    if patience >= max_patience:\n",
    "      print(\"Early stopping training loop at step {}\".format(step))\n",
    "      break      \n",
    "  if end_counterfit:\n",
    "    best_model = counterfit_model(best_model)\n",
    "  return best_model\n",
    " \n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off [1 point]\n",
    "\n",
    "For the bake-off, we will release two additional datasets. The announcement will go out on the discussion forum. We will also release reader code for these datasets that you can paste into this notebook. You will evaluate your custom model $M$ (from the previous question) on these new datasets using `full_word_similarity_evaluation`. Rules:\n",
    "\n",
    "1. Only one evaluation is permitted.\n",
    "1. No additional system tuning is permitted once the bake-off has started.\n",
    "\n",
    "The cells below this one constitute your bake-off entry.\n",
    "\n",
    "People who enter will receive the additional homework point, and people whose systems achieve the top score will receive an additional 0.5 points. We will test the top-performing systems ourselves, and only systems for which we can reproduce the reported results will win the extra 0.5 points.\n",
    "\n",
    "Late entries will be accepted, but they cannot earn the extra 0.5 points. Similarly, you cannot win the bake-off unless your homework is submitted on time.\n",
    "\n",
    "The announcement will include the details on where to submit your entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results on the DEV sets using distfunc 'cosine'':\n",
      "wordsim353         0.630593\n",
      "mturk771           0.641565\n",
      "simverb3500dev     0.470225\n",
      "simverb3500test    0.429700\n",
      "men                0.748682\n",
      "Macro-average      0.584153\n",
      "Name: Spearman r, dtype: float64\n",
      "\n",
      "\n",
      "Results on the TEST sets using distfunc 'jaccard':\n",
      "simlex999        0.046431\n",
      "mturk287         0.062449\n",
      "Macro-average    0.054440\n",
      "Name: Spearman r, dtype: float64\n",
      "\n",
      "Final bake-off score: 0.05444045540308437\n"
     ]
    }
   ],
   "source": [
    "# Enter your bake-off assessment code into this cell. \n",
    "# Please do not remove this comment.\n",
    "\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    pass\n",
    "    # Please enter your code in the scope of the above conditional.\n",
    "    ##### YOUR CODE HERE\n",
    "    \n",
    "    # the next section \"explains\" what are the hyperparams that generated the proposed embeddings matrix\n",
    "    # Model preparation\n",
    "    # load data\n",
    "    \"\"\"\n",
    "    mco_data = pd.read_csv(os.path.join(VSM_HOME, \"giga_window20-flat.csv.gz\"), index_col=0)\n",
    "    # train model & get retrofitted embeds\n",
    "    ae_embeds = ae_model(data=mco_data, \n",
    "                         n_embeds=300, \n",
    "                         epochs=100000, \n",
    "                         retrofit=True, \n",
    "                         delta=False,\n",
    "                         max_patience=5,\n",
    "                         end_counterfit=True,\n",
    "                         lsa_factor=2)\n",
    "    # calculate score\n",
    "    res = full_word_similarity_evaluation(ae_embeds)\n",
    "    print(\"Result:\\n{}\".format(res))\n",
    "    m_score = res['Macro-average']\n",
    "    fn = 'best_model_{:.4f}'.format(m_score).replace('.','')+'.csv.gz'\n",
    "    ae_embeds.to_csv(fn,compression='gzip')\n",
    "    print(\"Final embeddings {} result: {:.4f}\".format(ae_embeds.shape, m_score))\n",
    "    \"\"\"\n",
    "    # now we can use \"best_model_05842.csv.gz\"\n",
    "    distfunc = vsm.cosine\n",
    "    # the following file is based on the homework sumission\n",
    "    best_embeds = pd.read_csv(\"best_model_05842.csv.gz\", index_col=0)\n",
    "    res = full_word_similarity_evaluation(best_embeds, distfunc=distfunc)\n",
    "    # the results should match the 0.5842 macro score based on cosine distance\n",
    "    print(\"\\nResults on the DEV sets using distfunc '{}'':\\n{}\".format(distfunc.__name__, res))\n",
    "    \n",
    "    def mturk287_reader():\n",
    "        \"\"\"MTurk-287: http://tx.technion.ac.il/~kirar/Datasets.html\"\"\"\n",
    "        src_filename = os.path.join(\n",
    "            WORDSIM_HOME, 'bakeoff-wordsim-test-data', 'MTurk-287.csv')\n",
    "        return wordsim_dataset_reader(\n",
    "            src_filename, header=False)\n",
    "\n",
    "    def simlex999_reader(wordsim_test_home=WORDSIM_HOME):\n",
    "        \"\"\"SimLex999: https://www.cl.cam.ac.uk/~fh295/SimLex-999.zip\"\"\"\n",
    "        src_filename = os.path.join(\n",
    "            WORDSIM_HOME, 'bakeoff-wordsim-test-data', 'SimLex-999', 'SimLex-999.txt')\n",
    "        return wordsim_dataset_reader(\n",
    "            src_filename, delimiter=\"\\t\", header=True, score_col_index=3)\n",
    "\n",
    "    BAKEOFF = (simlex999_reader, mturk287_reader)\n",
    "    distfunc = vsm.jaccard\n",
    "    res = full_word_similarity_evaluation(best_embeds, readers=BAKEOFF, distfunc=distfunc)\n",
    "    print(\"\\n\\nResults on the TEST sets using distfunc '{}':\\n{}\".format(distfunc.__name__, res))\n",
    "    print(\"\\nFinal bake-off score: {}\".format(res['Macro-average']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On an otherwise blank line in this cell, please enter\n",
    "# your \"Macro-average\" value as reported by the code above. \n",
    "# Please enter only a number between 0 and 1 inclusive.\n",
    "# Please do not remove this comment.\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    pass\n",
    "    # Please enter your score in the scope of the above conditional.\n",
    "    ##### YOUR CODE HERE\n",
    "\n",
    "    0.054440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
