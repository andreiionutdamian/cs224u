{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework and bake-off: word-level entailment with neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2020\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Set-up](#Set-up)\n",
    "1. [Data](#Data)\n",
    "  1. [Edge disjoint](#Edge-disjoint)\n",
    "  1. [Word disjoint](#Word-disjoint)\n",
    "1. [Baseline](#Baseline)\n",
    "  1. [Representing words: vector_func](#Representing-words:-vector_func)\n",
    "  1. [Combining words into inputs: vector_combo_func](#Combining-words-into-inputs:-vector_combo_func)\n",
    "  1. [Classifier model](#Classifier-model)\n",
    "  1. [Baseline results](#Baseline-results)\n",
    "1. [Homework questions](#Homework-questions)\n",
    "  1. [Hypothesis-only baseline [2 points]](#Hypothesis-only-baseline-[2-points])\n",
    "  1. [Alternatives to concatenation [2 points]](#Alternatives-to-concatenation-[2-points])\n",
    "  1. [A deeper network [2 points]](#A-deeper-network-[2-points])\n",
    "  1. [Your original system [3 points]](#Your-original-system-[3-points])\n",
    "1. [Bake-off [1 point]](#Bake-off-[1-point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general problem is word-level natural language inference.\n",
    "\n",
    "Training examples are pairs of words $(w_{L}, w_{R}), y$ with $y = 1$ if $w_{L}$ entails $w_{R}$, otherwise $0$.\n",
    "\n",
    "The homework questions below ask you to define baseline models for this and develop your own system for entry in the bake-off, which will take place on a held-out test-set distributed at the start of the bake-off. (Thus, all the data you have available for development is available for training your final system before the bake-off begins.)\n",
    "\n",
    "<img src=\"fig/wordentail-diagram.png\" width=600 alt=\"wordentail-diagram.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [the first notebook in this unit](nli_01_task_and_data.ipynb) for set-up instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "import nli\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HOME = 'data'\n",
    "\n",
    "NLIDATA_HOME = os.path.join(DATA_HOME, 'nlidata')\n",
    "\n",
    "wordentail_filename = os.path.join(\n",
    "    NLIDATA_HOME, 'nli_wordentail_bakeoff_data.json')\n",
    "\n",
    "GLOVE_HOME = os.path.join(DATA_HOME, 'glove.6B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "I've processed the data into two different train/test splits, in an effort to put some pressure on our models to actually learn these semantic relations, as opposed to exploiting regularities in the sample.\n",
    "\n",
    "* `edge_disjoint`: The `train` and `dev` __edge__ sets are disjoint, but many __words__ appear in both `train` and `dev`.\n",
    "* `word_disjoint`: The `train` and `dev` __vocabularies are disjoint__, and thus the edges are disjoint as well.\n",
    "\n",
    "These are very different problems. For `word_disjoint`, there is real pressure on the model to learn abstract relationships, as opposed to memorizing properties of individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(wordentail_filename) as f:\n",
    "    wordentail_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outer keys are the  splits plus a list giving the vocabulary for the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['edge_disjoint', 'vocab', 'word_disjoint'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordentail_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dev', 'train'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordentail_data['edge_disjoint'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the split looks like; all three have this same format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['sweater', 'stroke'], 0],\n",
       " [['constipation', 'hypovolemia'], 0],\n",
       " [['disease', 'inflammation'], 0],\n",
       " [['herring', 'animal'], 1],\n",
       " [['cauliflower', 'outlook'], 0]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordentail_data['edge_disjoint']['dev'][: 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test to make sure no edges are shared between `train` and `dev`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli.get_edge_overlap_size(wordentail_data, 'edge_disjoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expect, a *lot* of vocabulary items are shared between `train` and `dev`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2916"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli.get_vocab_overlap_size(wordentail_data, 'edge_disjoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a large percentage of the entire vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8470"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordentail_data['vocab'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the distribution of labels in the `train` set. It's highly imbalanced, which will pose a challenge for learning. (I'll go ahead and reveal that the `dev` set is similarly distributed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distribution(split):\n",
    "    return pd.DataFrame(wordentail_data[split]['train'])[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    14650\n",
       "1     2745\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_distribution('edge_disjoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dev', 'train'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordentail_data['word_disjoint'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `word_disjoint` split, no __words__ are shared between `train` and `dev`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli.get_vocab_overlap_size(wordentail_data, 'word_disjoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because no words are shared between `train` and `dev`, no edges are either:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli.get_edge_overlap_size(wordentail_data, 'word_disjoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label distribution is similar to that of `edge_disjoint`, though the overall number of examples is a bit smaller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7199\n",
       "1    1349\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_distribution('word_disjoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even in deep learning, __feature representation is vital and requires care!__ For our task, feature representation has two parts: representing the individual words and combining those representations into a single network input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing words: vector_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider two baseline word representations methods:\n",
    "\n",
    "1. Random vectors (as returned by `utils.randvec`).\n",
    "1. 50-dimensional GloVe representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randvec(w, n=50, lower=-1.0, upper=1.0):\n",
    "    \"\"\"Returns a random vector of length `n`. `w` is ignored.\"\"\"\n",
    "    return utils.randvec(n=n, lower=lower, upper=upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any of the files in glove.6B will work here:\n",
    "\n",
    "glove_dim = 50\n",
    "\n",
    "glove_src = os.path.join(GLOVE_HOME, 'glove.6B.{}d.txt'.format(glove_dim))\n",
    "\n",
    "# Creates a dict mapping strings (words) to GloVe vectors:\n",
    "GLOVE = utils.glove2dict(glove_src)\n",
    "\n",
    "def glove_vec(w):    \n",
    "    \"\"\"Return `w`'s GloVe representation if available, else return \n",
    "    a random vector.\"\"\"\n",
    "    return GLOVE.get(w, randvec(w, n=glove_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining words into inputs: vector_combo_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we decide how to combine the two word vectors into a single representation. In more detail, where `u` is a vector representation of the left word and `v` is a vector representation of the right word, we need a function `vector_combo_func` such that `vector_combo_func(u, v)` returns a new input vector `z` of dimension `m`. A simple example is concatenation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_concatenate(u, v):\n",
    "    \"\"\"Concatenate np.array instances `u` and `v` into a new np.array\"\"\"\n",
    "    return np.concatenate((u, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vector_combo_func` could instead be vector average, vector difference, etc. (even combinations of those) – there's lots of space for experimentation here; [homework question 2](#Alternatives-to-concatenation-[1-point]) below pushes you to do some exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier model\n",
    "\n",
    "For a baseline model, I chose `TorchShallowNeuralClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TorchShallowNeuralClassifier(hidden_dim=50, max_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline results\n",
    "\n",
    "The following puts the above pieces together, using `vector_func=glove_vec`, since `vector_func=randvec` seems so hopelessly misguided for `word_disjoint`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model Sequential:\n",
      "       Sequential(\n",
      "        (0): Linear(in_features=100, out_features=50, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=50, out_features=2, bias=True)\n",
      "      )\n",
      "Finished epoch 100 of 100; error is 0.0256              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.915     0.938     0.926      1910\n",
      "           1      0.380     0.305     0.339       239\n",
      "\n",
      "    accuracy                          0.867      2149\n",
      "   macro avg      0.648     0.622     0.633      2149\n",
      "weighted avg      0.856     0.867     0.861      2149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_disjoint_experiment = nli.wordentail_experiment(\n",
    "    train_data=wordentail_data['word_disjoint']['train'],\n",
    "    assess_data=wordentail_data['word_disjoint']['dev'], \n",
    "    model=net, \n",
    "    vector_func=glove_vec,\n",
    "    vector_combo_func=vec_concatenate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework questions\n",
    "\n",
    "Please embed your homework responses in this notebook, and do not delete any cells from the notebook. (You are free to add as many cells as you like as part of your responses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis-only baseline [2 points]\n",
    "\n",
    "During our discussion of SNLI and MultiNLI, we noted that a number of research teams have shown that hypothesis-only baselines for NLI tasks can be remarkably robust. This question asks you to explore briefly how this baseline effects the 'edge_disjoint' and 'word_disjoint' versions of our task.\n",
    "\n",
    "For this problem, submit two functions:\n",
    "\n",
    "1. A `vector_combo_func` function called `hypothesis_only` that simply throws away the premise, using the unmodified hypothesis (second) vector as its representation of the example.\n",
    "\n",
    "1. A function called `run_hypothesis_only_evaluation` that does the following:\n",
    "    1. Loops over the two conditions 'word_disjoint' and 'edge_disjoint' and the two `vector_combo_func` values `vec_concatenate` and `hypothesis_only`, calling `nli.wordentail_experiment` to train on the conditions 'train' portion and assess on its 'dev' portion, with `glove_vec` as the `vector_func`. So that the results are consistent, use an `sklearn.linear_model.LogisticRegression` with default parameters as the model.\n",
    "    1. Returns a `dict` mapping `(condition_name, function_name)` pairs to the 'macro-F1' score for that pair, as returned by the call to `nli.wordentail_experiment`. (Tip: you can get the `str` name of your function `hypothesis_only` with `hypothesis_only.__name__`.)\n",
    "    \n",
    "The test functions `test_hypothesis_only` and `test_run_hypothesis_only_evaluation` will help ensure that your functions have the desired logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def hypothesis_only(u, v):\n",
    "    return v\n",
    "\n",
    "\n",
    "\n",
    "def run_hypothesis_only_evaluation(include_shallow=False):\n",
    "    ##### YOUR CODE HERE\n",
    "    results = {}\n",
    "    model_facts = [lambda: LogisticRegression()]\n",
    "    if include_shallow:\n",
    "        model_facts.append(lambda: TorchShallowNeuralClassifier(hidden_dim=50, max_iter=100))\n",
    "    for condition in [ 'word_disjoint', 'edge_disjoint']:\n",
    "        for vector_combo_func in [vec_concatenate, hypothesis_only]:\n",
    "            for model_fact in model_facts:\n",
    "                model = model_fact()\n",
    "                iter_name = (condition, vector_combo_func.__name__) \n",
    "                if include_shallow:\n",
    "                    iter_name = iter_name + (model.__class__.__name__,)\n",
    "                print(\"Running {}\".format(iter_name))\n",
    "                exp = nli.wordentail_experiment(\n",
    "                    train_data=wordentail_data[condition]['train'],\n",
    "                    assess_data=wordentail_data[condition]['dev'], \n",
    "                    model=model, \n",
    "                    vector_func=glove_vec,\n",
    "                    vector_combo_func=vector_combo_func\n",
    "                )   \n",
    "                results[iter_name] = exp['macro-F1']\n",
    "    pres = []\n",
    "    for itr in results: \n",
    "        pres.append((\"{:<15}\".format(str(itr)+':'), results[itr]))\n",
    "    pres = sorted(pres, key=lambda x:x[1])\n",
    "    for r in pres: print(r[0], r[1])\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hypothesis_only(hypothesis_only):\n",
    "    v = hypothesis_only(1, 2)\n",
    "    assert v == 2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hypothesis_only(hypothesis_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run_hypothesis_only_evaluation(run_hypothesis_only_evaluation):\n",
    "    results = run_hypothesis_only_evaluation()\n",
    "    assert ('word_disjoint', 'vec_concatenate') in results, \\\n",
    "        \"The return value of `run_hypothesis_only_evaluation` does not have the intended kind of keys\"\n",
    "    assert isinstance(results[('word_disjoint', 'vec_concatenate')], float), \\\n",
    "        \"The values of the `run_hypothesis_only_evaluation` result should be floats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ('word_disjoint', 'vec_concatenate')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.901     0.979     0.938      1910\n",
      "           1      0.446     0.138     0.211       239\n",
      "\n",
      "    accuracy                          0.885      2149\n",
      "   macro avg      0.673     0.558     0.574      2149\n",
      "weighted avg      0.850     0.885     0.857      2149\n",
      "\n",
      "Running ('word_disjoint', 'hypothesis_only')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.893     0.988     0.938      1910\n",
      "           1      0.343     0.050     0.088       239\n",
      "\n",
      "    accuracy                          0.884      2149\n",
      "   macro avg      0.618     0.519     0.513      2149\n",
      "weighted avg      0.831     0.884     0.843      2149\n",
      "\n",
      "Running ('edge_disjoint', 'vec_concatenate')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.875     0.970     0.920      7376\n",
      "           1      0.578     0.227     0.326      1321\n",
      "\n",
      "    accuracy                          0.857      8697\n",
      "   macro avg      0.727     0.599     0.623      8697\n",
      "weighted avg      0.830     0.857     0.830      8697\n",
      "\n",
      "Running ('edge_disjoint', 'hypothesis_only')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.872     0.975     0.921      7376\n",
      "           1      0.590     0.198     0.296      1321\n",
      "\n",
      "    accuracy                          0.857      8697\n",
      "   macro avg      0.731     0.587     0.608      8697\n",
      "weighted avg      0.829     0.857     0.826      8697\n",
      "\n",
      "('word_disjoint', 'hypothesis_only'): 0.5127320021476978\n",
      "('word_disjoint', 'vec_concatenate'): 0.5744400928401634\n",
      "('edge_disjoint', 'hypothesis_only'): 0.6083463518925352\n",
      "('edge_disjoint', 'vec_concatenate'): 0.6231823492908297\n"
     ]
    }
   ],
   "source": [
    "test_run_hypothesis_only_evaluation(run_hypothesis_only_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#run_hypothesis_only_evaluation(include_shallow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatives to concatenation [2 points]\n",
    "\n",
    "We've so far just used vector concatenation to represent the premise and hypothesis words. This question asks you to explore two simple alternative:\n",
    "\n",
    "1. Write a function `vec_diff` that, for a given pair of vector inputs `u` and `v`, returns the element-wise difference between `u` and `v`.\n",
    "\n",
    "1. Write a function `vec_max` that, for a given pair of vector inputs `u` and `v`, returns the element-wise max values between `u` and `v`.\n",
    "\n",
    "You needn't include your uses of `nli.wordentail_experiment` with these functions, but we assume you'll be curious to see how they do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_diff(u, v):\n",
    "    return u-v\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def vec_max(u, v):\n",
    "    return np.maximum(u,v)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vec_diff(vec_diff):\n",
    "    u = np.array([10.2, 8.1])\n",
    "    v = np.array([1.2, -7.1])\n",
    "    result = vec_diff(u, v)\n",
    "    expected = np.array([9.0, 15.2])\n",
    "    assert np.array_equal(result, expected), \\\n",
    "        \"Expected {}; got {}\".format(expected, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec_diff(vec_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vec_max(vec_max):\n",
    "    u = np.array([1.2,  8.1])\n",
    "    v = np.array([10.2, -7.1])\n",
    "    result = vec_max(u, v)\n",
    "    expected = np.array([10.2, 8.1])\n",
    "    assert np.array_equal(result, expected), \\\n",
    "        \"Expected {}; got {}\".format(expected, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec_max(vec_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A deeper network [2 points]\n",
    "\n",
    "It is very easy to subclass `TorchShallowNeuralClassifier` if all you want to do is change the network graph: all you have to do is write a new `define_graph`. If your graph has new arguments that the user might want to set, then you should also redefine `__init__` so that these values are accepted and set as attributes.\n",
    "\n",
    "For this question, please subclass `TorchShallowNeuralClassifier` so that it defines the following graph:\n",
    "\n",
    "$$\\begin{align}\n",
    "h_{1} &= xW_{1} + b_{1} \\\\\n",
    "r_{1} &= \\textbf{Bernoulli}(1 - \\textbf{dropout\\_prob}, n) \\\\\n",
    "d_{1} &= r_1 * h_{1} \\\\\n",
    "h_{2} &= f(d_{1}) \\\\\n",
    "h_{3} &= h_{2}W_{2} + b_{2}\n",
    "\\end{align}$$\n",
    "\n",
    "Here, $r_{1}$ and $d_{1}$ define a dropout layer: $r_{1}$ is a random binary vector of dimension $n$, where the probability of a value being $1$ is given by $1 - \\textbf{dropout_prob}$. $r_{1}$ is multiplied element-wise by our first hidden representation, thereby zeroing out some of the values. The result is fed to the user's activation function $f$, and the result of that is fed through another linear layer to produce $h_{3}$. (Inside `TorchShallowNeuralClassifier`, $h_{3}$ is the basis for a softmax classifier, so no activation function is applied to it.)\n",
    "\n",
    "For your implementation, please use `nn.Sequential`, `nn.Linear`, and `nn.Dropout` to define the required layers.\n",
    "\n",
    "For comparison, using this notation, `TorchShallowNeuralClassifier` defines the following graph:\n",
    "\n",
    "$$\\begin{align}\n",
    "h_{1} &= xW_{1} + b_{1} \\\\\n",
    "h_{2} &= f(h_{1}) \\\\\n",
    "h_{3} &= h_{2}W_{2} + b_{2}\n",
    "\\end{align}$$\n",
    "\n",
    "The following code starts this sub-class for you, so that you can concentrate on `define_graph`. Be sure to make use of `self.dropout_prob`\n",
    "\n",
    "For this problem, submit just your completed  `TorchDeepNeuralClassifier`. You needn't evaluate it, though we assume you will be keen to do that!\n",
    "\n",
    "You can use `test_TorchDeepNeuralClassifier` to ensure that your network has the intended structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TorchDeepNeuralClassifier(TorchShallowNeuralClassifier):\n",
    "    def __init__(self, dropout_prob=0.7, **kwargs):\n",
    "        self.dropout_prob = dropout_prob\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def define_graph(self):\n",
    "        \"\"\"Complete this method!\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        an `nn.Module` instance, which can be a free-standing class you \n",
    "        write yourself, as in `torch_rnn_classifier`, or the outpiut of \n",
    "        `nn.Sequential`, as in `torch_shallow_neural_classifier`.\n",
    "        \n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "            self.hidden_activation,\n",
    "            nn.Linear(self.hidden_dim, self.n_classes_)\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "##### YOUR CODE HERE    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_TorchDeepNeuralClassifier(TorchDeepNeuralClassifier):\n",
    "    dropout_prob = 0.55\n",
    "    assert hasattr(TorchDeepNeuralClassifier(), \"dropout_prob\"), \\\n",
    "        \"TorchDeepNeuralClassifier must have an attribute `dropout_prob`.\"\n",
    "    try:\n",
    "        inst = TorchDeepNeuralClassifier(dropout_prob=dropout_prob)\n",
    "    except TypeError:\n",
    "        raise TypeError(\"TorchDeepNeuralClassifier must allow the user \"\n",
    "                        \"to set `dropout_prob` on initialization\")\n",
    "    inst.input_dim = 10\n",
    "    inst.n_classes_ = 5\n",
    "    graph = inst.define_graph()\n",
    "    assert len(graph) == 4, \\\n",
    "        \"The graph should have 4 layers; yours has {}\".format(len(graph))    \n",
    "    expected = {\n",
    "        0: 'Linear',\n",
    "        1: 'Dropout',\n",
    "        2: 'Tanh',\n",
    "        3: 'Linear'}\n",
    "    for i, label in expected.items():\n",
    "        name = graph[i].__class__.__name__\n",
    "        assert label in name, \\\n",
    "            \"The {} layer of the graph should be a {} layer; yours is {}\".format(i, label, name)\n",
    "    assert graph[1].p == dropout_prob, \\\n",
    "        \"The user's value for `dropout_prob` should be the value of `p` for the Dropout layer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_TorchDeepNeuralClassifier(TorchDeepNeuralClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your original system [3 points]\n",
    "\n",
    "This is a simple dataset, but our focus on the 'word_disjoint' condition ensures that it's a challenging one, and there are lots of modeling strategies one might adopt. \n",
    "\n",
    "You are free to do whatever you like. We require only that your system differ in some way from those defined in the preceding questions. They don't have to be completely different, though. For example, you might want to stick with the model but represent examples differently, or the reverse.\n",
    "\n",
    "Keep in mind that, for the bake-off evaluation, the 'edge_disjoint' portions of the data are off limits. You can, though, train on the combination of the 'word_disjoint' 'train' and 'dev' portions. You are free to use different pretrained word vectors and the like. Please do not introduce additional entailment datasets into your training data, though.\n",
    "\n",
    "Please embed your code in this notebook so that we can rerun it.\n",
    "\n",
    "In the cell below, please provide a brief technical description of your original system, so that the teaching team can gain an understanding of what it does. This will help us to understand your code and analyze all the submissions to identify patterns and strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your system description in this cell.\n",
    "# Please do not remove this comment.\n",
    "__description__ = \"\"\"\n",
    "\n",
    "Solution\n",
    "\n",
    "For this particular task two major directions have been identified: that of employing a MLP on a particular \n",
    "representation of the input (u,v) tuple or that of creating architectures based on siamese networks. Although the \n",
    "MLP approach seems more direct the main area that has been explored is that of simease-based architectures. Random grid\n",
    "search was used on various architectures. \n",
    "\n",
    "More information regarding the architectural details are in the following markup cells followed by the full source code.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Process overview\n",
    "\n",
    "The overall approach was based on three steps:\n",
    "\n",
    "(1) define the baselines - see `get_baselines` function - that have both the above mentioned simple MLP approach \n",
    "as well as logistic regression \n",
    "\n",
    "(2) prepare a overall model architecture `ThWordEntailModel` incapsulated in `WordEntailClassifier` model wrapper that \n",
    "would allow us to experiment with classic siamese networks approaches such as \"Dimensionality Reduction by Learning \n",
    "an Invariant Mapping\" or \"Siamese Neural Networks for One-shot Image Recognition\" as well as approaches based on a \n",
    "graphs architecture with two-column separate encoders and a final classification MLP backbone.\n",
    "\n",
    "(3) prepare a grid search strategy that would cycle most of the major architectures proposed by (2) - this particular \n",
    "feat is provided by `run_grid_search` function\n",
    "\n",
    "(4) run and refine the random grid search and add features and/or new approaches. Two separated grids have been defined\n",
    "anc concatenated - one for the non-contrastive-loss graphs and another for the ones based on contrastive loss\n",
    "\n",
    "(5) select best architecture and re-run the experiment (including ensembles)\n",
    "\n",
    "## Experimentation details\n",
    "\n",
    "The actual training procedure was based on early stopping combined with efficient learning rate decay (that includes\n",
    "parameter/optimizer restoration). The early stopping mechanism used the proposed `dev_data`. Due to class imbalance and \n",
    "to the model tendency to favor \"not entail\", focal loss has been added in the experiment as a grid-search option vs standard\n",
    "BCE. To further test the model capabilities 4 different datasets have been extracted from the original two datasets (train \n",
    "and dev): a train dataset where all words are contained by the  pretrained GloVe, a similar dev set and a train/dev couple\n",
    "where either one or both words in each observation are not contained within the pretrained GloVe embeddings (see the\n",
    "function `test_glove_vs_data`).\n",
    "\n",
    "#### Word vectorizer and OOVs\n",
    "\n",
    "Due to the fact that both training and validation (as well as real-life) datasets have out-of-vocabulary words (out of the \n",
    "pretrained GloVe vocab) a \"approaximator\" function has been developed that generates word embeddings for the OOV words.\n",
    "Below we can see a few examples of word-embeddings neighbors obtained based on the generated embedding (value is cosine\n",
    "distance):\n",
    "\n",
    "```\n",
    "    replacement for 'unwrought'\n",
    "    wrought        1.1327e-10\n",
    "    devastation    4.6164e-01\n",
    "    ironwork       4.7856e-01\n",
    "    wreaked        4.8097e-01\n",
    "    railings       5.1378e-01\n",
    "\n",
    "\n",
    "    replacement for 'haematemesis'\n",
    "    emesis                  1.3017e-10\n",
    "    diaphoresis             5.0699e-01\n",
    "    itraconazole            5.4648e-01\n",
    "    chemotherapy-induced    5.5422e-01\n",
    "    anovulation             5.5664e-01\n",
    "\n",
    "    replacement for 'pyrexia'\n",
    "    pyrex           9.7854e-11\n",
    "    corningware     4.8760e-01\n",
    "    borosilicate    5.2159e-01\n",
    "    bakeware        5.2440e-01\n",
    "    glassware       5.3898e-01\n",
    "\n",
    "    replacement for 'cervicitis'\n",
    "    cervi         1.0381e-10\n",
    "    severini      5.9782e-01\n",
    "    pasqualino    6.0177e-01\n",
    "    kalantar      6.0200e-01\n",
    "    conason       6.0281e-01\n",
    "\n",
    "    replacement for 'dacryocystitis'\n",
    "    cystitis          1.0738e-10\n",
    "    interstitial      4.8357e-01\n",
    "    pyelonephritis    5.2237e-01\n",
    "    endometriosis     5.3036e-01\n",
    "    bronchiolitis     5.4040e-01\n",
    "\n",
    "    replacement for 'antheridium'\n",
    "    anther      1.4220e-10\n",
    "    stamen      5.4327e-01\n",
    "    anthers     5.6385e-01\n",
    "    piasters    5.9172e-01\n",
    "    pistil      6.0421e-01\n",
    "```\n",
    "\n",
    "### The classifiers architectures\n",
    "\n",
    "Th main paramaters of the `WordEntailClassifier` model wrapper are the following:\n",
    "\n",
    "```\n",
    "   model_name,  # model name\n",
    "   siam_lyrs,   # layers of the siamese or the individual word encoders\n",
    "   s_l2,        # applycation of l2 on siamese/paths\n",
    "   s_bn,        # BN in siams/paths\n",
    "   c_act,       # apply activation on siam/paths combiner\n",
    "   separ,       # use separate paths for each word\n",
    "   layers,      # layers of the final classifier dnn\n",
    "   inp_drp,     # apply drop on inputs\n",
    "   o_drp,       # apply drop on each fc\n",
    "   bn,          # apply BN on each liniar in final dnn\n",
    "   bn_inp,      # apply BN on inputs\n",
    "   activ,       # activation (all)\n",
    "   x_dev,       # x_dev for early stop w. lr decay\n",
    "   y_dev,       # y_dev for early stop w. lr decay\n",
    "   s_comb,      # method for combining siams/paths\n",
    "   rev,         # reverse targets during training/predict\n",
    "   lr,          # starting lr\n",
    "   loss,        # loss function name for CL/BCE/FL\n",
    "   bal,         # apply sample balancing during training\n",
    "\n",
    "   cl_m=1,        # if using CL this is margin\n",
    "   fl_a=4,        # focal loss weighting\n",
    "   fl_g=2,        # focal loss discount exponent\n",
    "   lr_decay=0.5,  # lr decay factor\n",
    "   batch=256,     # batch size\n",
    "   l2_strength=0, # l2 weight decay\n",
    "   max_epochs=10000,  # not really used\n",
    "   max_patience=10,   # maximum patience before reload & lr decay  \n",
    "   max_fails=40,      # max consecutive fails before stop\n",
    "```\n",
    "\n",
    "To further detail the different categories of models used in the experiments below we have three examples (to have clear\n",
    "self-explanatory models each *operation was encapsulated within a module*):\n",
    "\n",
    "#### Model type #1: Identical word encoders followed by a classifier\n",
    "```\n",
    "  ThWordEntailModel(\n",
    "    (siam_layers): ModuleList(\n",
    "      (0): InputPlaceholder(input_dim=300)\n",
    "      (1): Linear(in_features=300, out_features=256, bias=True)\n",
    "      (2): Tanh()\n",
    "      (3): Dropout(p=0.2, inplace=False)\n",
    "      (4): Linear(in_features=256, out_features=128, bias=True)\n",
    "      (5): Tanh()\n",
    "      (6): Dropout(p=0.2, inplace=False)\n",
    "      (7): Linear(in_features=128, out_features=64, bias=True)\n",
    "      (8): L2_Normalizer()\n",
    "    )\n",
    "    (post_layers): ModuleList(\n",
    "      (0): PathsCombiner(input_dim=64x2, output_dim=64, method='sqr', act=None)\n",
    "      (1): Linear(in_features=64, out_features=128, bias=False)\n",
    "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (3): Tanh()\n",
    "      (4): Dropout(p=0.2, inplace=False)\n",
    "      (5): Linear(in_features=128, out_features=32, bias=False)\n",
    "      (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (7): Tanh()\n",
    "      (8): Dropout(p=0.2, inplace=False)\n",
    "      (9): Linear(in_features=32, out_features=1, bias=True)\n",
    "    )\n",
    "  )\n",
    "  Loss: FocalLoss(alpha=4, gamma=2)\n",
    "```  \n",
    "In the above case we have either focal loss or the standard BCE.\n",
    "  \n",
    "#### Model type #2: Siamese encoders followed by euclidean distance and contrastive loss\n",
    "```\n",
    "  ThWordEntailModel(\n",
    "    (siam_layers): ModuleList(\n",
    "      (0): InputPlaceholder(input_dim=300)\n",
    "      (1): Linear(in_features=300, out_features=512, bias=False)\n",
    "      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (3): ReLU()\n",
    "      (4): Dropout(p=0.5, inplace=False)\n",
    "      (5): Linear(in_features=512, out_features=256, bias=False)\n",
    "      (6): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (7): L2_Normalizer()\n",
    "    )\n",
    "    (post_layers): ModuleList(\n",
    "      (0): PathsCombiner(input_dim=256x2, output_dim=1, method='eucl', act=None)\n",
    "    )\n",
    "  )\n",
    "  Loss: ConstrativeLoss(margin=1)  \n",
    "```  \n",
    "\n",
    "#### Model type #3: Separated word encoders that are combined and passed throgh the final dnn classifier\n",
    "```\n",
    "  ThWordEntailModel(\n",
    "    (path1_layers): ModuleList(\n",
    "      (0): InputPlaceholder(input_dim=300)\n",
    "      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (2): Linear(in_features=300, out_features=128, bias=True)\n",
    "      (3): L2_Normalizer()\n",
    "    )\n",
    "    (path2_layers): ModuleList(\n",
    "      (0): InputPlaceholder(input_dim=300)\n",
    "      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (2): Linear(in_features=300, out_features=128, bias=True)\n",
    "      (3): L2_Normalizer()\n",
    "    )\n",
    "    (post_layers): ModuleList(\n",
    "      (0): PathsCombiner(input_dim=128x2, output_dim=128, method='add', act=None)\n",
    "      (1): Linear(in_features=128, out_features=256, bias=True)\n",
    "      (2): SELU()\n",
    "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
    "      (4): SELU()\n",
    "      (5): Linear(in_features=128, out_features=64, bias=True)\n",
    "      (6): SELU()\n",
    "      (7): Linear(in_features=64, out_features=1, bias=True)\n",
    "    )\n",
    "  )\n",
    "  Loss: FocalLoss(alpha=4, gamma=2)\n",
    "```\n",
    "\n",
    "Please note that all above modules such as `ConstrativeLoss`, `FocalLoss`, `PathsCombiner`, `L2_Normalizer`, `InputPlaceholder` as well as the whole `ThWordEntailModel` can be reviewed in the following sections.\n",
    "\n",
    "### About vector combining\n",
    "\n",
    "In order to simplify the grid searching process we decided to use a single `vector_combine_function` that just pairs \n",
    "the words in each observation. This allows us to experiment a wide variate of combine functions at the level of of the\n",
    "`PathsCombiner`. For the classic case of not havin any kind of individual word re-encoder we have the following example:\n",
    "\n",
    "```\n",
    "  ThWordEntailModel(\n",
    "    (path1_layers): ModuleList(\n",
    "      (0): InputPlaceholder(input_dim=300)\n",
    "      (1): Dropout(p=0.3, inplace=False)\n",
    "    )\n",
    "    (path2_layers): ModuleList(\n",
    "      (0): InputPlaceholder(input_dim=300)\n",
    "      (1): Dropout(p=0.3, inplace=False)\n",
    "    )\n",
    "    (post_layers): ModuleList(\n",
    "      (0): PathsCombiner(input_dim=300x2, output_dim=600, method='cat', act=None)\n",
    "      (1): Linear(in_features=600, out_features=256, bias=False)\n",
    "      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (3): Sigmoid()\n",
    "      (4): Linear(in_features=256, out_features=128, bias=False)\n",
    "      (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (6): Sigmoid()\n",
    "      (7): Linear(in_features=128, out_features=64, bias=False)\n",
    "      (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (9): Sigmoid()\n",
    "      (10): Linear(in_features=64, out_features=1, bias=True)\n",
    "    )\n",
    "  )\n",
    "  Loss: BCEWithLogitsLoss()\n",
    "```\n",
    "In the above example the GloVe-300 embeddings are basically concatenated after a 30% dropout has been applied.\n",
    "\n",
    "## The results\n",
    "\n",
    "After multiple random grid search iterations we obtained a reduced list of classifier candidates. Another confirmed \n",
    "hypotesys was the fact that ensemble models outperform simple models as shown by the following results table:\n",
    "\n",
    "```\n",
    "Results:\n",
    "             MODEL    SCORE   pos_F1  pos_Rec  pos_Pre         vf\n",
    "10        H3v3_020  72.4773  51.4735  54.8117  48.5185  l_glv_rep\n",
    "8         H3v2_321  72.8769  52.9101  62.7615  45.7317  l_glv_rep\n",
    "4         H3v3_015  73.0251  54.1213  72.8033  43.0693      l_glv\n",
    "172  E_424_335_020  73.0790  51.3317  44.3515  60.9195  l_glv_rep\n",
    "9         H3v2_335  73.1663  52.3013  52.3013  52.3013  l_glv_rep\n",
    "171  E_424_335_020  73.2309  51.5815  44.3515  61.6279      l_glv\n",
    "1         H3v2_424  73.2915  52.2876  50.2092  54.5455      l_glv\n",
    "158  E_424_197_335  73.3479  51.9048  45.6067  60.2210  l_glv_rep\n",
    "157  E_424_197_335  73.4992  52.1531  45.6067  60.8939      l_glv\n",
    "0         H3v2_197  73.5365  53.1440  54.8117  51.5748      l_glv\n",
    "164  E_424_396_335  73.6420  52.5581  47.2803  59.1623  l_glv_rep\n",
    "159  E_424_197_020  73.6420  52.5581  47.2803  59.1623      l_glv\n",
    "2         H3v2_500  73.6656  53.0172  51.4644  54.6667      l_glv\n",
    "163  E_424_396_335  73.7169  52.6807  47.2803  59.4737      l_glv\n",
    "135  E_424_015_396  74.2958  54.3568  54.8117  53.9095      l_glv\n",
    "5         H3v3_171  74.3145  54.5455  56.4854  52.7344      l_glv\n",
    "30   E_197_500_017  74.3241  53.8813  49.3724  59.2965  l_glv_rep\n",
    "43   E_197_500_020  74.3399  53.6232  46.4435  63.4286      l_glv\n",
    "6         H3v3_197  74.4320  54.6939  56.0669  53.3865      l_glv\n",
    "146  E_424_171_396  74.4381  54.3478  52.3013  56.5611  l_glv_rep\n",
    "113  E_424_500_335  74.4446  53.6341  44.7699  66.8750      l_glv\n",
    "180  E_500_017_396  74.4464  54.0416  48.9540  60.3093  l_glv_rep\n",
    "335  E_396_321_020  74.4640  54.9407  58.1590  52.0599      l_glv\n",
    "7         H3v3_396  74.4640  54.9407  58.1590  52.0599      l_glv\n",
    "15   E_197_424_015  74.4870  54.5064  53.1381  55.9471      l_glv\n",
    "339  E_321_335_020  74.6320  55.1308  57.3222  53.1008      l_glv\n",
    "154  E_424_197_396  74.6333  54.6256  51.8828  57.6744  l_glv_rep\n",
    "289  E_015_396_321  74.6549  55.5133  61.0879  50.8711      l_glv\n",
    "3         H3v3_017  74.6551  54.8180  53.5565  56.1404      l_glv\n",
    "284  E_015_197_321  74.6707  55.5766  61.5063  50.6897  l_glv_rep\n",
    "340  E_321_335_020  74.7015  55.2419  57.3222  53.3074  l_glv_rep\n",
    "151  E_424_171_020  74.7068  54.6275  50.6276  59.3137      l_glv\n",
    "290  E_015_396_321  74.7375  55.6818  61.5063  50.8651  l_glv_rep\n",
    "131  E_424_015_171  74.7425  55.1148  55.2301  55.0000      l_glv\n",
    "47   E_197_017_171  77.4752  59.5937  55.2301  64.7059      l_glv\n",
    "218  E_500_396_321  77.5012  59.7345  56.4854  63.3803  l_glv_rep\n",
    "112  E_424_500_321  77.5472  59.6811  54.8117  65.5000  l_glv_rep\n",
    "245  E_017_171_321  77.5506  60.0423  59.4142  60.6838      l_glv\n",
    "217  E_500_396_321  77.5810  59.8670  56.4854  63.6792      l_glv\n",
    "246  E_017_171_321  77.6334  60.2105  59.8326  60.5932  l_glv_rep\n",
    "242  E_017_171_197  77.6374  59.8639  55.2301  65.3465  l_glv_rep\n",
    "241  E_017_171_197  77.7190  60.0000  55.2301  65.6716      l_glv\n",
    "229  E_017_015_171  77.7833  60.4255  59.4142  61.4719      l_glv\n",
    "240  E_017_015_020  78.0222  60.8511  59.8326  61.9048  l_glv_rep\n",
    "204  E_500_171_321  78.1480  60.8108  56.4854  65.8537  l_glv_rep\n",
    "203  E_500_171_321  78.1480  60.8108  56.4854  65.8537      l_glv\n",
    "239  E_017_015_020  78.2611  61.2766  60.2510  62.3377      l_glv\n",
    "```\n",
    "\n",
    "##### The following cell presents the utility functions used by the the main building blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "####                                                                       ####\n",
    "####                       Utility code section                            ####\n",
    "####                                                                       ####\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "import torch as th\n",
    "from datetime import datetime as dt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import OrderedDict\n",
    "from time import time\n",
    "import textwrap\n",
    "from sklearn.metrics import classification_report\n",
    "from itertools import combinations\n",
    "import vsm\n",
    "\n",
    "lst_log = []\n",
    "_date = dt.now().strftime(\"%Y%m%d_%H%M\")\n",
    "log_fn = dt.now().strftime(\"logs/\"+_date+\"_log.txt\")\n",
    "\n",
    "def P(s=''):\n",
    "  lst_log.append(s)\n",
    "  print(s, flush=True)\n",
    "  try:\n",
    "    with open(log_fn, 'w') as f:\n",
    "      for item in lst_log:\n",
    "        f.write(\"{}\\n\".format(item))\n",
    "  except:\n",
    "    pass\n",
    "  return\n",
    "\n",
    "def Pr(s=''):\n",
    "  print('\\r' + str(s), end='', flush=True)\n",
    "\n",
    "\n",
    "def get_object_params(obj, n=None):\n",
    "  \"\"\"\n",
    "  Parameters\n",
    "  ----------\n",
    "  obj : any type\n",
    "    the inspected object.\n",
    "  n : int, optional\n",
    "    the number of params that are returned. The default is None\n",
    "    (all params returned).\n",
    "  Returns\n",
    "  -------\n",
    "  out_str : str\n",
    "    the description of the object 'obj' in terms of parameters values.\n",
    "  \"\"\"\n",
    "  \n",
    "  out_str = obj.__class__.__name__+\"(\"\n",
    "  n_added_to_log = 0\n",
    "  for _iter, (prop, value) in enumerate(vars(obj).items()):\n",
    "    if type(value) in [int, float, bool]:\n",
    "      out_str += prop+'='+str(value) + ','\n",
    "      n_added_to_log += 1\n",
    "    elif type(value) in [str]:\n",
    "      out_str += prop+\"='\" + value + \"',\"\n",
    "      n_added_to_log += 1\n",
    "    \n",
    "    if n is not None and n_added_to_log >= n:\n",
    "      break\n",
    "  #endfor\n",
    "  \n",
    "  out_str = out_str[:-1] if out_str[-1]==',' else out_str\n",
    "  out_str += ')'\n",
    "  return out_str  \n",
    "\n",
    "  \n",
    "def prepare_grid_search(params_grid, valid_fn, nr_trials):\n",
    "  import itertools\n",
    "\n",
    "\n",
    "  params = []\n",
    "  values = []\n",
    "  for k in params_grid:\n",
    "    params.append(k)\n",
    "    assert type(params_grid[k]) is list, 'All grid-search params must be lists. Error: {}'.format(k)\n",
    "    values.append(params_grid[k])\n",
    "  combs = list(itertools.product(*values))\n",
    "  n_options = len(combs)\n",
    "  grid_iterations = []\n",
    "  for i in range(n_options):\n",
    "    comb = combs[i]\n",
    "    func_kwargs = {}\n",
    "    for j,k in enumerate(params):\n",
    "      func_kwargs[k] = comb[j]\n",
    "    grid_iterations.append(func_kwargs)\n",
    "  P(\"Filtering {} grid-search options...\".format(len(grid_iterations)))\n",
    "  cleaned_iters = [x for x in grid_iterations if valid_fn(x)]\n",
    "  n_options = len(cleaned_iters)\n",
    "  idxs = np.arange(n_options)\n",
    "  np.random.shuffle(idxs)\n",
    "  idxs = idxs[:nr_trials]\n",
    "  P(\"Generated {} random grid-search iters out of a total of {} iters\".format(\n",
    "      len(idxs), n_options))\n",
    "  return [cleaned_iters[i] for i in idxs]\n",
    "\n",
    "\n",
    "def add_res(dct, model_name, score, **kwargs):\n",
    "  if 'MODEL' not in dct:\n",
    "    dct['MODEL'] = []\n",
    "  if 'SCORE' not in dct:\n",
    "    dct['SCORE'] = []\n",
    "  n_existing = len(dct['MODEL'])\n",
    "  dct['MODEL'].append(model_name)\n",
    "  dct['SCORE'].append(score)\n",
    "  for key in kwargs:\n",
    "    if key not in dct:\n",
    "      dct[key] = ['-' ] * n_existing\n",
    "    dct[key].append(kwargs[key])\n",
    "  for k in dct:\n",
    "    if len(dct[k]) < (n_existing + 1):\n",
    "      dct[k] = dct[k] + [' '] * ((n_existing + 1) - len(dct[k]))\n",
    "  return dct\n",
    "\n",
    "\n",
    "def maybe_add_top_model(top_models, model, score, k=5):\n",
    "  if len(top_models) < k:\n",
    "    top_models.append([model, score])\n",
    "  else:\n",
    "    for i in range(k):\n",
    "      if top_models[i][1] < score:\n",
    "        for jj in range(k-1, i, -1):\n",
    "          top_models[jj] = top_models[jj-1].copy()\n",
    "        top_models[i][0] = model\n",
    "        top_models[i][1] = score\n",
    "        break          \n",
    "  return sorted(top_models, key=lambda x: x[1], reverse=True)\n",
    "       \n",
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "####                                                                       ####\n",
    "####                      END utility code section                         ####\n",
    "####                                                                       ####\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "###############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following section presents the main building blocks previously described \n",
    "\n",
    "This will include basic modules for various custom losses used in the models as well as modules used for special \n",
    "purpose in the proposed architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_label_distrib(data_split):\n",
    "  info = pd.DataFrame(data_split)[1].value_counts()\n",
    "  P(info)\n",
    "  return info \n",
    "\n",
    "\n",
    "def maybe_find_glove_replacement(w):\n",
    "  w = w.lower()\n",
    "  lw = len(w)\n",
    "  if lw < 4:\n",
    "    return None\n",
    "  else:   \n",
    "    nw1 = ''\n",
    "    found = False\n",
    "    for i in range(lw//2 + 2):\n",
    "      nw = w[:-(i+1)]\n",
    "      if nw in GLOVE:\n",
    "        nw1 = nw\n",
    "        found = True\n",
    "        break\n",
    "    nw2 = ''\n",
    "    for i in range(lw//2 + 2):\n",
    "      nw = w[(i+1):]\n",
    "      if nw in GLOVE:\n",
    "        nw2 = nw\n",
    "        found = True\n",
    "        break\n",
    "    if found:\n",
    "      return nw1 if len(nw1) > len(nw2) else nw2\n",
    "    return None\n",
    "\n",
    "def l_glv_rep(w):    \n",
    "  \"\"\"Return lower `w`'s GloVe representation if available, else return \n",
    "  a replacement (zeros vector is nothing is found).\"\"\"\n",
    "  if w in GLOVE:\n",
    "    return GLOVE[w]\n",
    "  else:\n",
    "    nw = maybe_find_glove_replacement(w)\n",
    "    v = np.random.uniform(low=-1e-7, high=1e-7, size=GLOVE_DIM)\n",
    "    if nw is not None:\n",
    "      v = v + GLOVE[nw]\n",
    "    return v\n",
    "    \n",
    "def l_glv(w):    \n",
    "  \"\"\"Return lower `w`'s GloVe representation if available, else return \n",
    "  a zeros vector.\"\"\"\n",
    "  return GLOVE.get(w.lower(), np.zeros(GLOVE_DIM))\n",
    "\n",
    "  \n",
    "def concat(u, v):\n",
    "  return np.concatenate((u,v))\n",
    "\n",
    "def summar(u, v):\n",
    "  return u + v\n",
    "\n",
    "def arr(u,v):\n",
    "  return np.array((u,v))\n",
    "\n",
    "\n",
    "def test_glove_vs_data(trn, dev):\n",
    "  train_words = set()\n",
    "  for x in trn:\n",
    "    train_words.add(x[0][0])\n",
    "    train_words.add(x[0][1])\n",
    "\n",
    "  dev_words = set()\n",
    "  for x in dev:\n",
    "    dev_words.add(x[0][0])\n",
    "    dev_words.add(x[0][1])\n",
    "  \n",
    "  glove_train = [x for x in trn if (x[0][0] in GLOVE) and (x[0][1] in GLOVE)]\n",
    "  glove_dev = [x for x in dev if (x[0][0] in GLOVE) and (x[0][1] in GLOVE)]\n",
    "  out_train = [x for x in trn if (x[0][0] not in GLOVE) or (x[0][1] not in GLOVE)]\n",
    "  out_dev = [x for x in dev if (x[0][0] not in GLOVE) or (x[0][1] not in GLOVE)]\n",
    "  \n",
    "  P(\"\\nGlove train: {} ({:.1f}%)\".format(len(glove_train), len(glove_train)/len(trn)*100))\n",
    "  P(\"\\nGlove dev: {} ({:.1f}%)\".format(len(glove_dev), len(glove_dev)/len(dev)*100))\n",
    "  \n",
    "  miss_train = [x.lower() for x in train_words if x.lower() not in GLOVE] \n",
    "  miss_dev = [x.lower() for x in dev_words if x.lower() not in GLOVE]\n",
    "  P(\"\\nTrain has {} words that are not in GLOVE: {}...\".format(len(miss_train), miss_train[:5]))\n",
    "  positives = 0\n",
    "  negatives = 0\n",
    "  for i, w in enumerate(miss_train):\n",
    "    for x in trn:\n",
    "      if x[0][0] == w or x[0][1] == w:\n",
    "        if x[1]:\n",
    "          positives += 1\n",
    "        else:\n",
    "          negatives += 1\n",
    "        if x[1] == 1 and positives < 5:\n",
    "          P(\"  {}\".format(x))\n",
    "        if x[1] == 0 and negatives < 5:\n",
    "          P(\"  {}\".format(x))\n",
    "  P(\"\\nPos vs neg: {} vs {}\".format(positives, negatives))\n",
    "  P(\"\\n\\nDev has {} words that are not in GLOVE: {}...\".format(len(miss_dev), miss_dev[:5]))\n",
    "  positives = 0\n",
    "  negatives = 0\n",
    "  for i, w in enumerate(miss_dev):\n",
    "    for x in dev:\n",
    "      if x[0][0] == w or x[0][1] == w:\n",
    "        if x[1]:\n",
    "          positives += 1\n",
    "        else:\n",
    "          negatives += 1\n",
    "        if x[1] == 1 and positives < 5:\n",
    "          P(\"  {}\".format(x))\n",
    "        if x[1] == 0 and negatives < 5:\n",
    "          P(\"  {}\".format(x))\n",
    "  P(\"\\nPos vs neg: {} vs {}\".format(positives, negatives))\n",
    "  res = {\n",
    "      'glove_train' : glove_train,\n",
    "      'glove_dev' : glove_dev,\n",
    "      'out_train' : out_train,\n",
    "      'out_dev' : out_dev\n",
    "      }\n",
    "  return res, (miss_train, miss_dev)\n",
    "\n",
    "\n",
    "class ConstrativeLoss(th.nn.Module):\n",
    "  def __init__(self, margin=0.2):\n",
    "    super(ConstrativeLoss, self).__init__()\n",
    "    self.margin = margin\n",
    "    \n",
    "    \n",
    "  def forward(self, dist, gold):\n",
    "    th_d_sq = th.pow(dist, 2)\n",
    "    th_d_sqm = th.pow(th.clamp(self.margin - dist, 0), 2)\n",
    "    loss = (1 - gold) * th_d_sq + gold * th_d_sqm\n",
    "    return loss.mean()\n",
    "  \n",
    "  def __repr__(self):\n",
    "    s = self.__class__.__name__ + \"(margin={})\".format(\n",
    "        self.margin,\n",
    "        )\n",
    "    return s  \n",
    "\n",
    "\n",
    "class FocalLossWithLogits_B(th.nn.Module):\n",
    "  def __init__(self, alpha=0.3, gamma=2):\n",
    "    super().__init__()\n",
    "    assert alpha <= 0.9 and alpha >= 0.1\n",
    "    self.alpha = alpha\n",
    "    self.gamma = gamma\n",
    "\n",
    "  def forward(self, inputs, targets):\n",
    "    pos_alpha = self.alpha\n",
    "    neg_alpha = 1 - self.alpha\n",
    "    eps = 1e-14\n",
    "    \n",
    "    y_pred = th.sigmoid(inputs)\n",
    "    \n",
    "    pos_pt = th.where(targets==1 , y_pred , th.ones_like(y_pred)) # positive pt (fill all the 0 place in y_true with 1 so (1-pt)=0 and log(pt)=0.0) where pt is 1\n",
    "    neg_pt = th.where(targets==0 , y_pred , th.zeros_like(y_pred)) # negative pt\n",
    "    \n",
    "    pos_pt = th.clamp(pos_pt, eps, 1 - eps)\n",
    "    neg_pt = th.clamp(neg_pt, eps, 1 - eps)\n",
    "    \n",
    "    pos_modulating = th.pow(1-pos_pt, self.gamma) # compute postive modulating factor for correct classification the value approaches to zero\n",
    "    neg_modulating = th.pow(neg_pt, self.gamma) # compute negative modulating factor\n",
    "    \n",
    "    \n",
    "    pos = - pos_alpha * pos_modulating * th.log(pos_pt) #pos part\n",
    "    neg = - neg_alpha * neg_modulating * th.log(1 - neg_pt) # neg part\n",
    "    \n",
    "    loss = pos + neg  # this is final loss to be returned with some reduction\n",
    "    \n",
    "    return th.mean(loss)\n",
    "      \n",
    "  def __repr__(self):\n",
    "    s = self.__class__.__name__ + \"(alpha={}, gamma={})\".format(\n",
    "        self.alpha,\n",
    "        self.gamma,\n",
    "        )\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FocalLossWithLogits_A(th.nn.Module):\n",
    "  def __init__(self, alpha=4, gamma=2):\n",
    "    super().__init__()\n",
    "    self.alpha = alpha\n",
    "    self.gamma = gamma\n",
    "\n",
    "  def forward(self, inputs, targets):\n",
    "    BCE_loss = th.nn.functional.binary_cross_entropy_with_logits(\n",
    "        inputs, \n",
    "        targets, \n",
    "        reduction='none',\n",
    "        )\n",
    "\n",
    "    pt = th.exp(-BCE_loss)\n",
    "    F_loss = self.alpha * th.pow(1 - pt, self.gamma) * BCE_loss\n",
    "    return th.mean(F_loss)\n",
    "      \n",
    "  def __repr__(self):\n",
    "    s = self.__class__.__name__ + \"(alpha={}, gamma={})\".format(\n",
    "        self.alpha,\n",
    "        self.gamma,\n",
    "        )\n",
    "    return s\n",
    "\n",
    "\n",
    "class InputPlaceholder(th.nn.Module):\n",
    "  def __init__(self, input_dim):\n",
    "    super().__init__()\n",
    "    self.input_dim = input_dim\n",
    "    \n",
    "  def forward(self, inputs):\n",
    "    return inputs\n",
    "  \n",
    "  def __repr__(self):\n",
    "    s = self.__class__.__name__ + \"(input_dim={})\".format(\n",
    "        self.input_dim,\n",
    "        )\n",
    "    return s\n",
    "  \n",
    "class L2_Normalizer(th.nn.Module):\n",
    "  def __init__(self,):\n",
    "    super().__init__()\n",
    "    \n",
    "  def forward(self, inputs):\n",
    "    return th.nn.functional.normalize(inputs, p=2, dim=1)\n",
    "\n",
    "class PathsCombiner(th.nn.Module):\n",
    "  def __init__(self, input_dim, method, activ=None):\n",
    "    super().__init__()\n",
    "    self.method = method\n",
    "    self.input_dim = input_dim\n",
    "    self.activ = self.get_activation(activ)\n",
    "    if method in ['sub','abs','sqr', 'add']:\n",
    "      self.output_dim = input_dim\n",
    "    elif method == 'cat':\n",
    "      self.output_dim = input_dim * 2\n",
    "    elif method == 'eucl':\n",
    "      self.output_dim = 1\n",
    "    else:\n",
    "      raise ValueError(\"Unknown combine method '{}'\".format(method))\n",
    "    return\n",
    "    \n",
    "  def forward(self, paths):\n",
    "    path1 = paths[0]\n",
    "    path2 = paths[1]\n",
    "#    if self.norm_each:\n",
    "#      path1 = th.nn.functional.normalize(path1, p=2, dim=1)\n",
    "#      path2 = th.nn.functional.normalize(path2, p=2, dim=1)\n",
    "      \n",
    "    if self.method == 'sub':\n",
    "      th_x = path1 - path2\n",
    "    elif self.method == 'add':\n",
    "      th_x = path1 + path2\n",
    "    elif self.method == 'cat':\n",
    "      th_x = th.cat((path1, path2), dim=1)\n",
    "    elif self.method == 'abs':\n",
    "      th_x = (path1 - path2).abs()\n",
    "    elif self.method == 'sqr':\n",
    "      th_x = th.pow(path1 - path2, 2)\n",
    "    elif self.method == 'eucl':\n",
    "      th_x = th.pairwise_distance(path1, path2, keepdim=True)    \n",
    "    \n",
    "    if self.activ is not None:\n",
    "      th_x = self.activ(th_x)\n",
    "      \n",
    "    return th_x\n",
    "  \n",
    "  def get_activation(self, act):\n",
    "    if act == 'relu':\n",
    "      return th.nn.ReLU()\n",
    "    elif act == 'tanh':\n",
    "      return th.nn.Tanh()\n",
    "    elif act == 'selu':\n",
    "      return th.nn.SELU()\n",
    "    elif act == 'sigmoid':\n",
    "      return th.nn.Sigmoid()\n",
    "    else:\n",
    "      return None\n",
    "  \n",
    "  \n",
    "  def __repr__(self):\n",
    "    s = self.__class__.__name__ + \"(input_dim={}x2, output_dim={}, method='{}', act={})\".format(\n",
    "        self.input_dim,\n",
    "        self.output_dim,\n",
    "        self.method,\n",
    "        self.activ,\n",
    "        )\n",
    "    return s\n",
    "\n",
    "class ThWordEntailModel(th.nn.Module):\n",
    "  def __init__(self,\n",
    "               input_dim,\n",
    "               siam_lyrs,\n",
    "               siam_norm,\n",
    "               siam_bn,\n",
    "               comb_activ,\n",
    "               separate_paths,\n",
    "               layers,\n",
    "               input_drop,\n",
    "               other_drop,\n",
    "               bn_inputs,\n",
    "               bn,\n",
    "               smethod,\n",
    "               loss_type,\n",
    "               device,\n",
    "               activ='relu',\n",
    "               ):\n",
    "    super().__init__()\n",
    "    self.device = device\n",
    "    self.has_input_drop = input_drop\n",
    "    self.has_input_bn = bn_inputs\n",
    "    self.smethod = smethod\n",
    "    self.separate = separate_paths\n",
    "    self.loss_type = loss_type\n",
    "    self.siam_norm = siam_norm\n",
    "    self.siam_bn = siam_bn\n",
    "    self.comb_activ = comb_activ\n",
    "    \n",
    "    if self.loss_type == 'cl' and (layers != [] or siam_lyrs == [] or separate_paths):\n",
    "      raise ValueError(\"Cannot have siamese nets with CL with this config: layers={}  siam_lyrs={} sep={}\".format(\n",
    "          layers, siam_lyrs, separate_paths))\n",
    "      \n",
    "    if other_drop != 0 and layers == [] and siam_lyrs == []:\n",
    "      raise ValueError(\"Cannot have dropout on no layers...\")\n",
    "\n",
    "    if self.separate:\n",
    "      paths = [[],[]]\n",
    "    else:\n",
    "      paths = [[]]\n",
    "    self.path_input = input_dim\n",
    "    \n",
    "    for path_no in range(len(paths)):\n",
    "      last_output = self.path_input\n",
    "      paths[path_no].append(InputPlaceholder(self.path_input))\n",
    "      if input_drop > 0:\n",
    "        paths[path_no].append(th.nn.Dropout(input_drop))\n",
    "      if bn_inputs:\n",
    "        paths[path_no].append(th.nn.BatchNorm1d(last_output))\n",
    "      if len(siam_lyrs) > 0:\n",
    "        for i, layer in enumerate(siam_lyrs):\n",
    "          paths[path_no].append(th.nn.Linear(last_output, layer, bias=not self.siam_bn))\n",
    "          if self.siam_bn:\n",
    "            paths[path_no].append(th.nn.BatchNorm1d(layer))\n",
    "          if i < (len(siam_lyrs) - 1):\n",
    "            paths[path_no].append(self.get_activation(activ))\n",
    "            if other_drop > 0:\n",
    "              paths[path_no].append(th.nn.Dropout(other_drop))\n",
    "          last_output = layer\n",
    "      if self.siam_norm:\n",
    "        paths[path_no].append(L2_Normalizer())\n",
    "    if self.separate :\n",
    "      self.path1_layers = th.nn.ModuleList(paths[0])\n",
    "      self.path2_layers = th.nn.ModuleList(paths[1])\n",
    "    else:\n",
    "      self.siam_layers = th.nn.ModuleList(paths[0])\n",
    "      \n",
    "    \n",
    "    siam_combine = PathsCombiner(\n",
    "        last_output, \n",
    "        method=self.smethod, \n",
    "        activ=self.comb_activ,\n",
    "        )\n",
    "    last_output = siam_combine.output_dim\n",
    "    post_lyrs = [siam_combine]    \n",
    "    if self.loss_type != 'cl':\n",
    "      for i, layer in enumerate(layers):\n",
    "        post_lyrs.append(th.nn.Linear(last_output, layer, bias=not bn))\n",
    "        if bn:\n",
    "          post_lyrs.append(th.nn.BatchNorm1d(layer))\n",
    "        post_lyrs.append(self.get_activation(activ))\n",
    "        if other_drop > 0:\n",
    "          post_lyrs.append(th.nn.Dropout(other_drop))\n",
    "        last_output = layer\n",
    "      post_lyrs.append(th.nn.Linear(last_output, 1))\n",
    "    self.post_layers = th.nn.ModuleList(post_lyrs)\n",
    "    return\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    th_path1 = inputs[:,0]\n",
    "    th_path2 = inputs[:,1]\n",
    "\n",
    "    if self.separate:\n",
    "      if len(self.path1_layers) > 0:\n",
    "        for th_layer in self.path1_layers:\n",
    "          th_path1 = th_layer(th_path1)\n",
    "        for th_layer in self.path2_layers:\n",
    "          th_path2 = th_layer(th_path2)\n",
    "    else:\n",
    "      if len(self.siam_layers) > 0:\n",
    "        for th_layer in self.siam_layers:\n",
    "          th_path1 = th_layer(th_path1)\n",
    "        for th_layer in self.siam_layers:\n",
    "          th_path2 = th_layer(th_path2)\n",
    "\n",
    "    \n",
    "    th_x = (th_path1, th_path2)    \n",
    "    # first layer in post-siam must be the combination layer\n",
    "    for layer in self.post_layers:\n",
    "      th_x = layer(th_x)    \n",
    "    return th_x\n",
    "    \n",
    "  \n",
    "  def get_activation(self, act):\n",
    "    if act == 'relu':\n",
    "      return th.nn.ReLU()\n",
    "    elif act == 'tanh':\n",
    "      return th.nn.Tanh()\n",
    "    elif act == 'selu':\n",
    "      return th.nn.SELU()\n",
    "    elif act == 'sigmoid':\n",
    "      return th.nn.Sigmoid()\n",
    "    else:\n",
    "      raise ValueError(\"Unknown activation function '{}'\".format(act))\n",
    "  \n",
    "      \n",
    "      \n",
    "\n",
    "\n",
    "class WordEntailClassifier():\n",
    "  def __init__(self, \n",
    "               model_name,  # model name\n",
    "               siam_lyrs,   # layers of the siamese or the individual word encoders\n",
    "               s_l2,        # applycation of l2 on siamese/paths\n",
    "               s_bn,        # BN in siams/paths\n",
    "               c_act,       # apply activation on siam/paths combiner\n",
    "               separ,       # use separate paths for each word\n",
    "               layers,      # layers of the final classifier dnn\n",
    "               inp_drp,     # apply drop on inputs\n",
    "               o_drp,       # apply drop on each fc\n",
    "               bn,          # apply BN on each liniar in final dnn\n",
    "               bn_inp,      # apply BN on inputs\n",
    "               activ,       # activation (all)\n",
    "               x_dev,       # x_dev for early stop w. lr decay\n",
    "               y_dev,       # y_dev for early stop w. lr decay\n",
    "               s_comb,      # method for combining siams/paths\n",
    "               rev,         # reverse targets during training/predict\n",
    "               lr,          # starting lr\n",
    "               loss,        # loss function name for CL/BCE/FL\n",
    "               bal,         # apply sample balancing during training\n",
    "               VF,\n",
    "               \n",
    "               cl_m=1,        # if using CL this is margin\n",
    "               fl_g=2,        # focal loss discount exponent\n",
    "               lr_decay=0.5,  # lr decay factor\n",
    "               batch=256,     # batch size\n",
    "               l2_strength=0, # l2 weight decay\n",
    "               max_epochs=10000,  # not really used\n",
    "               max_patience=10,   # maximum patience before reload & lr decay  \n",
    "               max_fails=40,      # max consecutive fails before stop\n",
    "               optim=th.optim.Adam,\n",
    "               \n",
    "               device=th.device(\"cuda\" if th.cuda.is_available() else \"cpu\"),\n",
    "               ):\n",
    "    self.model = None\n",
    "    self.model_name = model_name\n",
    "    self.layers = layers\n",
    "    self.siam_lyrs = siam_lyrs\n",
    "    self.input_drop = inp_drp\n",
    "    self.other_drop = o_drp\n",
    "    self.bn = bn\n",
    "    self.bn_inputs = bn_inp\n",
    "    self.activ=activ    \n",
    "    self.max_epochs = max_epochs\n",
    "    self.x_dev = x_dev\n",
    "    self.y_dev = np.array(y_dev)\n",
    "    self.batch_size = batch\n",
    "    self.max_patience = max_patience\n",
    "    self.optimizer = optim\n",
    "    self.siamese_method = s_comb\n",
    "    self.reverse_target = rev\n",
    "    self.device = device\n",
    "    self.lr = lr\n",
    "    self.lr_decay = lr_decay\n",
    "    self.l2_strength = l2_strength\n",
    "    self.max_fails = max_fails\n",
    "    self.separate_paths = separ\n",
    "    self.loss_type = loss\n",
    "    self.siam_norm = s_l2\n",
    "    self.siam_bn = s_bn\n",
    "    self.comb_activ = c_act\n",
    "    self.margin = cl_m\n",
    "    self.use_balancing = bal\n",
    "    self.focal_loss_alpha = 0.3 if loss == 'flb' else 4\n",
    "    self.focan_loss_gamma = fl_g\n",
    "    self.vector_func_name = VF if type(VF) == str else VF.__name__\n",
    "    if loss == 'cl' and not rev:\n",
    "      raise ValueError(\"CL must receive reversed targets\")\n",
    "    return\n",
    "  \n",
    "  \n",
    "  def define_graph(self):\n",
    "    if not hasattr(self, 'input_dim'):\n",
    "      self.input_dim = GLOVE_DIM\n",
    "    model = ThWordEntailModel(\n",
    "        input_dim=self.input_dim,\n",
    "        siam_lyrs=self.siam_lyrs,\n",
    "        separate_paths=self.separate_paths,\n",
    "        layers=self.layers,\n",
    "        input_drop=self.input_drop,\n",
    "        other_drop=self.other_drop,\n",
    "        bn=self.bn,\n",
    "        bn_inputs=self.bn_inputs,\n",
    "        activ=self.activ,\n",
    "        device=self.device,\n",
    "        smethod=self.siamese_method,\n",
    "        siam_norm=self.siam_norm,\n",
    "        loss_type=self.loss_type,\n",
    "        siam_bn=self.siam_bn,\n",
    "        comb_activ=self.comb_activ,\n",
    "        )\n",
    "    return model    \n",
    "      \n",
    "  \n",
    "  def fit(self, X, y):\n",
    "    utils.fix_random_seeds()\n",
    "    # Data prep:\n",
    "    X = np.array(X).astype(np.float32)\n",
    "    n_obs = X.shape[0]\n",
    "    # here is a trick: we consider the words that entail those that have\n",
    "    # minimal distance if using siamese\n",
    "    np_y = np.array(y).reshape(-1,1)\n",
    "    if self.reverse_target:\n",
    "      np_y = 1 - np_y\n",
    "    self.input_dim = X.shape[-1]\n",
    "    X = th.tensor(X, dtype=th.float32)\n",
    "    y = th.tensor(np_y, dtype=th.float32)\n",
    "    dataset = th.utils.data.TensorDataset(X, y)\n",
    "    \n",
    "    sampler = None\n",
    "    if self.use_balancing:\n",
    "      cls_0 = (np_y == 0).sum()\n",
    "      cls_1 = np_y.shape[0] - cls_0\n",
    "      cls_weights = 1 / np.array([cls_0, cls_1])\n",
    "      weights = [cls_weights[i] for i in np_y.ravel()]\n",
    "      sampler = th.utils.data.sampler.WeightedRandomSampler(weights, num_samples=len(weights))\n",
    "      \n",
    "    dataloader = th.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=self.batch_size, \n",
    "        shuffle=sampler is None,\n",
    "        pin_memory=True,\n",
    "        sampler=sampler,\n",
    "        )\n",
    "    # Optimization:\n",
    "    if self.loss_type == 'bce':\n",
    "      loss = th.nn.BCEWithLogitsLoss()\n",
    "    elif self.loss_type == 'cl':\n",
    "      loss = ConstrativeLoss(margin=self.margin)\n",
    "    elif 'fl' in self.loss_type:\n",
    "      if self.loss_type == 'flb':\n",
    "        loss = FocalLossWithLogits_B(\n",
    "            alpha=self.focal_loss_alpha,\n",
    "            gamma=self.focan_loss_gamma,\n",
    "            )\n",
    "      else:\n",
    "        loss = FocalLossWithLogits_A(\n",
    "            alpha=self.focal_loss_alpha,\n",
    "            gamma=self.focan_loss_gamma,\n",
    "            )        \n",
    "    else:\n",
    "      raise ValueError('unknown loss {}'.format(self.loss_type))\n",
    "    if self.model is None:\n",
    "      self.model = self.define_graph()\n",
    "      P(\"Initialized model:\")\n",
    "      self.print_model()\n",
    "      P(\"  Loss: {}\\n\\n\".format(str(loss) if loss.__class__.__name__ != 'method' else loss.__name__))\n",
    "\n",
    "    else:\n",
    "      P(\"\\rFitting already loaded model...\\t\\t\\t\")\n",
    "    self.model.to(self.device)\n",
    "    self.model.train()\n",
    "    optimizer = self.optimizer(\n",
    "        self.model.parameters(),\n",
    "        lr=self.lr,\n",
    "        weight_decay=self.l2_strength)\n",
    "    # Train:\n",
    "    patience = 0\n",
    "    fails = 0\n",
    "    best_f1 = 0\n",
    "    best_fn = ''\n",
    "    best_epoch = -1\n",
    "    self.errors = []\n",
    "    not_del_fns = []\n",
    "    for epoch in range(1, self.max_epochs+1):\n",
    "      epoch_error = 0.0\n",
    "      for batch_iter, (X_batch, y_batch) in enumerate(dataloader):\n",
    "        X_batch = X_batch.to(self.device, non_blocking=True)\n",
    "        y_batch = y_batch.to(self.device, non_blocking=True)\n",
    "        batch_preds = self.model(X_batch)\n",
    "        err = loss(batch_preds, y_batch)\n",
    "        epoch_error += err.item()\n",
    "        optimizer.zero_grad()\n",
    "        err.backward()\n",
    "        optimizer.step()\n",
    "        Pr(\"Training epoch {} - {:.1f}% - Patience {}/{},  Fails {}/{}\\t\\t\\t\\t\\t\".format(\n",
    "            epoch, \n",
    "            (batch_iter + 1) / (n_obs // self.batch_size + 1) * 100,\n",
    "            patience, self.max_patience,\n",
    "            fails, self.max_fails))\n",
    "      # end epoch\n",
    "      predictions = self.predict(self.x_dev)\n",
    "      macrof1 = utils.safe_macro_f1(self.y_dev, predictions)\n",
    "      # resume training\n",
    "      self.model.train()\n",
    "      if macrof1 > best_f1:\n",
    "        patience = 0\n",
    "        fails = 0\n",
    "        last_best_fn = best_fn\n",
    "        best_fn = \"models/{}_e{:03}_F{:.4f}.th\".format(\n",
    "            self.model_name, epoch, macrof1)\n",
    "        best_epoch = epoch\n",
    "        P(\"\\rFound new best macro-f1 {:.4f} > {:.4f} at epoch {}. \\t\\t\\t\".format(macrof1, best_f1, epoch))\n",
    "        best_f1 = macrof1\n",
    "        th.save(self.model.state_dict(), best_fn)\n",
    "        th.save(optimizer.state_dict(), best_fn + '.optim')\n",
    "        if last_best_fn != '':\n",
    "          try:\n",
    "            os.remove(last_best_fn)\n",
    "            os.remove(last_best_fn + '.optim')\n",
    "          except:\n",
    "            not_del_fns.append(last_best_fn)\n",
    "            not_del_fns.append(last_best_fn + '.optim')\n",
    "      else:\n",
    "        patience += 1\n",
    "        fails += 1\n",
    "        Pr(\"Finished epoch {}. Current score {:.3f} < {:.3f}. Patience {}/{},  Fails {}/{}\".format(\n",
    "            epoch, macrof1, best_f1, patience, self.max_patience, fails, self.max_fails))\n",
    "        if patience > self.max_patience:\n",
    "          lr_old = optimizer.param_groups[0]['lr'] \n",
    "          lr_new = lr_old * self.lr_decay\n",
    "          self.model.load_state_dict(th.load(best_fn))\n",
    "          optimizer.load_state_dict(th.load(best_fn + '.optim'))\n",
    "          for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr_new\n",
    "          P(\"\\nPatience reached {}/{}  -  reloaded from ep {} reduced lr from {:.1e} to {:.1e}\".format(\n",
    "              patience, self.max_patience, best_epoch, lr_old, lr_new))\n",
    "          patience = 0\n",
    "          \n",
    "        if fails > self.max_fails:\n",
    "          P(\"\\nMax fails {}/{} reached!\".format(fails, self.max_fails))\n",
    "          break\n",
    "          \n",
    "      self.errors.append(epoch_error)\n",
    "    # end all epochs\n",
    "    if best_fn != '':\n",
    "      P(\"Loading model from epoch {} with macro-f1 {:.4f}\".format(best_epoch, best_f1))\n",
    "      self.model.load_state_dict(th.load(best_fn))        \n",
    "      not_del_fns.append(best_fn + '.optim')\n",
    "      if best_f1 < 0.67:\n",
    "        P(\"  Removing '{}'\".format(best_fn))        \n",
    "        not_del_fns.append(best_fn)\n",
    "      else:\n",
    "        os.rename(best_fn, \"models/{}.th\".format(self.model_name))\n",
    "    P(\"  Cleaning after fit...\")\n",
    "    atmps = 0\n",
    "    while atmps < 10 and len(not_del_fns) > 0:   \n",
    "      removed = []\n",
    "      for fn in not_del_fns:\n",
    "        if os.path.isfile(fn):\n",
    "          try:\n",
    "            os.remove(fn)\n",
    "            removed.append(fn)\n",
    "            P(\"  Removed '{}'\".format(fn))\n",
    "          except:\n",
    "            pass\n",
    "        else:\n",
    "          removed.append(fn)            \n",
    "      not_del_fns = [x for x in not_del_fns if x not in removed]\n",
    "    return self\n",
    "  \n",
    "\n",
    "  def predict_proba(self, X):\n",
    "    self.model.eval()\n",
    "    with th.no_grad():\n",
    "      self.model.to(self.device)\n",
    "      X = th.tensor(X, dtype=th.float).to(self.device)\n",
    "      preds = self.model(X)\n",
    "      if self.loss_type in ['bce', 'fl', 'fla', 'flb']:\n",
    "        result = th.sigmoid(preds).cpu().numpy().ravel()\n",
    "        if self.reverse_target:\n",
    "          result = 1 - result\n",
    "      else:\n",
    "        dists = preds.cpu().numpy().ravel()\n",
    "        result = self._dist_to_proba(dists, eps=0.52)\n",
    "        \n",
    "      return result\n",
    "\n",
    "\n",
    "  def predict(self, X):\n",
    "    probs = self.predict_proba(X)\n",
    "    classes = (probs >= 0.5).astype(np.int8)\n",
    "\n",
    "    return classes\n",
    "  \n",
    "  def print_model(self, indent=2):\n",
    "    P(\"{}Model name: {}\".format(indent * \" \",self.model_name))\n",
    "    P(textwrap.indent(str(self.model), \" \" * indent))\n",
    "    P(\"{}Vector func: {}\".format(indent * \" \", self.vector_func_name))\n",
    "    P(\"{}Trained on {} data.\".format(indent * \" \", \"BALANCED\" if self.use_balancing else \"raw unbalanced\"))\n",
    "    return\n",
    "  \n",
    "  \n",
    "  def _dist_to_proba(self, y_pred, eps):\n",
    "    s = -0.75 * y_pred / eps + 1.75\n",
    "    d = -0.25 * y_pred / eps + 0.25\n",
    "    sgn = ((eps - y_pred) > 0) + 0 - ((eps - y_pred) < 0)\n",
    "    \n",
    "    yproba = ((s + sgn * d) / 2).clip(0)\n",
    "    return yproba\n",
    "  \n",
    "  def save(self, label=None):\n",
    "    if label is None:\n",
    "      label = self.model_name    \n",
    "    fn = 'models/{}.th'.format(label)\n",
    "    th.save(self.model.state_dict(), fn)\n",
    "    P(\"Saved '{}'\".format(fn))\n",
    "    return\n",
    "  \n",
    "  def load(self, label=None):\n",
    "    if label is None:\n",
    "      label = self.model_name    \n",
    "    fn = 'models/{}.th'.format(label)\n",
    "    if not os.path.isfile(fn):\n",
    "      raise ValueError(\"Model file '{}' not found!\".format(fn))\n",
    "    if self.model is None:\n",
    "      self.model = self.define_graph()\n",
    "    self.model.load_state_dict(th.load(fn))\n",
    "    P(\"Loaded model from '{}'\".format(fn))\n",
    "    return\n",
    "  \n",
    "\n",
    "  def has_saved(self, label=None):\n",
    "    if label is None:\n",
    "      label = self.model_name    \n",
    "    fn = 'models/{}.th'.format(label)\n",
    "    return os.path.isfile(fn)\n",
    "    \n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "class EnsembleWrapper():\n",
    "  def __init__(self, models, vector_func_name, verbose=False):\n",
    "    self.models = models\n",
    "    self.vector_func_name = vector_func_name\n",
    "    names = [x.model_name.split('_')[1] for x in self.models]\n",
    "    self.model_name = \"E_\" + \"_\".join(names)\n",
    "    if verbose:\n",
    "      P(\"Initialized ensemble '{}' with {} models using vectorizer '{}':\".format(\n",
    "          self.model_name, len(self.models), self.vector_func_name))\n",
    "      self.print_models()\n",
    "            \n",
    "  def print_models(self):\n",
    "    P(\"Ensemble '{}' architecture:\".format(self.model_name))\n",
    "    P(\"  \" + \"-\"* 80)\n",
    "    for i,model in enumerate(self.models):\n",
    "      P(\"  Model {}/{}\".format(i+1, len(self.models)))\n",
    "      model.print_model()\n",
    "      P(\"  \" + \"-\"* 80)\n",
    "  \n",
    "  def predict(self, x):\n",
    "    preds = []\n",
    "    for model in self.models:\n",
    "      model_preds = model.predict_proba(x)\n",
    "      preds.append(model_preds)        \n",
    "    final_preds_cat = np.vstack(preds).T\n",
    "    final_preds = final_preds_cat.mean(axis=1)\n",
    "    return (final_preds >= 0.5).astype(np.uint8)\n",
    "\n",
    "    \n",
    "  def fit(self, x, y):\n",
    "    P(\"****** Ensemble model passing fit call ******\")\n",
    "    return self\n",
    "  \n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_baselines(dct_res, trn, dev):\n",
    "  baseline_model_facts = {\n",
    "      \"BaseLR_C6L2\": lambda: LogisticRegression(fit_intercept=True, \n",
    "                                                solver='liblinear', \n",
    "                                                multi_class='auto',\n",
    "                                                C=0.6,\n",
    "                                                penalty='l2'),\n",
    "      \"BaseLR_C4L1\": lambda: LogisticRegression(fit_intercept=True, \n",
    "                                                solver='liblinear', \n",
    "                                                multi_class='auto',\n",
    "                                                C=0.4,\n",
    "                                                penalty='l1'),\n",
    "      \"BaseNN_50\" : lambda: TorchShallowNeuralClassifier(hidden_dim=50, eta=0.005),\n",
    "      \"BaseNN_150\" : lambda: TorchShallowNeuralClassifier(hidden_dim=150, eta=0.005),\n",
    "      \"BaseNN_300\" : lambda: TorchShallowNeuralClassifier(hidden_dim=300, eta=0.005),\n",
    "  }\n",
    "  \n",
    "  baseline_vector_combo_funcs = [\n",
    "      concat, \n",
    "  #    summar,\n",
    "      ]\n",
    "  for vf in [l_glv, l_glv_rep]:\n",
    "    for vcf in baseline_vector_combo_funcs:\n",
    "      for model_name in baseline_model_facts:\n",
    "        P(\"=\" * 70)\n",
    "        P(\"Running baseline model '{}' with '{}'\".format(\n",
    "            model_name, vcf.__name__))\n",
    "        model = baseline_model_facts[model_name]()\n",
    "        x_d, y_d = nli.word_entail_featurize(\n",
    "            data=dev, \n",
    "            vector_func=vf, \n",
    "            vector_combo_func=vcf\n",
    "            )\n",
    "        res = nli.wordentail_experiment(\n",
    "            train_data=trn,\n",
    "            assess_data=dev,\n",
    "            vector_func=vf,\n",
    "            vector_combo_func=vcf,\n",
    "            model=model,\n",
    "            )\n",
    "        score = res['macro-F1']\n",
    "        y_pred = model.predict(x_d)\n",
    "        report = classification_report(y_d, y_pred, digits=3, output_dict=True)\n",
    "        assert score == report['macro avg']['f1-score']\n",
    "        P_REC = report['1']['recall']\n",
    "        add_res(dct_res, model_name, round(score * 100,2), P_REC=round(P_REC*100,2), \n",
    "                VF=vf.__name__)\n",
    "        df = pd.DataFrame(dct_results).sort_values('SCORE')\n",
    "        P(\"\\nResults so far:\\n{}\\n\".format(df))\n",
    "  return dct_res\n",
    "\n",
    "\n",
    "def run_grid_search(dct_res, trn, dev, non_cl_runs=350, cl_runs=100):\n",
    "  grids = {\n",
    "      300: {\n",
    "          'non_cl' : {\n",
    "                \"siam_lyrs\" : [[],[600, 300],[600, 300, 150],],          \n",
    "                \"separ\" : [True,False,],      \n",
    "                \"layers\" : [[512, 256],[1024, 384, 128]],          \n",
    "                \"inp_drp\" : [0.3],  \n",
    "                \"o_drp\" : [0.5,],          \n",
    "                \"bn\" : [True,False],          \n",
    "                \"bn_inp\" : [True,False],          \n",
    "                \"activ\" : ['relu',],          \n",
    "                \"lr\"  :[0.005,],          \n",
    "                \"s_comb\" : ['sqr',],      \n",
    "                's_l2' : [True,False,],          \n",
    "                's_bn' :[True,False],          \n",
    "                'rev' :[True,False,],          \n",
    "                'loss' : ['bce','fla','flb'],\n",
    "                'c_act' : [None],                    \n",
    "                'VF':['l_glv','l_glv_rep',],          \n",
    "                'bal' : [True,False,],          \n",
    "                'cl_m' : [None,]        \n",
    "              },\n",
    "          'cl' : {\n",
    "                \"siam_lyrs\" : [[256, 128],[512, 256],[512, 256, 128],[1024, 512],],          \n",
    "                \"separ\" : [False,],      \n",
    "                \"layers\" : [[],],          \n",
    "                \"inp_drp\" : [0,0.3],  \n",
    "                \"o_drp\" : [0.5,],          \n",
    "                \"bn\" : [None],          \n",
    "                \"bn_inp\" : [True,False],          \n",
    "                \"activ\" : ['relu',],          \n",
    "                \"lr\"  :[0.0001,],          \n",
    "                'c_act' : [None,],\n",
    "                \"s_comb\" : ['eucl',],      \n",
    "                's_l2' : [True,],\n",
    "                's_bn' : [False],          \n",
    "                'rev' :[True,],          \n",
    "                'loss' : ['cl',],          \n",
    "                'VF':['l_glv','l_glv_rep',],          \n",
    "                'bal' : [True,False,],\n",
    "              }\n",
    "          },\n",
    "      100: {\n",
    "          'non_cl' : {\n",
    "                \"siam_lyrs\" : [[],[200, 100],[200, 100, 50],],          \n",
    "                \"separ\" : [True,False,],      \n",
    "                \"layers\" : [[368, 64],[512, 256, 64]],          \n",
    "                \"inp_drp\" : [0.3],  \n",
    "                \"o_drp\" : [0.5,],          \n",
    "                \"bn\" : [True,False],          \n",
    "                \"bn_inp\" : [True,False],          \n",
    "                \"activ\" : ['relu',],          \n",
    "                \"lr\"  :[0.005,],          \n",
    "                \"s_comb\" : ['sqr',],      \n",
    "                's_l2' : [True,False,],          \n",
    "                's_bn' :[True,False],          \n",
    "                'rev' :[True,False,],          \n",
    "                'loss' : ['bce','fla','flb'],\n",
    "                'c_act' : [None],                    \n",
    "                'VF':['l_glv','l_glv_rep',],          \n",
    "                'bal' : [True,False,],          \n",
    "                'cl_m' : [None,]        \n",
    "              },\n",
    "          'cl' : {\n",
    "                \"siam_lyrs\" : [[128, 64],[256, 128],[256, 128, 64],[512, 256],],          \n",
    "                \"separ\" : [False,],      \n",
    "                \"layers\" : [[],],          \n",
    "                \"inp_drp\" : [0,0.3],  \n",
    "                \"o_drp\" : [0.5,],          \n",
    "                \"bn\" : [None],          \n",
    "                \"bn_inp\" : [True,False],          \n",
    "                \"activ\" : ['relu',],          \n",
    "                \"lr\"  :[0.0001,],          \n",
    "                'c_act' : [None,],\n",
    "                \"s_comb\" : ['eucl',],      \n",
    "                's_l2' : [True,],\n",
    "                's_bn' : [False],          \n",
    "                'rev' :[True,],          \n",
    "                'loss' : ['cl',],          \n",
    "                'VF':['l_glv','l_glv_rep',],          \n",
    "                'bal' : [True,False,],\n",
    "              }\n",
    "          }\n",
    "      \n",
    "      }\n",
    "\n",
    "        \n",
    "  def filter_func(grid_iter):\n",
    "    test_contrastive_loss = (\n",
    "        grid_iter['separ'] or \n",
    "        grid_iter['rev'] == False or \n",
    "        grid_iter['layers'] != [] or\n",
    "        grid_iter['siam_lyrs'] == [] or\n",
    "        grid_iter['s_comb'] != 'eucl'\n",
    "        )\n",
    "    if grid_iter['loss'] == 'cl' and test_contrastive_loss:\n",
    "      return False\n",
    "    if grid_iter['layers'] == [] and grid_iter['siam_lyrs'] == [] and grid_iter['other_drop'] != 0 :\n",
    "      return False\n",
    "    if grid_iter['layers'] != [] and grid_iter['s_comb'] == 'eucl':\n",
    "      return False\n",
    "    if not grid_iter['separ'] and grid_iter['siam_lyrs'] == []:\n",
    "      return False\n",
    "    return True\n",
    "  dct_main_grid = grids[GLOVE_DIM]\n",
    "  options1 = prepare_grid_search(dct_main_grid['non_cl'], valid_fn=filter_func, nr_trials=non_cl_runs)\n",
    "  options2 = prepare_grid_search(dct_main_grid['cl'], valid_fn=filter_func, nr_trials=cl_runs)\n",
    "  options = options1 + options2\n",
    "  options = [options[x] for x in np.random.choice(len(options), size=len(options), replace=False)]\n",
    "  timings = []\n",
    "  t_left = np.inf\n",
    "  top_models = []\n",
    "  k=3\n",
    "  last_ensemble = ''\n",
    "  ver = '4'\n",
    "  GD = str(GLOVE_DIM)[0]\n",
    "  for grid_iter, option in enumerate(options):    \n",
    "    g_type = 'G' if option['VF'] == 'l_glv' else 'R'\n",
    "    model_name = 'H3_{}{}{}D{:03d}'.format(\n",
    "        ver, g_type, GD, grid_iter+1)\n",
    "    P(\"\\n\\n\" + \"=\" * 70)\n",
    "    P(\"Running grid search iteration {}/{}\\n '{}' : {}\".format(\n",
    "        grid_iter+1, len(options), model_name, option))\n",
    "#    for k in option:\n",
    "#      P(\"  {}={},\".format(k,option[k] if type(option[k]) != str else \"'\" + option[k] + \"'\"))\n",
    "    P(\"  Time left for grid search completion: {:.1f} hrs\".format(t_left / 3600))\n",
    "    VF = option['VF']\n",
    "    vector_func = globals()[VF]\n",
    "    _t_start = time()\n",
    "    #### we need this ...\n",
    "    x_dev, y_dev = nli.word_entail_featurize(\n",
    "        data=dev, \n",
    "        vector_func=vector_func, \n",
    "        vector_combo_func=arr\n",
    "        )\n",
    "    model = WordEntailClassifier(\n",
    "        model_name=model_name,\n",
    "        x_dev=x_dev,\n",
    "        y_dev=y_dev,\n",
    "        **option)\n",
    "    res = nli.wordentail_experiment(\n",
    "            train_data=trn,\n",
    "            assess_data=dev,\n",
    "            vector_func=vector_func,\n",
    "            vector_combo_func=arr,\n",
    "            model=model,\n",
    "            )\n",
    "    score = res['macro-F1']\n",
    "    top_models = maybe_add_top_model(\n",
    "        top_models=top_models,\n",
    "        model=model,\n",
    "        score=score,\n",
    "        k=k\n",
    "        )\n",
    "    y_pred = model.predict(x_dev)\n",
    "    report = classification_report(y_dev, y_pred, digits=3, output_dict=True)\n",
    "    assert round(score,3) == round(report['macro avg']['f1-score'],3), \"ERROR:  score {} differs from report score {}\".format(\n",
    "        round(score,3), round(report['macro avg']['f1-score'],3))\n",
    "    P_REC = report['1']['recall']\n",
    "    ####\n",
    "    t_res = time() - _t_start\n",
    "    timings.append(t_res)\n",
    "    t_left = (len(options) - grid_iter - 1) * np.mean(timings)\n",
    "    dct_res = add_res(\n",
    "        dct=dct_res, \n",
    "        model_name=model_name, \n",
    "        score=round(score * 100, 2), \n",
    "        P_REC=round(P_REC * 100, 2),\n",
    "        **option)\n",
    "    if len(top_models) >= k:\n",
    "      P(\"Testing ensemble so far...\")\n",
    "      ensemble = EnsembleWrapper(\n",
    "          [x[0] for x in top_models], \n",
    "          vector_func_name=VF)\n",
    "      if last_ensemble != ensemble.model_name:\n",
    "        last_ensemble = ensemble.model_name\n",
    "        res = nli.wordentail_experiment(\n",
    "                train_data=trn,\n",
    "                assess_data=dev,\n",
    "                vector_func=vector_func,\n",
    "                vector_combo_func=arr,\n",
    "                model=ensemble,\n",
    "                )\n",
    "        score = res['macro-F1']\n",
    "        y_pred = ensemble.predict(x_dev)\n",
    "        report = classification_report(y_dev, y_pred, digits=3, output_dict=True)\n",
    "        assert score == report['macro avg']['f1-score']\n",
    "        P_REC = report['1']['recall']\n",
    "        dct_res = add_res(\n",
    "          dct=dct_res, \n",
    "          model_name=ensemble.model_name, \n",
    "          score=score, \n",
    "          P_REC=P_REC,\n",
    "          VF=VF,\n",
    "          )      \n",
    "    df = pd.DataFrame(dct_res).sort_values('SCORE')\n",
    "    P(\"Results so far:\\n{}\".format(df.iloc[-100:]))\n",
    "    df.to_csv(\"models/\"+_date+\"_results.csv\")\n",
    "  # end grid\n",
    "  return df\n",
    "      \n",
    "def vect_neighbors(v, df):\n",
    "  import scipy\n",
    "  distfunc = scipy.spatial.distance.cosine\n",
    "  dists = df.apply(lambda x: distfunc(v, x), axis=1)\n",
    "  return dists.sort_values().head()\n",
    "\n",
    "\n",
    "def ensemble_test(clfs, trn, dev, vect_func):\n",
    "  res = {'MODEL':[],'SCORE':[]}\n",
    "\n",
    "  x_trn, y_trn = nli.word_entail_featurize(\n",
    "      data=trn, \n",
    "      vector_func=vect_func, \n",
    "      vector_combo_func=arr\n",
    "      )\n",
    "  x_dev, y_dev = nli.word_entail_featurize(\n",
    "      data=dev, \n",
    "      vector_func=vect_func, \n",
    "      vector_combo_func=arr\n",
    "      )\n",
    "  \n",
    "  for model in clfs:\n",
    "    P(\"Testing model '{}' on data vectorized with '{}'\".format(\n",
    "        model.model_name, vect_func.__name__))\n",
    "    y_pred = model.predict(x_dev)\n",
    "    report = classification_report(y_dev, y_pred, digits=3, output_dict=True)\n",
    "    res = add_res(\n",
    "        res, \n",
    "        model.model_name, \n",
    "        score=round(report['macro avg']['f1-score'] * 100,2),\n",
    "        pos_f1=report['1']['f1-score'] * 100,\n",
    "        pos_rc=report['1']['recall'] * 100,\n",
    "        pos_pr=report['1']['precision'] * 100,\n",
    "        )\n",
    "    \n",
    "  \n",
    "  ens = EnsembleWrapper(\n",
    "      clfs, \n",
    "      vector_func_name=vect_func.__name__,\n",
    "      )\n",
    "  y_pred = ens.predict(x_dev)\n",
    "  report = classification_report(y_dev, y_pred, digits=3, output_dict=True)\n",
    "  res = add_res(\n",
    "      res, \n",
    "      ens.model_name, \n",
    "      score=round(report['macro avg']['f1-score'] * 100, 2),\n",
    "      pos_f1=report['1']['f1-score'] * 100,\n",
    "      pos_rc=report['1']['recall'] * 100,\n",
    "      pos_pr=report['1']['precision'] * 100,\n",
    "      )\n",
    "  df = pd.DataFrame(res).sort_values('SCORE')\n",
    "  P(\"Results:\\n{}\".format(df.iloc[-50:]))\n",
    "  return df\n",
    "\n",
    "\n",
    "\n",
    "def ensemble_train_test(dct_models_params, trn, dev, vect_funcs, n_models=None):\n",
    "  clfs = []\n",
    "  res = {'MODEL':[],'SCORE':[]}\n",
    "  for ii, model_name in enumerate(dct_models_params):\n",
    "    model_params = dct_models_params[model_name]\n",
    "    c_vect_func = globals()[model_params['VF']]\n",
    "    P(\"\\n\" + \"-\"*80)\n",
    "    P(\"Loading or training ensemble component {}/{}: '{}' with vect_func: '{}'...\".format(\n",
    "        ii+1, len(dct_models_params), model_name, c_vect_func.__name__))\n",
    "    x_dev, y_dev = nli.word_entail_featurize(\n",
    "        data=dev, \n",
    "        vector_func=c_vect_func, \n",
    "        vector_combo_func=arr\n",
    "        )\n",
    "      \n",
    "    clf = WordEntailClassifier(model_name=model_name, \n",
    "                               x_dev=x_dev,\n",
    "                               y_dev=y_dev,\n",
    "                               **model_params)\n",
    "    if clf.has_saved():\n",
    "      clf.load()\n",
    "    else:\n",
    "      x_trn, y_trn = nli.word_entail_featurize(\n",
    "          data=trn, \n",
    "          vector_func=c_vect_func, \n",
    "          vector_combo_func=arr\n",
    "          )  \n",
    "      clf.fit(x_trn, y_trn)\n",
    "      \n",
    "    clfs.append(clf)\n",
    "    y_pred = clf.predict(x_dev)\n",
    "    report = classification_report(y_dev, y_pred, digits=3, output_dict=True)\n",
    "    res = add_res(\n",
    "        res, \n",
    "        model_name,         \n",
    "        score=round(report['macro avg']['f1-score'] * 100,2),\n",
    "        pos_F1=report['1']['f1-score'] * 100,\n",
    "        pos_Rec=report['1']['recall'] * 100,\n",
    "        pos_Pre=report['1']['precision'] * 100,\n",
    "        vf=c_vect_func.__name__,\n",
    "        )\n",
    "    df = pd.DataFrame(res).sort_values('SCORE')\n",
    "    P(\"Results:\\n{}\".format(df))\n",
    "  \n",
    "  best_ens = None\n",
    "  best_mf1 = 0\n",
    "  if n_models is None:\n",
    "    lst_n_models = list(range(2,6))\n",
    "  elif type(n_models) == int:\n",
    "    lst_n_models = [n_models]\n",
    "  else:\n",
    "    lst_n_models = n_models\n",
    "\n",
    "  all_clfs_combs = []\n",
    "  for n_clfs in lst_n_models:\n",
    "    all_clfs_combs = all_clfs_combs + list(combinations(clfs, n_clfs))\n",
    "  P(\"Testing/searching for best ensemble...\")\n",
    "  for i, selected_clfs in enumerate(all_clfs_combs):    \n",
    "    for vect_func in vect_funcs:\n",
    "      Pr(\" Testing ensmble {}/{} ({:.2f}%) with {} models and {} vector func\\t\".format(\n",
    "          i+1, len(all_clfs_combs), (i+1)/len(all_clfs_combs) * 100, \n",
    "          len(selected_clfs), vect_func.__name__))\n",
    "      x_trn, y_trn = nli.word_entail_featurize(\n",
    "          data=trn, \n",
    "          vector_func=vect_func, \n",
    "          vector_combo_func=arr,\n",
    "          )\n",
    "      x_dev, y_dev = nli.word_entail_featurize(\n",
    "          data=dev, \n",
    "          vector_func=vect_func, \n",
    "          vector_combo_func=arr,\n",
    "          )\n",
    "        \n",
    "      ens = EnsembleWrapper(\n",
    "          selected_clfs, \n",
    "          vector_func_name=vect_func.__name__,\n",
    "          verbose=False,\n",
    "          )\n",
    "      y_pred = ens.predict(x_dev)\n",
    "      report = classification_report(y_dev, y_pred, digits=3, output_dict=True)\n",
    "      score = round(report['macro avg']['f1-score'] * 100,2)\n",
    "      if score > best_mf1:\n",
    "        best_mf1 = score\n",
    "        best_ens = ens\n",
    "      res = add_res(\n",
    "          res, \n",
    "          ens.model_name, \n",
    "          score=score,\n",
    "          pos_F1=report['1']['f1-score'] * 100,\n",
    "          pos_Rec=report['1']['recall'] * 100,\n",
    "          pos_Pre=report['1']['precision'] * 100,\n",
    "          vf=vect_func.__name__,        \n",
    "          )\n",
    "  df = pd.DataFrame(res).sort_values('SCORE')\n",
    "  df.to_csv('models/{}_ensembles.csv'.format(_date))\n",
    "  P(\"Results:\\n{}\".format(df.iloc[-50:]))\n",
    "  return best_ens, df\n",
    "\n",
    "\n",
    "def test_model(model, trn, dev, dct_res, model_sufix=''):\n",
    "  model_name = model.model_name\n",
    "  VECT_FUNC = globals()[model.vector_func_name]\n",
    "  P(\"Testing model '{}' with vector func '{} on train: {},  dev: {}\".format(\n",
    "      model_name, VECT_FUNC.__name__, len(trn), len(dev)))\n",
    "  dct_res_extra = {\n",
    "      \"DATA\" : [],\n",
    "      \"VF\" : [],\n",
    "      \"Macro-F1\" : [],\n",
    "      \"Pos F1\" : [],\n",
    "      \"Pos Recall\" : [],\n",
    "      \"Pos Precis\" : [],\n",
    "      }\n",
    "  def _log_data_result(dn, mf1, pf1, rec, prec, vf):\n",
    "    dct_res_extra['DATA'].append(dn)\n",
    "    dct_res_extra['VF'].append(vf.__name__)\n",
    "    dct_res_extra['Macro-F1'].append(mf1)\n",
    "    dct_res_extra['Pos F1'].append(pf1)\n",
    "    dct_res_extra['Pos Recall'].append(rec)\n",
    "    dct_res_extra['Pos Precis'].append(prec)\n",
    "\n",
    "  x_trn, y_trn = nli.word_entail_featurize(\n",
    "      data=trn, \n",
    "      vector_func=VECT_FUNC, \n",
    "      vector_combo_func=arr\n",
    "      )\n",
    "    \n",
    "  x_dev, y_dev = nli.word_entail_featurize(\n",
    "      data=dev, \n",
    "      vector_func=VECT_FUNC, \n",
    "      vector_combo_func=arr\n",
    "      )\n",
    "  y_pred = model.predict(x_dev)\n",
    "  report = classification_report(y_dev, y_pred, digits=3, output_dict=True)\n",
    "  mf1 = round(report['macro avg']['f1-score'] * 100,2)\n",
    "  pf1 = round(report['1']['f1-score'] * 100,2)\n",
    "  prc = round(report['1']['recall'] * 100,2)\n",
    "  ppr = round(report['1']['precision'] * 100,2)\n",
    "  dct_res = add_res(\n",
    "      dct=dct_res,\n",
    "      model_name=model_name + model_sufix,\n",
    "      score=mf1,\n",
    "      P_REC=prc)\n",
    "  P(\"\\n{}\\n  MF1: {:.2f}, 1F1: {:.2f}, 1R: {:.2f}, 1P: {:.2f}\\n\".format(model_name, mf1, pf1, prc, ppr))\n",
    "  _log_data_result(\n",
    "      'dev_full', \n",
    "      mf1,\n",
    "      pf1,\n",
    "      prc,\n",
    "      ppr,\n",
    "      VECT_FUNC,\n",
    "      )\n",
    "  y_pred = model.predict(x_trn)\n",
    "  report = classification_report(y_trn, y_pred, digits=3, output_dict=True)\n",
    "  _log_data_result(\n",
    "      'train_full', \n",
    "      report['macro avg']['f1-score'] * 100,\n",
    "      report['1']['f1-score'] * 100,\n",
    "      report['1']['recall'] * 100,\n",
    "      report['1']['precision'] * 100,\n",
    "      VECT_FUNC,\n",
    "      )\n",
    "  \n",
    "  for test_name in dct_data_GLOBAL:\n",
    "    P(\"\\n\\nTesting on '{}':\".format(test_name))\n",
    "    x, y = nli.word_entail_featurize(\n",
    "        data=dct_data_GLOBAL[test_name], \n",
    "        vector_func=VECT_FUNC, \n",
    "        vector_combo_func=arr\n",
    "        )\n",
    "    yh = model.predict(x)\n",
    "    P(classification_report(y, yh, digits=3))\n",
    "    report = classification_report(y, yh, digits=3, output_dict=True)\n",
    "    _log_data_result(\n",
    "        test_name, \n",
    "        report['macro avg']['f1-score'] * 100,\n",
    "        report['1']['f1-score'] * 100,\n",
    "        report['1']['recall'] * 100,\n",
    "        report['1']['precision'] * 100,\n",
    "        VECT_FUNC,\n",
    "        )\n",
    "  df = pd.DataFrame(dct_res_extra).sort_values('Macro-F1')\n",
    "  P(df)\n",
    "  return dct_res\n",
    "\n",
    "\n",
    "def train_test_config(model_name, model_config, trn, dev, dct_res):\n",
    "    \n",
    "  VECT_FUNC = globals()[model_config['VF']]\n",
    "  \n",
    "\n",
    "  x_dev, y_dev = nli.word_entail_featurize(\n",
    "      data=dev, \n",
    "      vector_func=VECT_FUNC, \n",
    "      vector_combo_func=arr\n",
    "      )\n",
    "\n",
    "  model = WordEntailClassifier(      \n",
    "    batch=256,\n",
    "    x_dev=x_dev,\n",
    "    y_dev=y_dev,\n",
    "    model_name=model_name,\n",
    "    optim=th.optim.Adam,\n",
    "    **model_config      \n",
    "    )\n",
    "  \n",
    "  if not model.has_saved():\n",
    "    _ = nli.wordentail_experiment(\n",
    "          train_data=trn,\n",
    "          assess_data=dev,\n",
    "          vector_func=VECT_FUNC,\n",
    "          vector_combo_func=arr,\n",
    "          model=model,    \n",
    "        )\n",
    "  else:\n",
    "    model.load()\n",
    "  \n",
    "  dct_res = test_model(model, trn, dev, dct_res)\n",
    "  \n",
    "  return dct_res\n",
    "\n",
    "\n",
    "def test_model_configs(dct_test_models, dct_res):\n",
    "  for _model_name, _model_params in dct_test_models.items():\n",
    "    VECT_FUNC = globals()[_model_params['VF']]\n",
    "    _x_trn, _y_trn = nli.word_entail_featurize(\n",
    "        data=train_data, \n",
    "        vector_func=VECT_FUNC, \n",
    "        vector_combo_func=arr\n",
    "        )\n",
    "    _x_dev, _y_dev = nli.word_entail_featurize(\n",
    "        data=dev_data, \n",
    "        vector_func=VECT_FUNC, \n",
    "        vector_combo_func=arr\n",
    "        )\n",
    "    _model = WordEntailClassifier(      \n",
    "      batch=256,\n",
    "      x_dev=_x_dev,\n",
    "      y_dev=_y_dev,\n",
    "      model_name=_model_name,\n",
    "      optim=th.optim.Adam,\n",
    "      **_model_params      \n",
    "      )  \n",
    "    _model.load()\n",
    "    _y_pred = _model.predict(_x_dev)\n",
    "    report = classification_report(_y_dev, _y_pred, digits=3, output_dict=True)\n",
    "    mf1 = round(report['macro avg']['f1-score'] * 100,2)\n",
    "    pf1 = round(report['1']['f1-score'] * 100,2)\n",
    "    prc = round(report['1']['recall'] * 100,2)\n",
    "    ppr = round(report['1']['precision'] * 100,2)\n",
    "    dct_res = add_res(\n",
    "        dct_res,\n",
    "        model_name=_model_name,\n",
    "        score=mf1,\n",
    "        pf1=pf1,\n",
    "        prc=prc,\n",
    "        ppr=ppr,\n",
    "        )\n",
    "  df_scores = pd.DataFrame(dct_res).sort_values('SCORE')\n",
    "  P(\"-\" * 80 + \"\\nResults:\\n\")\n",
    "  P(df_scores)\n",
    "  return dct_res\n",
    "  \n",
    "      \n",
    "\n",
    "def train_models(dct_models, trn, dev):\n",
    "  all_models = []\n",
    "  for i, (model_name, model_params) in enumerate(dct_models.items()):\n",
    "    P(\"\\nLoading or training/saving model {}/{}\".format(i+1, len(dct_models)))\n",
    "    VECT_FUNC = globals()[model_params['VF']]\n",
    "    x_trn, y_trn = nli.word_entail_featurize(\n",
    "        data=trn, \n",
    "        vector_func=VECT_FUNC, \n",
    "        vector_combo_func=arr\n",
    "        )\n",
    "    x_dev, y_dev = nli.word_entail_featurize(\n",
    "        data=dev, \n",
    "        vector_func=VECT_FUNC, \n",
    "        vector_combo_func=arr\n",
    "        )\n",
    "    model = WordEntailClassifier(      \n",
    "      batch=256,\n",
    "      x_dev=x_dev,\n",
    "      y_dev=y_dev,\n",
    "      model_name=model_name,\n",
    "      optim=th.optim.Adam,\n",
    "      **model_params      \n",
    "      )  \n",
    "    if model.has_saved():\n",
    "      model.load()\n",
    "      all_models.append(model)\n",
    "      continue\n",
    "    _ = nli.wordentail_experiment(\n",
    "          train_data=train_data,\n",
    "          assess_data=dev_data,\n",
    "          vector_func=VECT_FUNC,\n",
    "          vector_combo_func=arr,\n",
    "          model=model,    \n",
    "        )\n",
    "    all_models.append(model)\n",
    "  return all_models\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next cell is the actual code that generates the Original system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe-300...\n",
      "GloVe-300 loaded.\n",
      "\n",
      "Glove train: 8195 (95.9%)\n",
      "\n",
      "Glove dev: 2038 (94.8%)\n",
      "\n",
      "Train has 71 words that are not in GLOVE: ['worktop', 'overhead-travelling', 'prodrome', 'galactosidase', 'underpant']...\n",
      "  [['plum', 'worktop'], 0]\n",
      "  [['potato', 'overhead-travelling'], 0]\n",
      "  [['chill', 'prodrome'], 0]\n",
      "  [['coughing', 'prodrome'], 0]\n",
      "  [['chorioretinitis', 'blindness'], 1]\n",
      "  [['chorioretinitis', 'inflammation'], 1]\n",
      "  [['iridocyclitis', 'inflammation'], 1]\n",
      "  [['scleritis', 'inflammation'], 1]\n",
      "\n",
      "Pos vs neg: 16 vs 299\n",
      "\n",
      "\n",
      "Dev has 24 words that are not in GLOVE: ['tenosynovitis', 'hypernatremia', 'wharve', 'fermoota', 'vruttanama']...\n",
      "  [['antigen', 'tenosynovitis'], 0]\n",
      "  [['colitis', 'tenosynovitis'], 0]\n",
      "  [['corticosteroid', 'tenosynovitis'], 0]\n",
      "  [['depression', 'tenosynovitis'], 0]\n",
      "  [['tenosynovitis', 'antigen'], 1]\n",
      "  [['tenosynovitis', 'corticosteroid'], 1]\n",
      "  [['tenosynovitis', 'infection'], 1]\n",
      "\n",
      "Pos vs neg: 3 vs 92\n",
      "Train label dist:\n",
      "0    7199\n",
      "1    1349\n",
      "Name: 1, dtype: int64\n",
      "Dev label dist:\n",
      "0    1910\n",
      "1     239\n",
      "Name: 1, dtype: int64\n",
      "\n",
      "Loading or training/saving model 1/3\n",
      "Loaded model from 'models/H3_3G017.th'\n",
      "\n",
      "Loading or training/saving model 2/3\n",
      "Loaded model from 'models/H3_3G015.th'\n",
      "\n",
      "Loading or training/saving model 3/3\n",
      "Loaded model from 'models/H3_3G093.th'\n",
      "Initialized ensemble 'E_3G017_3G015_3G093' with 3 models using vectorizer 'l_glv':\n",
      "Ensemble 'E_3G017_3G015_3G093' architecture:\n",
      "  --------------------------------------------------------------------------------\n",
      "  Model 1/3\n",
      "  Model name: H3_3G017\n",
      "  ThWordEntailModel(\n",
      "    (path1_layers): ModuleList(\n",
      "      (0): InputPlaceholder(input_dim=300)\n",
      "      (1): Dropout(p=0.3, inplace=False)\n",
      "      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): L2_Normalizer()\n",
      "    )\n",
      "    (path2_layers): ModuleList(\n",
      "      (0): InputPlaceholder(input_dim=300)\n",
      "      (1): Dropout(p=0.3, inplace=False)\n",
      "      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): L2_Normalizer()\n",
      "    )\n",
      "    (post_layers): ModuleList(\n",
      "      (0): PathsCombiner(input_dim=300x2, output_dim=300, method='abs', act=None)\n",
      "      (1): Linear(in_features=300, out_features=128, bias=False)\n",
      "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Sigmoid()\n",
      "      (4): Dropout(p=0.2, inplace=False)\n",
      "      (5): Linear(in_features=128, out_features=32, bias=False)\n",
      "      (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (7): Sigmoid()\n",
      "      (8): Dropout(p=0.2, inplace=False)\n",
      "      (9): Linear(in_features=32, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  Vector func: l_glv\n",
      "  Trained on raw unbalanced data.\n",
      "  --------------------------------------------------------------------------------\n",
      "  Model 2/3\n",
      "  Model name: H3_3G015\n",
      "  ThWordEntailModel(\n",
      "    (path1_layers): ModuleList(\n",
      "      (0): InputPlaceholder(input_dim=300)\n",
      "      (1): Dropout(p=0.3, inplace=False)\n",
      "      (2): Linear(in_features=300, out_features=256, bias=False)\n",
      "      (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU()\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=256, out_features=128, bias=False)\n",
      "      (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU()\n",
      "      (9): Dropout(p=0.5, inplace=False)\n",
      "      (10): Linear(in_features=128, out_features=64, bias=False)\n",
      "      (11): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (12): L2_Normalizer()\n",
      "    )\n",
      "    (path2_layers): ModuleList(\n",
      "      (0): InputPlaceholder(input_dim=300)\n",
      "      (1): Dropout(p=0.3, inplace=False)\n",
      "      (2): Linear(in_features=300, out_features=256, bias=False)\n",
      "      (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU()\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=256, out_features=128, bias=False)\n",
      "      (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU()\n",
      "      (9): Dropout(p=0.5, inplace=False)\n",
      "      (10): Linear(in_features=128, out_features=64, bias=False)\n",
      "      (11): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (12): L2_Normalizer()\n",
      "    )\n",
      "    (post_layers): ModuleList(\n",
      "      (0): PathsCombiner(input_dim=64x2, output_dim=64, method='sqr', act=None)\n",
      "      (1): Linear(in_features=64, out_features=256, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Dropout(p=0.5, inplace=False)\n",
      "      (7): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (8): ReLU()\n",
      "      (9): Dropout(p=0.5, inplace=False)\n",
      "      (10): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  Vector func: l_glv\n",
      "  Trained on BALANCED data.\n",
      "  --------------------------------------------------------------------------------\n",
      "  Model 3/3\n",
      "  Model name: H3_3G093\n",
      "  ThWordEntailModel(\n",
      "    (path1_layers): ModuleList(\n",
      "      (0): InputPlaceholder(input_dim=300)\n",
      "      (1): Dropout(p=0.3, inplace=False)\n",
      "      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): L2_Normalizer()\n",
      "    )\n",
      "    (path2_layers): ModuleList(\n",
      "      (0): InputPlaceholder(input_dim=300)\n",
      "      (1): Dropout(p=0.3, inplace=False)\n",
      "      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): L2_Normalizer()\n",
      "    )\n",
      "    (post_layers): ModuleList(\n",
      "      (0): PathsCombiner(input_dim=300x2, output_dim=300, method='abs', act=None)\n",
      "      (1): Linear(in_features=300, out_features=256, bias=False)\n",
      "      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Tanh()\n",
      "      (4): Linear(in_features=256, out_features=128, bias=False)\n",
      "      (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Tanh()\n",
      "      (7): Linear(in_features=128, out_features=64, bias=False)\n",
      "      (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (9): Tanh()\n",
      "      (10): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  Vector func: l_glv\n",
      "  Trained on raw unbalanced data.\n",
      "  --------------------------------------------------------------------------------\n",
      "Ensemble results with standard train/dev distrib:\n",
      "Testing model 'E_3G017_3G015_3G093' with vector func 'l_glv on train: 8548,  dev: 2149\n",
      "\n",
      "E_3G017_3G015_3G093\n",
      "  MF1: 77.20, 1F1: 59.63, 1R: 61.51, 1P: 57.87\n",
      "\n",
      "\n",
      "\n",
      "Testing on 'glove_train':\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.995     0.960     0.977      6862\n",
      "           1      0.827     0.977     0.895      1333\n",
      "\n",
      "    accuracy                          0.963      8195\n",
      "   macro avg      0.911     0.968     0.936      8195\n",
      "weighted avg      0.968     0.963     0.964      8195\n",
      "\n",
      "\n",
      "\n",
      "Testing on 'glove_dev':\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.950     0.941     0.945      1802\n",
      "           1      0.579     0.623     0.600       236\n",
      "\n",
      "    accuracy                          0.904      2038\n",
      "   macro avg      0.764     0.782     0.773      2038\n",
      "weighted avg      0.907     0.904     0.905      2038\n",
      "\n",
      "\n",
      "\n",
      "Testing on 'out_train':\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.982     0.994     0.988       337\n",
      "           1      0.833     0.625     0.714        16\n",
      "\n",
      "    accuracy                          0.977       353\n",
      "   macro avg      0.908     0.810     0.851       353\n",
      "weighted avg      0.976     0.977     0.976       353\n",
      "\n",
      "\n",
      "\n",
      "Testing on 'out_dev':\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\nlu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.973     1.000     0.986       108\n",
      "           1      0.000     0.000     0.000         3\n",
      "\n",
      "    accuracy                          0.973       111\n",
      "   macro avg      0.486     0.500     0.493       111\n",
      "weighted avg      0.947     0.973     0.960       111\n",
      "\n",
      "          DATA     VF  Macro-F1   Pos F1  Pos Recall  Pos Precis\n",
      "5      out_dev  l_glv   49.3151   0.0000      0.0000      0.0000\n",
      "0     dev_full  l_glv   77.2000  59.6300     61.5100     57.8700\n",
      "3    glove_dev  l_glv   77.2672  60.0000     62.2881     57.8740\n",
      "4    out_train  l_glv   85.1243  71.4286     62.5000     83.3333\n",
      "1   train_full  l_glv   93.5850  89.3733     97.2572     82.6717\n",
      "2  glove_train  l_glv   93.6456  89.5461     97.6744     82.6667\n",
      "****** Ensemble model passing fit call ******\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.951     0.944     0.948      1910\n",
      "           1      0.579     0.615     0.596       239\n",
      "\n",
      "    accuracy                          0.907      2149\n",
      "   macro avg      0.765     0.780     0.772      2149\n",
      "weighted avg      0.910     0.907     0.909      2149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "USE_NEW_SPLIT = False\n",
    " \n",
    "GLOVE_DIM = 300\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('precision', 4)  \n",
    "  \n",
    "if \"GLOVE\" not in globals() or len(next(iter(GLOVE.values()))) != GLOVE_DIM:\n",
    "  P(\"Loading GloVe-{}...\".format(GLOVE_DIM))\n",
    "  GLOVE = utils.glove2dict(os.path.join(GLOVE_HOME, 'glove.6B.{}d.txt'.format(GLOVE_DIM)))  \n",
    "  P(\"GloVe-{} loaded.\".format(GLOVE_DIM))\n",
    "\n",
    "with open(wordentail_filename) as f:\n",
    "  wordentail_data = json.load(f)  \n",
    "  \n",
    "train_data = wordentail_data['word_disjoint']['train']\n",
    "dev_data = wordentail_data['word_disjoint']['dev']\n",
    "dct_results = OrderedDict({'MODEL':[], 'SCORE':[], 'P_REC': []})\n",
    "\n",
    "maybe_find_glove_replacement('aromatization')\n",
    "glv_analysis = test_glove_vs_data(train_data, dev_data)\n",
    "dct_data_GLOBAL = glv_analysis[0]\n",
    "miss_train, miss_dev = glv_analysis[1]\n",
    "\n",
    "sol_models_params = {\n",
    "#'H3_2G424': {'siam_lyrs': [256, 128], 'separ': True, 'layers': [256, 128, 64], 'inp_drp': 0.3, 'o_drp': 0.2, 'bn': True, 'bn_inp': False, 'activ': 'relu', 'lr': 0.005, 's_comb': 'sub', 's_l2': True, 's_bn': False, 'rev': False, 'loss': 'fl', 'c_act': None, 'VF': 'l_glv', 'bal': True},\n",
    "'H3_3G017' : {'siam_lyrs': [], 'separ': True, 'layers': [128, 32], 'inp_drp': 0.3, 'o_drp': 0.2, 'bn': True, 'bn_inp': True, 'activ': 'sigmoid', 'lr': 0.005, 's_comb': 'abs', 's_l2': True, 's_bn': True, 'rev': True, 'loss': 'fla', 'c_act': None, 'VF': 'l_glv', 'bal': False},  \n",
    "'H3_3G015' : {'siam_lyrs': [256, 128, 64], 'separ': True, 'layers': [256, 128, 64], 'inp_drp': 0.3, 'o_drp': 0.5, 'bn': False, 'bn_inp': False, 'activ': 'relu', 'lr': 0.01, 's_comb': 'sqr', 's_l2': True, 's_bn': True, 'rev': False, 'loss': 'fla', 'c_act': None, 'VF': 'l_glv', 'bal': True},\n",
    "'H3_3G093' : {'siam_lyrs': [], 'separ': True, 'layers': [256, 128, 64], 'inp_drp': 0.3, 'o_drp': 0, 'bn': True, 'bn_inp': True, 'activ': 'tanh', 'lr': 0.01, 's_comb': 'abs', 's_l2': True, 's_bn': True, 'rev': True, 'loss': 'flb', 'c_act': None, 'VF': 'l_glv', 'bal': False},\n",
    "#\"H3_2R321\" : {'siam_lyrs': [256, 128, 64], 'separ': True, 'layers': [128, 32], 'inp_drp': 0.3, 'o_drp': 0, 'bn': True, 'bn_inp': False, 'activ': 'sigmoid', 'lr': 0.01, 's_comb': 'sqr', 's_l2': False, 's_bn': True, 'rev': True, 'loss': 'bce', 'c_act': None, 'bal': True, 'VF' : 'l_glv_rep'},\n",
    "  }\n",
    "\n",
    "SOL_VECT_FUNC = l_glv\n",
    "\n",
    "P(\"Train label dist:\")\n",
    "calc_label_distrib(train_data)\n",
    "P(\"Dev label dist:\")\n",
    "calc_label_distrib(dev_data)\n",
    "# reduce DEV and add to train\n",
    "new_dev_size = 500\n",
    "train_added = len(dev_data) - new_dev_size\n",
    "dev_to_train_idxs = np.random.choice(len(dev_data), size=train_added, replace=False)\n",
    "added_train_data = [dev_data[x] for x in dev_to_train_idxs]\n",
    "\n",
    "\n",
    "base_sol_models = train_models(\n",
    "  sol_models_params, \n",
    "  trn=train_data, \n",
    "  dev=dev_data,\n",
    "  )\n",
    "\n",
    "base_solution_ens = EnsembleWrapper(\n",
    "  models=base_sol_models,\n",
    "  vector_func_name=SOL_VECT_FUNC.__name__,\n",
    "  verbose=True\n",
    "  )  \n",
    "P(\"Ensemble results with standard train/dev distrib:\")\n",
    "dct_results = test_model(\n",
    "  base_solution_ens,\n",
    "  trn=train_data,\n",
    "  dev=dev_data,\n",
    "  dct_res=dct_results,\n",
    "  )\n",
    "\n",
    "if USE_NEW_SPLIT:  \n",
    "    new_train_data = train_data + added_train_data\n",
    "    new_dev_data = [dev_data[x] for x in range(len(dev_data)) if x not in dev_to_train_idxs]\n",
    "    new_model_dict = {k+'XT':v for k,v in sol_models_params.items()}\n",
    "\n",
    "    P(\"NEW Train label distrib:\")\n",
    "    calc_label_distrib(new_train_data)\n",
    "    P(\"NEW Dev label distrib:\")\n",
    "    calc_label_distrib(new_dev_data)\n",
    "\n",
    "    sol_models = train_models(\n",
    "        new_model_dict,\n",
    "        trn=new_train_data,\n",
    "        dev=new_dev_data) \n",
    "\n",
    "\n",
    "    solution_ensemble = EnsembleWrapper(\n",
    "        models=sol_models,\n",
    "        vector_func_name=SOL_VECT_FUNC.__name__,\n",
    "        verbose=True\n",
    "        )  \n",
    "\n",
    "    P(\"\\nTEST on original train/dev splits\")\n",
    "    dct_results = test_model(\n",
    "        solution_ensemble, \n",
    "        trn=train_data,\n",
    "        dev=dev_data,\n",
    "        dct_res=dct_results,\n",
    "        model_sufix='_std')\n",
    "    P(\"\\nTEST on NEW train/dev splits\")\n",
    "    dct_results = test_model(\n",
    "        solution_ensemble, \n",
    "        trn=new_train_data,\n",
    "        dev=new_dev_data,\n",
    "        dct_res=dct_results,\n",
    "        model_sufix='_ext')\n",
    "\n",
    "else:\n",
    "    solution_ensemble = base_solution_ens\n",
    "\n",
    "solution_result = nli.wordentail_experiment(\n",
    "  train_data=train_data,\n",
    "  assess_data=dev_data,\n",
    "  vector_func=globals()[solution_ensemble.vector_func_name],\n",
    "  vector_combo_func=arr,\n",
    "  model=solution_ensemble,\n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off [1 point]\n",
    "\n",
    "The goal of the bake-off is to achieve the highest macro-average F1 score on __word_disjoint__, on a test set that we will make available at the start of the bake-off. The announcement will go out on the discussion forum. To enter, you'll be asked to run `nli.bake_off_evaluation` on the output of your chosen `nli.wordentail_experiment` run. \n",
    "\n",
    "The cells below this one constitute your bake-off entry.\n",
    "\n",
    "The rules described in the [Your original system](#Your-original-system-[3-points]) homework question are also in effect for the bake-off.\n",
    "\n",
    "Systems that enter will receive the additional homework point, and systems that achieve the top score will receive an additional 0.5 points. We will test the top-performing systems ourselves, and only systems for which we can reproduce the reported results will win the extra 0.5 points.\n",
    "\n",
    "Late entries will be accepted, but they cannot earn the extra 0.5 points. Similarly, you cannot win the bake-off unless your homework is submitted on time.\n",
    "\n",
    "The announcement will include the details on where to submit your entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Bake-off ensemble architecture ======\n",
      "Ensemble 'E_3G017_3G015_3G093' architecture:\n",
      "  --------------------------------------------------------------------------------\n",
      "  Model 1/3\n",
      "  Model name: H3_3G017\n",
      "  ThWordEntailModel(\n",
      "    (path1_layers): ModuleList(\n",
      "      (0): InputPlaceholder(input_dim=300)\n",
      "      (1): Dropout(p=0.3, inplace=False)\n",
      "      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): L2_Normalizer()\n",
      "    )\n",
      "    (path2_layers): ModuleList(\n",
      "      (0): InputPlaceholder(input_dim=300)\n",
      "      (1): Dropout(p=0.3, inplace=False)\n",
      "      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): L2_Normalizer()\n",
      "    )\n",
      "    (post_layers): ModuleList(\n",
      "      (0): PathsCombiner(input_dim=300x2, output_dim=300, method='abs', act=None)\n",
      "      (1): Linear(in_features=300, out_features=128, bias=False)\n",
      "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Sigmoid()\n",
      "      (4): Dropout(p=0.2, inplace=False)\n",
      "      (5): Linear(in_features=128, out_features=32, bias=False)\n",
      "      (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (7): Sigmoid()\n",
      "      (8): Dropout(p=0.2, inplace=False)\n",
      "      (9): Linear(in_features=32, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  Vector func: l_glv\n",
      "  Trained on raw unbalanced data.\n",
      "  --------------------------------------------------------------------------------\n",
      "  Model 2/3\n",
      "  Model name: H3_3G015\n",
      "  ThWordEntailModel(\n",
      "    (path1_layers): ModuleList(\n",
      "      (0): InputPlaceholder(input_dim=300)\n",
      "      (1): Dropout(p=0.3, inplace=False)\n",
      "      (2): Linear(in_features=300, out_features=256, bias=False)\n",
      "      (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU()\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=256, out_features=128, bias=False)\n",
      "      (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU()\n",
      "      (9): Dropout(p=0.5, inplace=False)\n",
      "      (10): Linear(in_features=128, out_features=64, bias=False)\n",
      "      (11): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (12): L2_Normalizer()\n",
      "    )\n",
      "    (path2_layers): ModuleList(\n",
      "      (0): InputPlaceholder(input_dim=300)\n",
      "      (1): Dropout(p=0.3, inplace=False)\n",
      "      (2): Linear(in_features=300, out_features=256, bias=False)\n",
      "      (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU()\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=256, out_features=128, bias=False)\n",
      "      (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU()\n",
      "      (9): Dropout(p=0.5, inplace=False)\n",
      "      (10): Linear(in_features=128, out_features=64, bias=False)\n",
      "      (11): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (12): L2_Normalizer()\n",
      "    )\n",
      "    (post_layers): ModuleList(\n",
      "      (0): PathsCombiner(input_dim=64x2, output_dim=64, method='sqr', act=None)\n",
      "      (1): Linear(in_features=64, out_features=256, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Dropout(p=0.5, inplace=False)\n",
      "      (7): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (8): ReLU()\n",
      "      (9): Dropout(p=0.5, inplace=False)\n",
      "      (10): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  Vector func: l_glv\n",
      "  Trained on BALANCED data.\n",
      "  --------------------------------------------------------------------------------\n",
      "  Model 3/3\n",
      "  Model name: H3_3G093\n",
      "  ThWordEntailModel(\n",
      "    (path1_layers): ModuleList(\n",
      "      (0): InputPlaceholder(input_dim=300)\n",
      "      (1): Dropout(p=0.3, inplace=False)\n",
      "      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): L2_Normalizer()\n",
      "    )\n",
      "    (path2_layers): ModuleList(\n",
      "      (0): InputPlaceholder(input_dim=300)\n",
      "      (1): Dropout(p=0.3, inplace=False)\n",
      "      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): L2_Normalizer()\n",
      "    )\n",
      "    (post_layers): ModuleList(\n",
      "      (0): PathsCombiner(input_dim=300x2, output_dim=300, method='abs', act=None)\n",
      "      (1): Linear(in_features=300, out_features=256, bias=False)\n",
      "      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Tanh()\n",
      "      (4): Linear(in_features=256, out_features=128, bias=False)\n",
      "      (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Tanh()\n",
      "      (7): Linear(in_features=128, out_features=64, bias=False)\n",
      "      (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (9): Tanh()\n",
      "      (10): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  Vector func: l_glv\n",
      "  Trained on raw unbalanced data.\n",
      "  --------------------------------------------------------------------------------\n",
      "Bake-off evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.914     0.860     0.886      1767\n",
      "           1      0.550     0.679     0.608       446\n",
      "\n",
      "    accuracy                          0.823      2213\n",
      "   macro avg      0.732     0.770     0.747      2213\n",
      "weighted avg      0.841     0.823     0.830      2213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enter your bake-off assessment code into this cell. \n",
    "# Please do not remove this comment.\n",
    "##### YOUR CODE HERE\n",
    "\n",
    "P(\"====== Bake-off ensemble architecture ======\")\n",
    "bakeoff_model['model'].print_models()\n",
    "\n",
    "bakeoff_model = solution_result\n",
    "test_data_filename = os.path.join(\n",
    "    NLIDATA_HOME,\n",
    "    \"bakeoff-wordentail-data\",\n",
    "    \"nli_wordentail_bakeoff_data-test.json\")\n",
    "\n",
    "P(\"Bake-off evaluation:\")\n",
    "\n",
    "nli.bake_off_evaluation(\n",
    "    bakeoff_model,\n",
    "    test_data_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.747"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On an otherwise blank line in this cell, please enter\n",
    "# your macro-avg f1 value as reported by the code above. \n",
    "# Please enter only a number between 0 and 1 inclusive.\n",
    "# Please do not remove this comment.\n",
    "\n",
    "##### YOUR CODE HERE\n",
    "\n",
    "0.747\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
